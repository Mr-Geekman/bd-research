\chapter{Обзор литературы}

\section{Классические методы}

\subsection{Расстояние Дамерау-Левенштейна}

Исправление опечаток --- одна из старейших задач в автоматической обработке естественного языка. Работы Левенштейна \cite{Levenshtein1965} и Дамерау \cite{Damerau1964} в середине 60-х позволили построить меру разности двух слов на основании количества элементарных изменений, которые надо внести в одно слово, чтобы получить другое. Такой подход позволяет для слов, отсутствующих в словаре, находить близкие словарные слова. Это делает возможным нахождение по несловарному слову похожих словарных.

Рассмотрим подробнее как устроено расстояние Дамерау-Левенштейна (иногда называется редакционным расстоянием). Для начала выделим список элементарных операций. Пусть имеем строку $s$. Операции:
\begin{itemize}
	\item вставка --- добавление какого-либо символа из алфавита в строку $s$;
	\item удаление --- удаление какого-либо символа из $s$;
	\item замена --- замена какого-либо символа из $s$ на другой символ из алфавита;
	\item транспозиция --- перестановка двух смежных символов.
\end{itemize}

Тогда рассматриваемое расстояние между строками $s$ и $t$ --- это минимальное количество элементарных операций, требуемых чтобы из $s$ получить $t$. 

\subsection{Модель шумного канала}

Рассмотрим классическую схему решения задачи исправления опечаток на основе модели шумного канала. Пусть канал принимает в себя слово $s$, а на выходе после преобразований получается строка $t$. Мы, как наблюдатели, видим только $t$ и хотим восстановить $s$. Для этого мы можем попробовать оценить $P(s|t)$. Несколько преобразуем это выражение, согласно формуле Байеса:

\begin{equation*}
	P(s|t) = \frac{P(s) P(t|s)}{\sum\limits_{s} P(s) P(t|s)}.
\end{equation*}

Знаменатель нам не интересен, потому что не зависит от $s$. Тогда для нахождения исходной строки требуется максимизировать числитель. Первый множитель --- это априорная вероятность исправления. Ее можно получить при помощи языковой модели. Второй множитель --- это модель шумного канала (или модель ошибок), которая описывает вероятности различных преобразований внутри канала. 

Схема была предложена в двух работах: <<Context-based spelling correction>> \cite{Mays1991} и <<A Spelling Correction Program Based on a Noisy Channel Model>>\cite{Kernighan1990}. Рассмотрим подробнее вторую, так как она доступна публично. Языковая модель была выбрана униграммная, а в качестве модели ошибок было рассмотрено взвешенное расстояние Дамерау-Левенштейна. Дело в том, что в его обычно версии все операции имеют одинаковый вес. Но если мы посмотрим на то, как люди набирают текст на клавиатуре, то при печати разные ошибки совершаются с разной вероятностью. Например, буквы, стоящие близко друг от друга на клавиатуре перепутать гораздо проще, чем те, что находятся друг от друга далеко. Таким образом, требуется научиться взвешивать разные операции в зависимости от их частот. Тогда расстояние --- это минимальная сумма весов рассматриваемых элементарных операций. 

В качестве элементарных операций авторами работы были выбраны следующие классы:
\begin{itemize}
	\item удаление буквы $s$ после буквы $t$;
	\item вставка буквы $s$ после буквы $t$;
	\item замена буквы $s$ на букву $t$;
	\item транспозиция соседних букв $s$ и $t$.
\end{itemize}

Называя их классами, мы подразумеваем, что операции, над различными буквами $s$ и $t$ рассматриваются раздельно. Для оценки вероятностей всех преобразований был использован EM-алгоритм \cite{Dempster1977}.

Можно заметить, что у этой схемы есть большой потенциал для улучшения. Так, авторы работы <<An Improved Error Model for Noisy Channel Spelling Correction>> \cite{Brill2000} усложнили модель ошибок, значительно повысив качество.

\subsection{Дальнейшее развитие}

После этого можно выделить два общих направления развития в решении задачи исправления опечаток. 

Первое направление нацелено на эффективный поиск кандидатов. Нахождение словарных слов на заданном расстоянии Дамерау-Левенштейна может быть непростой задачей для языков с развитой морфологией, например для русского. Для иллюстрации рассмотрим как растет количество словарных слов если увеличивать допустимое расстояние. В качестве словаря возьмем объединение wiktionary \cite{Wiktionary} и словаря Хагена \cite{Hagen}. В таблице \ref{table:distances_examples} ясно видно, что с ростом параметра РР (редакционное расстояние) количество словоформ на заданном расстоянии растет практически экспоненциально и совпадает с точностью до порядка для всех выбранных слов.

\begin{table}[h]
	\begin{center}
		\caption{Количества слов на заданном редакционном расстоянии (РР).}
		\label{table:distances_examples}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Токен} & \textbf{РР = 1}  & \textbf{РР = 2} & \textbf{РР = 3} & \textbf{РР = 4}  \\
			\hline
			делать & 7  & 137 & 1588 & 12913  \\
			длать & 6  & 159 & 2312 & 19063  \\
			далеко & 7  & 72 & 1476 & 16582 \\
			далико & 3  & 126 & 2089 & 17763 \\
			собака & 9  & 92 & 1812 & 17714 \\
			сабака & 9  & 153 & 2067 & 17758 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

В работе <<Error-tolerant Finite-state Recognition with Applications to Morphological Analysis and Spelling Correction>> \cite{Oflazer1996} был предложен алгоритм поиска в глубину по префиксному бору, построенному над словарем. Для уменьшения количества рассматриваемых кандидатов применялась эвристика на нижнюю оценку расстояния.  Авторы работы <<Fast approximate string matching with finite automata>> \cite{Hulden2009} использовали алгоритм A-star \cite{Hart1968} с разработанными для него эвристиками.

Второе направление нацелено на использование контекста для выбора корректного исправления.  Так, например, в работе <<Combining Trigram-based and Feature-based Methods for Context-Sensitive Spelling Correction>> \cite{Golding1996} были предложены подходы, основанные на триграммах частей речи, совстречаемости слов. Задача состояла в исправлении словарных ошибок в применении к заранее выбранным множествам слов, которые часто путают, например <<their/there/they're>>. В такой постановке необходимо грамотно использовать контекст. 

В работе <<Four types of context for automatic spelling correction>> \cite{Flor2012} исследовалось влияние различных контекстных признаков на качество исправления несловарных слов. В качестве данных были взяты эссе носителей английского языка и изучающих его, как иностранный язык. Опишем предложенный алгоритм:
\begin{enumerate}
	\item Обнаружение позиций для исправления. Используется проверка по словарю.
	\item Для каждой выбранной на предыдущем этапе позиции генерируется список кандидатов на исправление на основании расстояния Дамерау-Левенштейна. 
	\item Вычисляются признаки кандидатов, в том числе и контекстные.
	\item Производится ранжирование на признаках и выбирается самое высоко оцененное исправление.
\end{enumerate}

Посмотрим теперь на то, какие контекстные признаки были испробованы:
\begin{itemize}
	\item Словарные n-граммы. 
	
	Подсчитывается сумма логарифмов от частот n-грамм, включающих в себя контекст и исправляемое слово, замененное на рассматриваемого кандидата. Были также попытки заменить суммы логарифмов на PMI (поточечная взаимная информация).
	\item Семантическая связь. 
	
	Этот признак отвечает за то, насколько рассматриваемых кандидат хорошо вписывается в тему, о которой говорится в тексте. Например, имеем токен <<carh>>, а в качестве кандидатов на исправление рассматриваются <<car>> и <<card>>. Если в тексте речь идет об автомобилях, то больше подходит первый вариант, а если о казино, то второй. Для подсчета самого признака авторы брали сумму нормализованных значений поточечной взаимной информации между кандидатом и словами из контекста. В отличие от предыдущего признака этот способен охватывать больший контекст.
	\item Déjà vu.
	
	Признак основан на нормализованном подсчете других форм слова рассматриваемого кандидата. Если кандидат или его вариации встречаются в тексте эссе часто, то это можно рассматривать, как свидетельство в его пользу. 
	
	\item Тематический список.
	
	Начало эссе можно рассматривать, как некий набор тематических ключевых слов. Если кандидат на исправление появляется в нем, то это может свидетельствовать о его превосходстве в рамках выбранной тематики эссе.
	
\end{itemize}

Как видим, не все эти признаки могут быть применены к отдельным предложениям.

\section{Методы, применяемые для русского языка}

В этом разделе будут рассмотрены методы, которые применялись для исправления опечаток в русскоязычных текстах. 

До проведения соревнования SpellRuEval было не так много работ в этом направлении. Можно выделить работы <<Исправление поисковых запросов в Яндексе>> \cite{Bajtin2008}, <<Автоматическое исправление опечаток в поисковых запросах без учета контекста>>\cite{Panina2013}, которые концентрируют свое внимание именно на поисковых запросах. Рассмотрим, как во второй работе производится исправление ошибок в несловарных словах:
\begin{enumerate}
	\item Генерация исправления. По строке $q$ генерируется исправление $c$ на основе модели шумного канала.
	\item Выравнивание. Исходная строка и сгенерированное исправление выравниваются, чтобы понять какой токен исходного токена во что переходит.
	\item Фильтрация. На этом этапе отбираются только те ошибки, которые авторы намерены были исправлять.
	\item Классификация. Для каждого исправления генерируются признаки и на основе логистической регрессии над ними решается стоит применять это исправление или нет.
\end{enumerate}

Посмотрим также на используемые признаки:
\begin{itemize}
	\item веса $q$, $c$ в словарной языковой модели;
	\item веса $q$, $c$ в символьной языковой модели;
	\item длины $q$, $c$ в символах;
	\item индикаторы присутствия в словаре для $q$, $c$;
	\item язык $q$, $c$ (русский или английский);
	\item вероятность написания $q$, $c$ с заглавной буквы. Данные собраны по большому корпусу;
	\item взвешенное редакционное расстояние от $q$ до $c$;
	\item взаимный словарный контекст между $q$ и $c$. Строится на основе слов, встречающихся с $q$ и $c$ в 3-граммах.
\end{itemize}

Как видим, на самом деле некая контекстная информация все же была использована.

Также с исправлением опечаток связана работа <<Моделирование расширенной лемматизации для русского языка на основе морфологического парсера TnT-Russian>>\cite{Shavrina2015}, где исследуется алгоритм исправления опечаток в применении к текстам из социальных медиа, содержащихся в ГИКРЯ. Работа модели состояла из следующий этапов:
\begin{enumerate}
	\item Обучение взвешенного расстояния Дамерау-Левенштейна на основе EM-алгоритма.
	\item Поиск кандидатов в словаре на расстоянии Дамерау-Левенштейна, не превышающем 2.
	\item Ранжирование кандидатов при помощи модели ошибок на основе взвешенного расстояния Дамерау-Левенштейна. Модель предполагает рассмотрение всех выравниваний, которые не намного хуже оптимального.
\end{enumerate}

\subsection{SpellRuEval}

Теперь рассмотрим, какие методы применялись на соревновании SpellRuEval, для этого изучим статью победителей соревнования: <<Automatic spelling correction for Russian social media texts>> \cite{Sorokin2016}.

Исправление состояло из трех этапов:
\begin{enumerate}
	\item Генерация кандидатов.
	
	Для каждого токена находятся словарные слова на заданном расстоянии Дамерау-Левенштейна. К ним добавляются также слова, имеющие тот же фонетический код, что и у исправляемого. Алгоритм фонетического кодирования был построен специально для данной задачи на основе уже существующего алгоритма Double Metaphone \cite{Philips2000}.  Дополнительно вносятся около 50 вручную сформированных соответствия. 
	
	Для обработки ошибки вставки пробела рассматриваются группы последовательных слов. Чтобы обработать ошибку удаления пробела, при поиске слов в боре допускается вставка пробела после получения словарного слова и продолжение поиска.
	\item Отсечение гипотез.
	
	После предыдущего этапа для каждого токена или последовательной группы токенов имеется список кандидатов. На основе всех этих данных может быть сформировано исправленное предложение, путем комбинации всех возможных сочетаний кандидатов. В таком случае доступных исправлений предложения будет очень много, и их нужно проредить. Для этого каждый вариант исправления оценивается согласно языковой модели и модели ошибок, которая в свою очередь смотрит на заглавное/строчное написание слова, источник кандидата, присутствие в словаре и другие признаки. Итого, остаются лишь те варианты, которые не хуже лучшего в какое-то фиксированное число раз.
	
	\item Ранжирование.
	
	Когда осталось не так много возможных предложений-кандидатов по ним вычисляются признаки, над которыми затем выполняется ранжирование при помощи логистической регрессии. Рассмотрим признаки, использованные в лучшей посылке:
	\begin{itemize}
		\item количество слов в предложении;
		\item вывод модели ошибок;
		\item вывод языковой модели;
		\item количество измененных слов;
		\item количество несловарных слов;
		\item количество исправлений в словарных словах;
		\item количество исправлений на расстоянии Дамерау-Левенштейна, равном одному;
		\item количество исправлений, полученных при помощи фонетического кодирования;
		\item количество исправлений, полученных при помощи вручную созданных соответствий;
		\item количество исправлений, вставляющих пробел в слово;
		\item количество исправлений, объединяющих слова;
		\item количество несловарных слов, которые можно разбить на словарные подстроки;
		\item взвешенное расстояние Дамерау-Левенштейна.
	\end{itemize}
\end{enumerate}

Стоит также описать какие техники не дали прироста в качестве. Так, для внесения в модель знания о морфологии были использованы вероятности из языковой модели, построенной на частях речи. Были также испытаны признаки, призванные охватить семантику, посчитанные на основе совстречаемостей словоформ, и признаки предлогов.
