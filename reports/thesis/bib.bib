@article{Rozovskaya2019,
abstract = {Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically rich languages, with a focus on Russian. We present a corrected and error-tagged corpus of Russian learner writing and develop models that make use of existing state-of-the-art methods that have been well studied for English. Although impressive results have recently been achieved for grammar error correction of non-native English writing, these results are limited to domains where plentiful training data are available. Because annotation is extremely costly, these approaches are not suitable for the majority of domains and languages. We thus focus on methods that use “minimal supervision”; that is, those that do not rely on large amounts of annotated training data, and show how existing minimal-supervision approaches extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology.},
author = {Rozovskaya, Alla and Roth, Dan},
doi = {10.1162/tacl_a_00251},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Grammar Error Correction in Morphologically Rich Languages The Case of Russian.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {1--17},
title = {{Grammar Error Correction in Morphologically Rich Languages: The Case of Russian}},
volume = {7},
year = {2019}
}
@article{Flor2019,
abstract = {Spelling correction has attracted a lot of attention in the NLP community. However, models have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our data: 88.12{\{}$\backslash${\%}{\}} accuracy. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1{\{}$\backslash${\%}{\}}). Furthermore, this approach allows easy portability to new domains. We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data.},
author = {Flor, Michael and Fried, Michael and Rozovskaya, Alla},
doi = {10.18653/v1/w19-4407},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/A Benchmark Corpus of English Misspellingsand a Minimally-supervised Model for Spelling Correction.pdf:pdf},
number = {Section 3},
pages = {76--86},
title = {{A Benchmark Corpus of English Misspellings and a Minimally-supervised Model for Spelling Correction}},
year = {2019}
}


@article{Shavrina2015,
abstract = {Автоматические методы морфологического анализа и лемматизации, предназначенные для литературного русского языка, могут давать невысокие результаты, будучи применёнными к так называемым социальным медиа (микроблоги, социальные сети и т. д.). Одной из причин является орфографическая вариативность текстов в социальных медиа, зачастую вызванная опечатками. Мы предлагаем интегрировать модуль исправления опечаток в алгоритм морфологического анализа на примере Генерального интернет-корпуса русского языка (ГИКРЯ), что позволит осуществить расширенную лемматизацию. Также в работе предлагается новый алгоритм исправления опечаток, основанный на взвешенном расстоянии Левенштейна и проводится анализ типичных нарушений орфографической нормы в текстах социальных медиа.},
author = {Шаврина, Т. О. and Сорокин, А. А.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/Modeling advanced lemmatization for Russian language using TnT-Russian morphological parser.pdf:pdf},
title = {{Моделирование расширенной лемматизации для русского языка на основе морфологического парсера TnT-Russian}},
year = {2015},
language={russian}
}
@article{Sorokin2016a,
abstract = {This paper reports on the first competition on automatic spelling correction for Russian language - SpellRuEval - held within the framework of "Dialogue Evaluation". The competition aims to bring together groups of Russian academic researchers and IT-companies in order to gain and exchange the experience in automatic spelling correction, especially concentrating on social media texts. The data for the competition was taken from Russian segment of Live Journal. 7 teams took part in the competition, the best results were achieved by the model using edit distance and phonetic similarity for candidate search and n-gram language model for their reranking. We discuss in details the algorithms used by the teams, as well as the methodology of evaluation for automatic spelling correction.},
author = {Сорокин, А. А. and Байтин, А. В. and Галинская, И. Е. and Рыкунова, Е. Д. and Шаврина, Т. О.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/SpellRueval the First Competition on automatic Spelling Correction for Russian.pdf:pdf},
issn = {20757182},
journal = {Компьютерная Лингвистика и Интеллектуальные Технологии},
keywords = {Automatic methods for processing Russian,Automatic spelling correction,Language of social media,Spelling correction},
pages = {660--673},
title = {{SpellRuEval: The first competition on automatic spelling correction for Russian}},
year = {2016},
language={russian}
}
@article{Sorokin2016,
abstract = {This paper describes an automatic spelling correction system for Russian. The system utilizes information from different levels, using edit distance for candidate search and a combination of weighted edit distance and language model for candidate hypotheses selection. The hypotheses are then reranked by logistic regression using edit distance score, language model score etc. as features. We also experimented with morphological and semantic features but did not get any advantage. Our system has won the first SpellRuEval competition for Russian spell checkers by all the metrics and achieved F1-Measure of 75{\%}.},
author = {Сорокин, А. А. and Шаврина, Т. О.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/Automatic spelling correction for russian social media texts.pdf:pdf},
issn = {20757182},
journal = {Компьютерная Лингвистика и Интеллектуальные Технологии},
keywords = {исправление опечаток,автоматическое исправление опечаток,язык социальных медиа,нормализация текста,словарные опечатки},
pages = {688--701},
title = {{Автоматическое исправление опечаток и орфографических ошибок для русскоязычных социальных медиа}},
year = {2016},
language={russian}
}
@article{Panina2013,
abstract = {Анализируя ошибки в поисковых запросах нетрудно заметить, что большая часть из них имеет однозначное исправление, не зависящее от словарного окружения, и может быть исправлена в автоматическом режиме. В данной работе мы попытались выделить классы ошибок, которые можно исправлять автоматически, определить долю контекстно-независимых исправлений, и для выбранного множества ошибок разработать классификатор, позволяющий разделить исправления на надежные (пригодные для автоматической замены) и малонадежные (пригодные только для подсказки). В качестве кандидатов для исправлений были использованы подсказки поискового спелл-чекера, знакомые пользователям поисковых систем по сообщению «Возможно, вы имели в виду...». Для обучения классификатора были использованы лексические и статистические признаки словарного (бесконтекстного) уровня. Проведенные эксперименты показали высокую эффективность признаков и возможность настройки классификатора на заданный уровень точности. Применение предложенного},
author = {Панина, М. Ф. and Байтин, А. В. and Галинская, И. Е.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/Автоматическое исправление опечаток в поисковых запросах без учета контекста.pdf:pdf},
journal = {Компьютерная Лингвистика И Интеллектуальные Технологии: По Материалам Ежегодной Международной Конференции «Диалог»},
keywords = {autocorrection,confidence estimation,machine learning,query spelling correction,spellchecker,автоисправление опечаток,исправление опечаток в поисковых запросах,контекстно-независимые ошибки,машинное обучение,оценка надежности},
number = {12(19)},
pages = {556--567},
title = {{Автоматическое исправление опечаток в поисковых запросах без учета контекста}},
year = {2013},
language={russian}
}
@article{Shavrina2017,
abstract = {The "Taiga" project unites the corpus and the syntactic parser, being created in a new field of the corpus linguistics: the material obtained primarily meets the needs of machine learning, rather than linguistic search. The authors consider in detail the methodology for constructing the corpus, balance, volume and composition of its' segments, format and quality of tagging-which meets the current requirements for the development of tools for processing Russian language. Within the framework of the project, the creation of a large and open-source syntactic corpus in the Universal dependencies format is planned.},
author = {Шаврина, T. O. and Шаповалова, O.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/К МЕТОДИКЕ СОЗДАНИЯ КОРПУСОВ ДЛЯ МАШИННОГО ОБУЧЕНИЯ$\backslash$: «ТАЙГА», .pdf:pdf},
journal = {По материалам Международной Конференции <<Корпусная Лингвистика – 2017>>},
keywords = {corpus,corpus construction,machine learning,parsers for russian,representativity,syntax parsing,web corpus},
pages = {78--84},
title = {{К методике создания корпусов для машинного обучения: <<Тайга>>, синтаксический корпус и парсер}},
year = {2017},
language={russian}
}
@article{Piperski2013,
author = {Piperski, Alexander and Belikov, Vladimir and Kopylov, Nikolay and Morozov, Eugene and Selegey, Vladimir and Sharoff, Serge},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/Big and diverse is beautiful A large corpus of Russian to study linguistic variation.pdf:pdf},
journal = {8th Web ac Corpus Workshop},
pages = {24--28},
title = {{Big and diverse is beautiful: A large corpus of Russian to study linguistic variation}},
url = {http://blog.twitter.com/},
year = {2013},
language={english}
}
@article{Damerau1964,
abstract = {The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition. The unidentified input word is compared to the dictionary again, testing each time to see if the words match assuming one of these errors occurred. During a test run on garbled text, correct identifications were made for over 95 percent of these error types. {\textcopyright} 1964, ACM. All rights reserved.},
author = {Damerau, F.},
doi = {10.1145/363958.363994},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/A technique for computer detection and correction of spelling errors.pdf:pdf},
isbn = {0000000000},
issn = {15577317},
journal = {Communications of the ACM},
number = {3},
pages = {171--176},
title = {{A technique for computer detection and correction of spelling errors}},
volume = {7},
year = {1964},
language={english}
}
@article{Levenshtein1965,
author = {Левенштейн, В. И.},
journal = {Докл. АН СССР},
number = {4},
pages = {845--848},
title = {{Двоичные коды с~исправлением выпадений, вставок и замещений символов}},
volume = {163},
year = {1965},
language={russian}
}
@article{Kernighan1990,
abstract = {This paper describes a new program, correct, which takes words rejected by the Unix spell program, proposes a list of candidate corrections, and sorts them by probability. The probability scores are the novel contribution of this work. Probabilities are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in the speech recognition literature (Jelinek, 1985), one can often recover the intended correction, c, from a typo, t, by finding the correction c that maximizes Pr(c)Pr(tlc). The first factor, Pr(c), is a prior model of word probabilities; the second factor, Pr(tc), is a model of the noisy channel that accounts for spelling transformations on letter sequences (e.g., insertions, deletions, substitutions and reversals). Both sets of probabilities were trained on data collected from the Associated Press (AP) newswire. This text is ideally suited for this purpose since it contains a large number of typos (about two thousand per month).},
author = {Kernighan, Mark D. and Church, Kenneth W. and Gale, William A.},
doi = {10.3115/997939.997975},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/A Spelling Correction Program Based on a Noisy Channel Model.pdf:pdf},
title = {{A spelling correction program based on a noisy channel model}},
year = {1990},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/997939.997975},
booktitle = {Proceedings of the 13th Conference on Computational Linguistics - Volume 2},
pages = {205–210},
numpages = {6},
location = {Helsinki, Finland},
series = {COLING '90},
language={english}
}
@article{Mays1991,
title={{Context based spelling correction}},
author={Mays, Eric and Damerau, Fred J and Mercer, Robert L},
journal={Information Processing \& Management},
volume={27},
number={5},
pages={517--522},
year={1991},
publisher={Elsevier},
language={english}
}
@article{Oflazer1996,
abstract = {This paper presents the notion of error-tolerant recognition with finite-state recognizers along with results from some applications. Error-tolerant recognition enables the recognition of strings that deviate mildly from any string in the regular set recognized by the underlying finite-state recognizer Such recognition has applications to error-tolerant morphological processing, spelling correction and approximate string matching in information retrieval. After a description of the concepts and algorithms involved, we give examples from two applications: in the context of morphological analysis, error-tolerant recognition allows misspelled input word forms to be corrected and morphologically analyzed concurrently. We present an application of this to error-tolerant analysis of the agglutinative morphology of Turkish words. The algorithm can be applied to morphological analysis of any language whose morphology has been fully captured by a single (and possibly very large) finite-state transducer, regardless of the word formation processes and morpholographemic phenomena involved. In the context of spelling correction, error-tolerant recognition can be used to enumerate candidate correct forms from a given misspelled string within a certain edit distance. Error-tolerant recognition can be applied to spelling correction for any language, if (a) it has a word list comprising all inflected forms, or (b) its morphology has been fully described by a finite-state transducer. We present experimental results for spelling correction for a number of languages. These results indicate that such recognition works very efficiently for candidate generation in spelling correction for many European languages (English, Dutch, French, German, and Italian, among others) with very large word lists of root and inflected forms (some containing well over 200,000 forms), generating all candidate solutions within 10 to 45 milliseconds (with an edit distance of 1) on a SPARCStation 10/41. For spelling correction in Turkish, error-tolerant recognition operating with a (circular) recognizer of Turkish words (with about 29,000 states and 119,000 transitions) can generate all candidate words in less than 20 milliseconds, with an edit distance of 1.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9504031},
author = {Oflazer, Kemal},
doi = {10.1184/r1/6287645.v1},
eprint = {9504031},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Error-tolerant finite-state recognition with applications.pdf:pdf},
issn = {08912017},
journal = {Computational Linguistics},
number = {1},
pages = {69--89},
primaryClass = {cmp-lg},
title = {{Error-tolerant Finite-state Recognition with Applications to Morphological Analysis and Spelling Correction}},
volume = {22},
year = {1996},
language={english}
}
@article{Golding1996,
abstract = {This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as peace and piece, or quiet and quite) and also the problem of correcting particular word usage errors (such as amount and number, or among and between). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance.},
author = {Golding, Andrew R. and Schabes, Yves},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Combining trigram-based and feature-based methods for context-sensitive spelling correction.pdf:pdf},
mendeley-groups = {ML/NLP},
pages = {71--78},
title = {{Combining Trigram-based and feature-based methods for context-sensitive spelling correction}},
year = {1996},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/981863.981873},
doi = {10.3115/981863.981873},
language={english}
}
@article{Flor2012,
abstract = {This paper presents an investigation on using four types of contextual information for improving the accuracy of automatic correction of single-token non-word misspellings. The task is framed as contextually-informed re-ranking of correction candidates. Immediate local context is captured by word n-grams statistics from a Web-scale language model. The second approach measures how well a candidate correction fits in the semantic fabric of the local lexical neighborhood, using a very large Distributional Semantic Model. In the third approach, recognizing a misspelling as an instance of a recurring word can be useful for re-ranking. The fourth approach looks at context beyond the text itself. If the approximate topic can be known in advance, spelling correction can be biased towards the topic. Effectiveness of proposed methods is demonstrated with an annotated corpus of 3,000 student essays from international high-stakes English language assessments. The paper also describes an implemented system that achieves high accuracy on this task.},
author = {Flor, Michael},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Four types of context for automatic spelling correction.pdf:pdf},
issn = {12489433},
journal = {TAL Traitement Automatique des Langues},
keywords = {Automatic spelling correction,Context,Language models,N-grams},
number = {3},
pages = {61--99},
title = {{Four types of context for automatic spelling correction}},
volume = {53},
year = {2012},
language={english}
}
@article{Bajtin2008,
author = {Байтин, А.},
journal = {Российские Интернет Технологии},
title = {{Исправление поисковых запросов в Яндексе}},
year = {2008},
language={russian}
}
@article{Dempster1977,
abstract = {A series solution of the general three-dimensional equations of linear elasticity is used to find accurate natural frequencies and mode shapes for the flexural vibrations of thick free circular plates. The approximate solution for thick plates, which includes shear and rotary inertia effects, is compared with the accurate series solution. It is found that the approximate solution yields frequencies of sufficient accuracy for most engineering applications within the range of applicability of the approximate theory. {\textcopyright} 1979 by ASME.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/j.2517-6161.1977.tb01600.x},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Maximum Likelihood from Incomplete Data Via the EM Algorithm.pdf:pdf},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
mendeley-groups = {Bachelor Research},
number = {1},
pages = {1--22},
title = {{Maximum Likelihood from Incomplete Data Via the EM Algorithm}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1977.tb01600.x},
volume = {39},
year = {1977},
language={english}
}
@article{Brill2000,
abstract = {The noisy channel model has been applied to a wide range of problems, including spelling correction. These models consist of two components: a source model and a channel model. Very little research has gone into improving the channel model for spelling correction. This paper describes a new channel model for spelling correction, based on generic string to string edits. Using this model gives significant performance improvements compared to previously proposed models.},
author = {Brill, Eric and Moore, Robert C.},
doi = {10.3115/1075218.1075255},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/An Improved Error Model for Noisy Channel Spelling Correction.pdf:pdf},
number = {Kukich 1992},
pages = {286--293},
title = {{An improved error model for noisy channel spelling correction}},
year = {2000},
booktitle = {Proceedings of the 38th Annual Meeting on Association for Computational Linguistics},
pages = {286–293},
numpages = {8},
location = {Hong Kong},
series = {ACL '00},
language={english}
}
@article{Gage1994,
abstract = {Data compression is becoming increasingly important as a way to stretch disk space and speed up data transfers. This article describes a simple general-purpose data compression algorithm, called Byte Pair Encoding (BPE), which provides almost as much compression as the popular Lempel, Ziv, and Welch (LZW) method [3, 2]. (I mention the LZW method in particular because it delivers good overall performance and is widely used.) BPE's compression speed is somewhat slower than LZW's, but BPE's expansion is faster. The main advantage of BPE is the small, fast expansion routine, ideal for applications with limited memory. The accompanying C code provides an efficient implementation of the algorithm.},
author = {Gage, Philip},
doi = {10.5555/177910.177914.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/A new algorithm for data compression.pdf:pdf},
journal = {The C Users Journal},
pages = {1--14},
title = {{A New Algorithm for Data Compression}},
url = {https://www.derczynski.com/papers/archive/BPE{\_}Gage.pdf},
year = {1994},
language={english}
}
@article{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:home/mrgeekman/Documents/MIPT/NLP/Lections/11/BERT.pdf:pdf},
isbn = {9781950737130},
journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
mendeley-groups = {ML/NLP},
number = {Mlm},
pages = {4171--4186},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
volume = {1},
year = {2019},
language={english}
}
@article{Burtsev2015,
abstract = {Adoption of messaging communication and voice assistants has grown rapidly in the last years. This creates a demand for tools that speed up prototyping of featurerich dialogue systems. An open-source library DeepPavlov is tailored for development of conversational agents. The library prioritises efficiency, modularity, and extensibility with the goal to make it easier to develop dialogue systems from scratch and with limited data available. It supports modular as well as end-to-end approaches to implementation of conversational agents. Conversational agent consists of skills and every skill can be decomposed into components. Components are usually models which solve typical NLP tasks such as intent classification, named entity recognition or pre-trained word vectors. Sequence-to-sequence chit-chat skill, question answering skill or task-oriented skill can be assembled from components provided in the library.},
author = {Burtsev, Mikhail and Seliverstov, Alexander and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yuri and Kuznetsov, Denis and Litinsky, Alexey and Logacheva, Varvara and Lymar, Alexey and Malykh, Valentin and Petrov, Maxim and Polulyakh, Vadim and Pugachev, Leonid and Sorokin, Alexey and Vikhreva, Maria and Zaynutdinov, Marat},
doi = {10.18653/v1/p18-4021},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Deeppavlov Open-source library for dialogue systems.pdf:pdf},
isbn = {9781948087650},
journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of System Demonstrations},
pages = {122--127},
title = {{DeepPavlov: Open-Source library for dialogue systems}},
year = {2015},
language={english}
}
@article{Hart1968,
abstract = {Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.},
author = {Hart, Peter and Nilsson, Nils and Raphael, Bertram},
doi = {10.1109/TSSC.1968.300136},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/A formal basis for the heuristic determination of minimum cost paths.pdf:pdf},
issn = {0536-1567},
journal = {IEEE Transactions on Systems Science and Cybernetics},
number = {2},
pages = {100--107},
title = {{A Formal Basis for the Heuristic Determination of Minimum Cost Paths}},
url = {http://ieeexplore.ieee.org/document/4082128/},
volume = {4},
year = {1968},
language={english}
}
@article{Hulden2009,
author = {Hulden, Mans},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Fast approximate string matching with finite automata.pdf:pdf},
issn = {1989-7553},
journal = {Procesamiento del Lenguaje Natural},
keywords = {Algorithm,Approximate string matching,BK tree,Bitap algorithm,Boyer–Moore string search algorithm,Commentz-Walter algorithm,Computer science,Finite-state machine,String metric,String searching algorithm,Theoretical computer science},
number = {43},
pages = {57--64},
title = {{Fast approximate string matching with finite automata}},
volume = {43},
year = {2009},
language={english}
}
@article{Vichovanetch2018,
title={Обзор алгоритмов фонетического кодирования},
author={Выхованец, В. С. and Цзяньмин, Ду and Сакулин, С. А.},
journal={Управление большими системами: сборник трудов},
number={73},
year={2018},
language={russian}
}
@misc{Russell1917,
title={U.S. Patent 1 261 167A, Index},
author={Russell, R. C.},
year={1917},
language={english}
}
@misc{Russell1922,
title={U.S. Patent 1 435 663, Index},
author={Russell, R. C.},
year={1921},
language={english}
}
@article{Philips2000,
title={{The double metaphone search algorithm}},
author={Philips, Lawrence},
journal={C/C++ users journal},
volume={18},
number={6},
pages={38--43},
year={2000},
publisher={CMP Media, Inc.},
language={english}
}
@article{Heafield2013,
abstract = {We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7{\%} of the RAM and 14.0{\%} of the wall time taken by SRILM. The code is open source as part of KenLM. {\textcopyright} 2013 Association for Computational Linguistics.},
author = {Heafield, Kenneth and Pouzyrevsky, Ivan and Clark, Jonathan H. and Koehn, Philipp},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Scalable Modified Kneser-Ney Language Model Estimation.pdf:pdf},
isbn = {9781937284510},
journal = {ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
mendeley-groups = {ML/NLP},
pages = {690--696},
title = {{Scalable modified Kneser-Ney language model estimation}},
volume = {2},
year = {2013},
language={english}
}
@article{Heafield2011,
abstract = {We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is de signed for speed. Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57{\%} of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations.},
author = {Heafield, Kenneth},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/KenLM Faster and Smaller Language Model Queries.pdf:pdf},
isbn = {978-1-937284-12-1},
journal = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
mendeley-groups = {ML/NLP},
number = {2009},
pages = {187--197},
title = {{KenLM : Faster and Smaller Language Model Queries}},
url = {http://www.aclweb.org/anthology/W11-2123{\%}5Cnhttp://kheafield.com/code/kenlm},
year = {2011},
language={english}
}
@inproceedings{Chen1996,
address = {Morristown, NJ, USA},
author = {Chen, Stanley F. and Goodman, Joshua},
booktitle = {Proceedings of the 34th annual meeting on Association for Computational Linguistics -},
doi = {10.3115/981863.981904},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/An Empirical Study of Smoothing Techniques for Language Models.pdf:pdf},
isbn = {9781626239777},
pages = {310--318},
publisher = {Association for Computational Linguistics},
title = {{An empirical study of smoothing techniques for language modeling}},
url = {http://portal.acm.org/citation.cfm?doid=981863.981904},
volume = {213},
year = {1996},
language={english}
}
@article{Kuratov2019,
abstract = {The paper introduces methods of adaptation of multilingual masked language models for a specific language. Pre-trained bidirectional language models show state-of-the-art performance on a wide range of tasks including reading comprehension, natural language inference, and sentiment analysis. At the moment there are two alternative approaches to train such models: monolingual and multilingual. While language specific models show superior performance, multilingual models allow to perform a transfer from one language to another and solve tasks for different languages simultaneously. This work shows that transfer learning from a multilingual model to monolingual model results in significant growth of performance on such tasks as reading comprehension, paraphrase detection, and sentiment analysis. Furthermore, multilingual initialization of monolingual model substantially reduces training time. Pre-trained models for the Russian language are open sourced.},
archivePrefix = {arXiv},
arxivId = {1905.07213},
author = {Kuratov, Yuri and Arkhipov, Mikhail},
eprint = {1905.07213},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Русский язык/Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Adaptation of deep bidirectional multilingual transformers for Russian language}},
year = {2019},
language={english}
}
@article{Joachims2002,
abstract = {This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.},
author = {Joachims, Thorsten},
doi = {10.1145/775047.775067},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Optimizing Search Engines using Clickthrough Data.pdf:pdf},
isbn = {158113567X},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {133--142},
title = {{Optimizing search engines using clickthrough data}},
year = {2002},
language={english}
}
@article{Boser1992,
abstract = {A training algorithm that maximizes the mar- gin between the training patterns and the de- cision boundary is presented. The technique is applicable to a wide variety of classifiac- tion functions, including Perceptions, polyno- mials, and Radial Basis Functions. The ef- fective number of parameters is adjusted auto- matically to match the complexity of the prob- lem. The solution is expressed as a linear com- bination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the general- ization performance based on the leave-one-out method and the VC-dimension are given. Ex- perimental results on optical character recog- nition problems demonstrate the good gener- alization obtained when compared with other learning algorithms.},
author = {Boser, Bernhard E. and Vapnik, Vladimir N. and Guyon, Isabelle M.},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/A training algorithm for optimal margin classifiers.pdf:pdf},
journal = {Perception},
pages = {144--152},
title = {{Training Algorithm Margin for Optimal Classifiers}},
year = {1992},
language={english}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and M{\"{u}}ller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.5555/1953048},
eprint = {1201.0490},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Scikit-learn Machine learning in Python.pdf:pdf},
journal = {Environmental Health Perspectives},
month = {jan},
number = {9},
pages = {097008},
title = {{Scikit-learn: Machine Learning in Python}},
url = {https://ehp.niehs.nih.gov/doi/10.1289/EHP4713 http://arxiv.org/abs/1201.0490},
volume = {127},
year = {2012},
language={english}
}
@article{Riordan2019,
abstract = {Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding character representations to word-based models and test these hypotheses on large-scale real world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAP-SAS content scoring dataset.},
author = {Riordan, Brian and Flor, Michael and Pugh, Robert},
doi = {10.18653/v1/w19-4411},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/How to account formispellings$\backslash$:Quantifying the benefit of character representations in neural contentscoring models.pdf:pdf},
mendeley-groups = {Bachelor Research},
pages = {116--126},
title = {{How to account for mispellings: Quantifying the benefit of character representations in neural content scoring models}},
year = {2019},
language={english}
}
@article{Kumar2020,
abstract = {Owing to the phenomenal success of BERT on various NLP tasks and benchmark datasets, industry practitioners are actively experimenting with fine-tuning BERT to build NLP applications for solving industry use cases. For most datasets that are used by practitioners to build industrial NLP applications, it is hard to guarantee absence of any noise in the data. While BERT has performed exceedingly well for transferring the learnings from one use case to another, it remains unclear how BERT performs when fine-tuned on noisy text. In this work, we explore the sensitivity of BERT to noise in the data. We work with most commonly occurring noise (spelling mistakes, typos) and show that this results in significant degradation in the performance of BERT. We present experimental results to show that BERT's performance on fundamental NLP tasks like sentiment analysis and textual similarity drops significantly in the presence of (simulated) noise on benchmark datasets viz. IMDB Movie Review, STS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline that are responsible for this drop in performance. Our findings suggest that practitioners need to be vary of presence of noise in their datasets while fine-tuning BERT to solve industry use cases.},
archivePrefix = {arXiv},
arxivId = {2003.12932},
author = {Kumar, Ankit and Makhija, Piyush and Gupta, Anuj},
doi = {10.18653/v1/2020.wnut-1.3},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Noisy Text Data$\backslash$: Achilles' Heel of BERT.pdf:pdf},
mendeley-groups = {Bachelor Research},
pages = {16--21},
title = {{Noisy Text Data: Achilles' Heel of BERT}},
year = {2020},
language={english}
}
@article{Dorogush2018,
abstract = {In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are significantly faster than other gradient boosting libraries on ensembles of similar sizes.},
archivePrefix = {arXiv},
arxivId = {1810.11363},
author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
eprint = {1810.11363},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/CatBoost gradient boosting with categorical features support.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--7},
title = {{CatBoost: Gradient boosting with categorical features support}},
year = {2018},
language={english}
}
@article{Brown2020,
abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
archivePrefix = {arXiv},
arxivId = {2005.14165},
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
eprint = {2005.14165},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Language Models are Few-Shot Learners.pdf:pdf},
issn = {23318422},
journal = {arXiv},
mendeley-groups = {ML/NLP},
title = {{Language models are few-shot learners}},
year = {2020},
language={english}
}
@article{Gulin2011,
abstract = {The problem of ranking the documents according to their relevance to a given query is a hot topic in information retrieval. Most learning-to-rank methods are supervised and use human editor judgements for learning. In this paper, we introduce novel pairwise method called YetiRank that modifies Friedman's gradient boosting method in part of gradient computation for optimization and takes uncertainty in human judgements into account. Proposed enhancements allowed YetiRank to outperform many state-of-the-art learning to rank methods in offline experiments as well as take the first place in the second track of the Yahoo! learning-to-rank contest. Even more remarkably, the first result in the learning to rank competition that consisted of a transfer learning task was achieved without ever relying on the bigger data from the " transfer-from " domain.},
author = {Gulin, Andrey and Kuralenok, Igor and Pavlov, Dmitry},
file = {:home/mrgeekman/Documents/MIPT/НИР/Papers/Winning The Transfer Learning Track of Yahoo!'s LearningTo Rank Challenge with YetiRank.pdf:pdf},
journal = {JMLR Workshop},
keywords = {gradient boosting,ir evaluation,learning to rank},
pages = {63--76},
title = {{Winning The Transfer Learning Track of Yahoo!'s Learning To Rank Challenge with YetiRank}},
url = {http://download.yandex.vibrow.com/company/to{\_}rank{\_}challenge{\_}with{\_}yetirank.pdf},
volume = {14},
year = {2011},
language={english}
}
@online{Hagen,
author = "Хаген, М.",
title = "Полная парадигма Русского языка",
url  = "http://www.lingvodics.com/dics/details/4309/",
language={russian}
}
@online{Wiktionary,
title = "Викисловарь",
url = "https://ru.wiktionary.org",
language={russian}
}
@online{CatBoostDocs,
title = "CatBoost is a high-performance open source library for gradient boosting on decision trees",
url  = "https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus",
language={english}
}
@online{DeepPavlovTable,
title = "Automatic spelling correction pipelines",
url  = "http://docs.deeppavlov.ai/en/master/features/models/spelling_correction.html",
language={english}
}
@book{GrammarErrorDefinition,
title={{Новый словарь методических терминов и понятий (теория и практика обучения языкам)}},
author = {Азимов, Э. Г. and Щукин, А. Н.},
year = {2009},
location = {М.},
publisher = {ИКАР},
pages={53},
language={russian}
}