{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Демонстрация Spell Checker\n",
    "\n",
    "В этом ноутбуке будет произведена демонстрация написанной модели на примере нескольких предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:14:21.429030Z",
     "start_time": "2021-01-21T20:14:21.404158Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:14:24.462915Z",
     "start_time": "2021-01-21T20:14:21.432070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from string import punctuation\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "\n",
    "from deeppavlov.models.spelling_correction.levenshtein import (\n",
    "    LevenshteinSearcherComponent\n",
    ")\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "import kenlm\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer, MosesPunctNormalizer\n",
    "\n",
    "from src.models.SpellChecker import *\n",
    "from src.models.BertScorer.bert_scorer_correction import (\n",
    "    BertScorerCorrection\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:14:24.506725Z",
     "start_time": "2021-01-21T20:14:24.467250Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с того, что инициализируем все необходимые компоненты модели. Параллельно так же будет описана роль каждого компонента в системе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer/Detokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот компонент отвечает за токенизаци/детокенизацию исходного предложения. В качестве основы было решено взять токенизатор из библиотеки [sacremoses](https://github.com/alvations/sacremoses).\n",
    "\n",
    "Опция `escape=False` установлена для корректной работы удаления пунктуации, иначе, например, пунктуационный символ `\"` заменяется на `&quot;` и его не удается отследить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:14:24.589898Z",
     "start_time": "2021-01-21T20:14:24.558706Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_tokenizer = MosesTokenizer(lang='ru')\n",
    "raw_detokenizer = MosesDetokenizer(lang='ru')\n",
    "tokenizer = lambda x: raw_tokenizer.tokenize(x, escape=False)\n",
    "detokenizer = lambda x: raw_detokenizer.detokenize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот компонент отвечает за то, чтобы генерировать кандидатов для каждой позиции токенизированного предложения. \n",
    "\n",
    "На данный момент берутся слова из словаря на заданном расстоянии Дамерау-Левенштейна от исходного токена. Иногда эти слова еще разбиваются пробелами.\n",
    "\n",
    "В качестве словаря был взят [этот](https://github.com/danakt/russian-words/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:16:39.778197Z",
     "start_time": "2021-01-21T20:14:24.592216Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 23:14:24.624 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/mrgeekman/Documents/MIPT/НИР/Repo/data/external/russian_words/russian_words_vocab.dict]\n"
     ]
    }
   ],
   "source": [
    "vocab_path = os.path.join(DATA_PATH, 'external', 'russian_words', \n",
    "                          'russian_words_vocab.dict')\n",
    "vocab = SimpleVocabulary(load_path=vocab_path, save_path=vocab_path)\n",
    "levenshtein_searcher_component = LevenshteinSearcherComponent(\n",
    "    words=vocab.keys(), max_distance=1\n",
    ")\n",
    "candidate_generator = LevenshteinCandidateGenerator(\n",
    "    levenshtein_searcher_component\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот компонент отвечает за нахождение оптимальной позиции для замены и предварительный выбор кандидатов для этой позиции.\n",
    "\n",
    "На данный момент берутся две языковые модели: слева-направо и справа-налево, которые работают с текстом без пунктуации. Для каждой позиции по просматривается весь список кандидатов при использовании левого и правого контекстов и подсчитывается log-prob. Если токен состоит из нескольких подтокенов (например, если в слове есть пробел), то скор суммируется.\n",
    "\n",
    "Так как для каждого кандидата имеется два log-prob их результат аггрегируется. На данный момент от них считается среднее гармоническое. \n",
    "\n",
    "Мы также знаем какой из кандидатов в рассматриваемой позиции соответствует текущему токену, что позволяет нам найти разницу между его скором и скором другого кандидата. Так как мы оперируем с log-prob, то это можно понимать, как логарифм того, насколько изменится вероятность при рассмотрении нового токена вместо изначального.\n",
    "\n",
    "Выбранная позиция для изменения -- та, на которой достигается наибольшая разница скоров между текущим токеном и одним из кандидатов. На основе разницы скоров так же отбирается предварительный список кандидатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:16:53.559046Z",
     "start_time": "2021-01-21T20:16:39.781670Z"
    }
   },
   "outputs": [],
   "source": [
    "model_left_right = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'left_right_3_100.arpa.binary')\n",
    ")\n",
    "model_right_left = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'right_left_3_100.arpa.binary')\n",
    ")\n",
    "position_selector = KenlmPositionSelector(model_left_right, model_right_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот компонент отвечает за остановку итераций исправления.\n",
    "\n",
    "На данный момент рассматривается разница между скорами текущего токена и предварительно лучшего кандидата в выбранной позиции и сравнивается с константой $\\alpha$. Так как мы в position selector работали с log-prob, то это можно интерпретировать, как увеличение вероятности в $\\exp{\\alpha}$ раз. Если это увеличение не превышает заданной константы, то работа алгоритма завершается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:16:53.752350Z",
     "start_time": "2021-01-21T20:16:53.569228Z"
    }
   },
   "outputs": [],
   "source": [
    "margin_constant = np.log(2)\n",
    "stopping_criteria = MarginStoppingCriteria(margin_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот компонент отвечает за выбор наилучшего кандидата из списка предложенных position selector.\n",
    "\n",
    "На данный момент берется модель [Conversational RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html). В первую очередь токенизируется (WordPiece) исходное предложение с MASK-токеном на месте замяемого токена. Хочется просто запустить Masked Language Modeling и попробовать поподставлять кандидатов вместо MASK, но проблема в том, что некоторые кандидаты состоят из более, чем одного токена. В таком случае мы токенизируем всех кандидатов и пытаемся двигать MASK-токен по каждой позиции внутри него, делая другие позиции UNK-токеном и отключая для них attention (не обязательно ставить UNK-токен, это было сделано для удобства). Для аггрегации log-prob скоров внутри одного кандидата берется среднее.\n",
    "\n",
    "В результате отбирается кандидат с наилучшим скором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:17:08.467704Z",
     "start_time": "2021-01-21T20:16:53.754805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/mrgeekman/Documents/MIPT/НИР/Repo/notebooks/../models/conversational_rubert/pytorch_model.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at /home/mrgeekman/Documents/MIPT/НИР/Repo/notebooks/../models/conversational_rubert/pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "BERT_PATH = os.path.join(MODEL_PATH, 'conversational_rubert')\n",
    "config = BertConfig.from_json_file(\n",
    "    os.path.join(BERT_PATH, 'bert_config.json')\n",
    ")\n",
    "model = BertForMaskedLM.from_pretrained(\n",
    "    os.path.join(BERT_PATH, 'pytorch_model.bin'),\n",
    "    config=config\n",
    ")\n",
    "bert_tokenizer = BertTokenizer(os.path.join(BERT_PATH, 'vocab.txt'))\n",
    "scorer_basis = BertScorerCorrection(model, bert_tokenizer)\n",
    "agg_subtoken_func = np.mean\n",
    "candidate_scorer = BertCandidateScorer(scorer_basis, agg_subtoken_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше были описаны все компоненты модели. Итого, имеем этапы:\n",
    "1. Генерация кандидатов\n",
    "2. Итерации до тех пор, пока не сработает критерий останова или не исчерпается максимальное количество итераций\n",
    "    * Поиск лучшей позиции для исправления и отбор кандидатов для исправления\n",
    "    * Проверка критерия останова\n",
    "    * Выбор лучшего исправления\n",
    "    * Исправление текущего предложения\n",
    "    * Возможное вынесение некоторых позиций из рассмотрения position selector на следующей итерации.\n",
    "    \n",
    "В последнем пункте упомянут механизм вынесения некоторых позиций из рассмотрения. Опишем его более подробно. Дело в том, что иногда position selector находит много мест для возможных исправлений, но в итоге выбирает не самое удачное, на котором candidate scorer выбирает тот токен, который уже там стоит. Это значит, что на следующей итерации ничего не изменится и итерация в точности повторится. Чтобы такого не происходило такие неудачные позиции перестают рассматриваться в position selector до тех пор, пока хоть какая-то коррекция не будет произведена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:17:08.513471Z",
     "start_time": "2021-01-21T20:17:08.470397Z"
    }
   },
   "outputs": [],
   "source": [
    "# количество кандидатов, отбираемых position selector\n",
    "num_selected_candidates = 16\n",
    "# максимальное количество итераций\n",
    "max_it = 5\n",
    "\n",
    "spellchecker = IterativeSpellChecker(\n",
    "    candidate_generator,\n",
    "    position_selector,\n",
    "    candidate_scorer,\n",
    "    stopping_criteria,\n",
    "    tokenizer,\n",
    "    detokenizer,\n",
    "    num_selected_candidates,\n",
    "    max_it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Демонстрация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем 20 случайных предложений из обучающей части датасета вместе с ответами и посмотрим на результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:17:08.562718Z",
     "start_time": "2021-01-21T20:17:08.515938Z"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 20\n",
    "\n",
    "with open(\n",
    "    os.path.join(DATA_PATH, 'external', 'spell_ru_eval', 'train_source.txt'), \n",
    "    'r'\n",
    ") as inf:\n",
    "    all_sentences = inf.readlines()\n",
    "    \n",
    "with open(\n",
    "    os.path.join(DATA_PATH, 'external', 'spell_ru_eval', \n",
    "                 'train_corrected.txt'), \n",
    "    'r'\n",
    ") as inf:\n",
    "    all_corrected_sentences = inf.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:17:08.612062Z",
     "start_time": "2021-01-21T20:17:08.564627Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "num_sentences = len(all_sentences)\n",
    "all_indices = np.arange(num_sentences)\n",
    "np.random.shuffle(all_indices)\n",
    "indices = all_indices[:num_examples]\n",
    "\n",
    "examples = [all_sentences[idx].strip() for idx in indices]\n",
    "examples_true = [all_corrected_sentences[idx].strip() for idx in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим наш spell checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:17:42.021568Z",
     "start_time": "2021-01-21T20:17:08.614506Z"
    }
   },
   "outputs": [],
   "source": [
    "examples_corrected = spellchecker(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T20:17:42.093628Z",
     "start_time": "2021-01-21T20:17:42.024255Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Original:\tлюблю ужастики, но не смотрю теперь итак темноты боюсь, а если посмотрю потом ваще спать не могу )\n",
      "Corrected:\tлюблю ужастики но не смотрю теперь и так темноты боюсь а если посмотрю потом ваще спать не могу\n",
      "True:\t\tлюблю ужастики но не смотрю теперь и так темноты боюсь а если посмотрю потом вообще спать не могу\n",
      "\n",
      "2\n",
      "Original:\tЭтот телевизионный коктейль смешан таким образом что с утра нам вдалбливают о проблемах со здоровьем, потом о проблемах на планете, а тут еще и свои запары вдобавок.\n",
      "Corrected:\tэтот телевизионный коктейль смешан таким образом что с утра нам вдалбливают о проблемах со здоровьем потом о проблемах на планете а тут еще и свои запасы вдобавок\n",
      "True:\t\tЭтот телевизионный коктейль смешан таким образом что с утра нам вдалбливают о проблемах со здоровьем потом о проблемах на планете а тут еще и свои запары вдобавок\n",
      "\n",
      "3\n",
      "Original:\t\" Ты не видела, где мои носки?\n",
      "Corrected:\tты не видела где мои носки\n",
      "True:\t\tТы не видела где мои носки\n",
      "\n",
      "4\n",
      "Original:\tНо став старше, она осознала, что точность и четкость - ее все.\n",
      "Corrected:\tно став старше она осознала что точность и четкость не все\n",
      "True:\t\tНо став старше она осознала что точность и четкость ее все\n",
      "\n",
      "5\n",
      "Original:\tПользоваццо сервисом проще простогО!\n",
      "Corrected:\tпользоваццо сервисом проще простого\n",
      "True:\t\tПользоваться сервисом проще простого\n",
      "\n",
      "6\n",
      "Original:\tПлавать на каяке в одиночку не рекомендуется.\n",
      "Corrected:\tплавать на каяке в одиночку не рекомендуется\n",
      "True:\t\tПлавать на каяке в одиночку не рекомендуется\n",
      "\n",
      "7\n",
      "Original:\tнекотрые ничево не соображали вообще с небольшой затратой време ни удалось доиграть...\n",
      "Corrected:\tнекоторые ничего не соображали вообще с небольшой затратой време ни удалось дои рать\n",
      "True:\t\tнекоторые ничего не соображали вообще с небольшой затратой времени удалось доиграть\n",
      "\n",
      "8\n",
      "Original:\tОтвественный редактор издания, Юлия Потемкина, прислала мне потрясающий ролик про триатлон.\n",
      "Corrected:\tотвественный редактор издания юлия потемкина прислала мне потрясающий ролик про триатлон\n",
      "True:\t\tОтветственный редактор издания Юлия Потемкина прислала мне потрясающий ролик про триатлон\n",
      "\n",
      "9\n",
      "Original:\tНа следующей неделе в четверг 11.08.11 вечером собираюсь в Магнитогоск своим ходом.\n",
      "Corrected:\tна следующей неделе в четверг 11.08.11 вечером собираюсь в магнитогоск своим ходом\n",
      "True:\t\tНа следующей неделе в четверг 11.08.11 вечером собираюсь в Магнитогоск своим ходом\n",
      "\n",
      "10\n",
      "Original:\tХотя странно, когда я забирала к себе на выходные старого кота, который живет у родителей, да и собаку в придачу, то такого концерта мой кот не устраивал.\n",
      "Corrected:\tхотя странно когда я забирала к себе на выходные старого кота который живет у родителей да и собаку в придачу то такого концерта мой кот не устраивал\n",
      "True:\t\tХотя странно когда я забирала к себе на выходные старого кота который живет у родителей да и собаку в придачу то такого концерта мой кот не устраивал\n",
      "\n",
      "11\n",
      "Original:\tа я в метро без книжек не умею.\n",
      "Corrected:\tа я в метро без книжек не умею\n",
      "True:\t\tа я в метро без книжек не умею\n",
      "\n",
      "12\n",
      "Original:\tи наверное хороше, что я там был один...\n",
      "Corrected:\tи наверное хорошо что я там был один\n",
      "True:\t\tи наверное хорошо что я там был один\n",
      "\n",
      "13\n",
      "Original:\tтебе можно и нужно его продавать!!!\n",
      "Corrected:\tтебе можно и нужно его продавать\n",
      "True:\t\tтебе можно и нужно его продавать\n",
      "\n",
      "14\n",
      "Original:\tДети растут ооочень быстро, на прошлой фотке она совсем младенчиком была )\n",
      "Corrected:\tдети растут ооочень быстро на прошлой фотке она совсем младенчиком была\n",
      "True:\t\tДети растут очень быстро на прошлой фотке она совсем младенчиком была\n",
      "\n",
      "15\n",
      "Original:\tэто вобщем жуткий курс о особеностях развития психики детей и взрослых в условиях \" стесненного \" или нарушенного функционирования организма.\n",
      "Corrected:\tэто в общем жуткий курс об особенностях развития психики детей и взрослых в условиях стесненного или нарушенного функционирования организма\n",
      "True:\t\tэто в общем жуткий курс об особенностях развития психики детей и взрослых в условиях стесненного или нарушенного функционирования организма\n",
      "\n",
      "16\n",
      "Original:\tОслуживание безупречноне, а цена - как раз такая, что вы ищите.\n",
      "Corrected:\tобслуживание безупречное а цена как раз такая что вы ищите\n",
      "True:\t\tОслуживание безупречное а цена как раз такая что вы ищете\n",
      "\n",
      "17\n",
      "Original:\tЕй в общем-то все равно было, на кого учиться, лишь бы мама с папой не переживали, что у них девочка не пристроена.\n",
      "Corrected:\tей в общем-то все равно было на кого учиться лишь бы мама с папой не переживали что у них девочка не пристроена\n",
      "True:\t\tЕй в общем-то все равно было на кого учиться лишь бы мама с папой не переживали что у них девочка не пристроена\n",
      "\n",
      "18\n",
      "Original:\tПозаввчера пересматривал \" Горбатую гору \" и в памяти всплыло сразу 2 человека...\n",
      "Corrected:\tпозавчера пересматривал горбатую гору и в памяти всплыло сразу 2 человека\n",
      "True:\t\tПозавчера пересматривал Горбатую гору и в памяти всплыло сразу 2 человека\n",
      "\n",
      "19\n",
      "Original:\tМужчинаи женщина не могут жить друг без друга, но частенько случается, что идруг с другом им становится тяжеловато.\n",
      "Corrected:\tмужчина и женщина не могут жить друг без друга но частенько случается что друг с другом им становится тяжеловато\n",
      "True:\t\tМужчина и женщина не могут жить друг без друга но частенько случается что и друг с другом им становится тяжеловато\n",
      "\n",
      "20\n",
      "Original:\tМне только кажется, потому что я в них совсем не разбираюсь.\n",
      "Corrected:\tмне только кажется потому что я в них совсем не разбираюсь\n",
      "True:\t\tМне только кажется потому что я в них совсем не разбираюсь\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_examples):\n",
    "    print(i+1)\n",
    "    print(f'Original:\\t{examples[i]}')\n",
    "    print(f'Corrected:\\t{examples_corrected[i]}')\n",
    "    print(f'True:\\t\\t{examples_true[i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем каждый пример по-отдельности и проанализируем ошибки:\n",
    "1. $\\pm$ \n",
    "    * Корректно исправлено \"и так\" на \"итак\".\n",
    "    * Не исправлено \"ваще\" на \"вообще\". Большая разница в расстоянии Дамера-Левенштейна.\n",
    "2. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "3. $+$ \n",
    "    * Нет ошибок, нет исправлений.\n",
    "4. $-$\n",
    "    * Без нужды исправлено \"не\" на \"ее\".\n",
    "5. $-$\n",
    "    * Не исправлено \"пользоваццо\" на \"пользоваться\". Большая разница в расстоянии Дамерау-Левенштейна.\n",
    "6. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "7. $\\pm$\n",
    "    * Корректно исправлено \"некотрые\" на \"некоторые\".\n",
    "    * Корректно исправлено \"ничево\" на \"ничего\".\n",
    "    * Не исправлено \"време ни\" на \"времени\". На данный момент нет механизма по объединению слов, разбитых по пробелу, поэтому, возможно, произошла такая странная аномалия.\n",
    "    * Без нужды исправлено \"доиграть\" на \"дои рать\".\n",
    "8. $-$\n",
    "    * Не исправлено \"отвественный\" на \"ответственный\". Ошибка произошла в candidate scorer. Кандидат \"отвественный\" состоял из четырех WordPiece-токенов, а \"ответственный\" из двух. Так оказалось, что средний скор токенов второго оказался меньше, чем средний скор токенов первого. Если бы аггрегуриющая функция по сабтокенам была суммой, то победил бы кандидат \"ответственный\".\n",
    "9. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "    * Я не нашел информацию про то есть ли город Магнитогоск, поэтому, возможно, ошибка в оригинальном датасете.\n",
    "10. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "11. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "12. $+$\n",
    "    * Корректно исправлено \"хороше\" на \"хорошо\"\n",
    "13. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "14. $-$\n",
    "    * Не исправлено \"ооочень\" на \"очень\". Большая разница в расстоянии Дамерау-Левенштейна.\n",
    "15. $+$\n",
    "    * Корректно исправлено \"вобщем\" на \"в общем\". \n",
    "    * Корректно исправлено \"о\" на \"об\".\n",
    "    * Корректно исправлено \"особеностях\" на \"особенностях\".\n",
    "16. $\\pm$\n",
    "    * Без нужды исправлено \"ослуживание\" на \"обслуживание\". (Возможно, ошибка в оригинальном датасете).\n",
    "    * Корректно исправлено \"безупречноне\" на \"безупречное\".\n",
    "17. $+$\n",
    "    * Нет ошибок, нет исправлений.\n",
    "18. $+$\n",
    "    * Корректно исправлено \"Позаввчера\" на \"позавчера\".\n",
    "19. $\\pm$\n",
    "    * Корректно исправлено \"Мужчинаи\" на \"мужчина и\"\n",
    "    * Некорректно исправлено \"идруг\" на \"друг\" вместо \"и друг\". В списке кандидатов есть \"и друг\", но каждый его сабтокен получил меньший скор, чем \"друг\".\n",
    "20. $+$\n",
    "    * Нет ошибок, нет исправлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "1. Продемонстрирована работа модели.\n",
    "2. Обнаружено, что модель иногда исправляет там, где исправлять не нужно. Возможно, этот эффект можно уменьшить если лучше настроить stopping criteria и взять качественную языковую модель.\n",
    "3. Обнаружены возможные ошибки в датасете:\n",
    "    * В предложении 686 про \"Магнитогоск\".\n",
    "    * В предложении 368 про \"ослуживание\".\n",
    "4. Обнаружены потенциальные ошибки/классы ошибок:\n",
    "    * При окончании на \"ццо\" вместо \"тся\"/\"ться\".\n",
    "    * При растягивании гласной, например, \"ооочень\" вместо \"очень\".\n",
    "    * В статье \"Automatic spelling correction for russian social media texts\" частые паттерны, не улавливаемые при помощи модели на основе расстояния Дамерау-Левенштейна была захардкожены. Их можно вручную внедрять в список кандидатов. Например:\n",
    "      * ваще -- вообще\n",
    "      * грит -- говорит\n",
    "      * щас -- сейчас\n",
    "5. Требуется поэкспериментировать над:\n",
    "    * Аггрегирующей функцией по скорам над WordPiece-токенами. Если брать среднее, то появляются ошибки, как в предложении 8, если брать среднее, то будут ошибки при разбиении слова по пробелам, например, в предложении 1 не исправится \"итак\" на \"и так\", а в предложении 15 \"вобщем\" на \"в общем\". Может быть, надо брать сумму по сабтокенам одного слова и среднее по разным словам.\n",
    "    * Максимальным обрабатываемым расстоянием Дамера-Левенштейна;\n",
    "    * Частями position selector, например, использовать более совершенную языковую модель.\n",
    "6. Следует подумать над механизмом обработки ошибки, когда требуется объединить два токена."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
