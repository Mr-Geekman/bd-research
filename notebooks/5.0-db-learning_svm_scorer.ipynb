{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение SVM скорера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом нотбуке будет произведен сбор обучающей выборки и обучение скорера на основе ranking SVM использованием дополнительных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:33.270993Z",
     "start_time": "2021-01-28T12:16:33.251256Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:40:37.637473Z",
     "start_time": "2021-01-28T12:40:37.460807Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from copy import copy\n",
    "from string import punctuation\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "import kenlm\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "from src.models.SpellChecker import *\n",
    "from src.models.BertScorer.bert_scorer_correction import (\n",
    "    BertScorerCorrection\n",
    ")\n",
    "from src.evaluation.spell_ru_eval import align_sents\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:36.025590Z",
     "start_time": "2021-01-28T12:16:35.976586Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нахождение корректных токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с того, что найдем какие токены в каждой позиции правильные. Так как наша модель не умеет никак объединять токены надо будет найти те предложения, где токены исходного предложения не объединяются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:36.067020Z",
     "start_time": "2021-01-28T12:16:36.028997Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_tokenizer = MosesTokenizer(lang='ru')\n",
    "raw_detokenizer = MosesDetokenizer(lang='ru')\n",
    "tokenizer = lambda x: raw_tokenizer.tokenize(x, escape=False)\n",
    "detokenizer = lambda x: raw_detokenizer.detokenize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем все предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:36.114296Z",
     "start_time": "2021-01-28T12:16:36.069506Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(DATA_PATH, 'external', 'spell_ru_eval', 'train_source.txt'), \n",
    "    'r'\n",
    ") as inf:\n",
    "    sentences = inf.readlines()\n",
    "    \n",
    "with open(\n",
    "    os.path.join(DATA_PATH, 'external', 'spell_ru_eval', \n",
    "                 'train_corrected.txt'), \n",
    "    'r'\n",
    ") as inf:\n",
    "    sentences_corrected = inf.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем какое-либо случайное предложение на данный момент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:36.156760Z",
     "start_time": "2021-01-28T12:16:36.117886Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idx = np.random.randint(0, len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:36.661903Z",
     "start_time": "2021-01-28T12:16:36.611988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Намазывем уже остывший корж \" кремом \" ( \" фромаж блан \" или творог, риккота, протертые сквозь мелкое сито, даже густая сметана подойдет ), совсем немного, только, чтобы ягоды потом прилипли.\n",
      "\n",
      "Намазываем уже остывший корж кремом фромаж блан или творог риккота протертые сквозь мелкое сито даже густая сметана подойдет совсем немного только чтобы ягоды потом прилипли\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[idx]\n",
    "sentence_corrected = sentences_corrected[idx]\n",
    "print(sentence)\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:36.883082Z",
     "start_time": "2021-01-28T12:16:36.816261Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_sentence_raw = tokenizer(\n",
    "    sentence.lower().replace('ё', 'е')\n",
    ")\n",
    "tokenized_sentence_corrected = tokenizer(\n",
    "    sentence_corrected.lower().replace('ё', 'е')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем пунктуацию из изначального предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:37.190519Z",
     "start_time": "2021-01-28T12:16:37.154926Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "indices_mapping = []\n",
    "for i, token in enumerate(tokenized_sentence_raw):\n",
    "    if not re.fullmatch(f'[{punctuation}]+', token):\n",
    "        tokenized_sentence.append(token)\n",
    "        indices_mapping.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем убрать пунктуацию и вывести выравнивание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:37.531601Z",
     "start_time": "2021-01-28T12:16:37.464762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['намазывем']\t['намазываем']\n",
      "['уже']\t['уже']\n",
      "['остывший']\t['остывший']\n",
      "['корж']\t['корж']\n",
      "['кремом']\t['кремом']\n",
      "['фромаж']\t['фромаж']\n",
      "['блан']\t['блан']\n",
      "['или']\t['или']\n",
      "['творог']\t['творог']\n",
      "['риккота']\t['риккота']\n",
      "['протертые']\t['протертые']\n",
      "['сквозь']\t['сквозь']\n",
      "['мелкое']\t['мелкое']\n",
      "['сито']\t['сито']\n",
      "['даже']\t['даже']\n",
      "['густая']\t['густая']\n",
      "['сметана']\t['сметана']\n",
      "['подойдет']\t['подойдет']\n",
      "['совсем']\t['совсем']\n",
      "['немного']\t['немного']\n",
      "['только']\t['только']\n",
      "['чтобы']\t['чтобы']\n",
      "['ягоды']\t['ягоды']\n",
      "['потом']\t['потом']\n",
      "['прилипли']\t['прилипли']\n"
     ]
    }
   ],
   "source": [
    "alignment = align_sents(tokenized_sentence, tokenized_sentence_corrected)\n",
    "for pair in alignment:\n",
    "    left_indices, right_indices = pair\n",
    "    print(f'{tokenized_sentence[left_indices[0]:left_indices[1]]}\\t'\n",
    "          f'{tokenized_sentence_corrected[right_indices[0]:right_indices[1]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, тут выравнивание оказалось очень простым: 1 к 1. Таким образом, для каждой позиции слева мы нашли крректный токен справа. На пунктуацию можем внимания не обращать, потому что она не может попасть в позиции для исправлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь такое надо сделать со всеми предложениями в датасете, но надо учесть, что иногда нам могут встречаться случаи, когда нескольким токенам слева соответствует один токен справа (например, когда в слове случайно вставлен пробел). Мы с таким работать не умеем, а потому будем игнорировать такие предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:40.675817Z",
     "start_time": "2021-01-28T12:16:40.622613Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_true_correction(sentence, sentence_corrected):\n",
    "    \"\"\"Find correction for sentence.\"\"\"\n",
    "    tokenized_sentence_raw = tokenizer(\n",
    "        sentence.lower().replace('ё', 'е')\n",
    "    )\n",
    "    tokenized_sentence_corrected = tokenizer(\n",
    "        sentence_corrected.lower().replace('ё', 'е')\n",
    "    )\n",
    "    # remove punctuation from source sentence and make mapping \n",
    "    # to initial indices\n",
    "    tokenized_sentence = []\n",
    "    indices_mapping = []\n",
    "    for i, token in enumerate(tokenized_sentence_raw):\n",
    "        if not re.fullmatch(f'[{punctuation}]+', token):\n",
    "            tokenized_sentence.append(token)\n",
    "            indices_mapping.append(i)\n",
    "    \n",
    "    alignment = align_sents(tokenized_sentence, tokenized_sentence_corrected)\n",
    "    answer = {}\n",
    "    for i, pair in enumerate(alignment):\n",
    "        left_indices, right_indices = pair\n",
    "        if left_indices[1] - left_indices[0] > 1:\n",
    "            return None\n",
    "        answer[indices_mapping[left_indices[0]]] = (\n",
    "            detokenizer(\n",
    "                 tokenized_sentence_corrected[\n",
    "                     right_indices[0]:right_indices[1]]\n",
    "            )\n",
    "        )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняем действие над всеми предложениями в обучающем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:47.058803Z",
     "start_time": "2021-01-28T12:16:40.988645Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fail_indices = []\n",
    "succ_indices = []\n",
    "answers = {}\n",
    "for i, (sentence, sentence_corrected) in enumerate(\n",
    "    zip(sentences, sentences_corrected)\n",
    "):\n",
    "    answer = find_true_correction(sentence, sentence_corrected)\n",
    "    if answer is None:\n",
    "        fail_indices.append(i)\n",
    "    else:\n",
    "        succ_indices.append(i)\n",
    "        answers[i] = answer\n",
    "        \n",
    "sentences_to_check = [sentences[idx] for idx in succ_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как много предложений, которые мы не смогли обработать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:47.111869Z",
     "start_time": "2021-01-28T12:16:47.061579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество ошибок со слияниями: 70\n",
      "Доля ошибок со слияниями: 0.041\n"
     ]
    }
   ],
   "source": [
    "num_fails = len(fail_indices)\n",
    "all_fails = 1727 # знаем исходя из тестировния\n",
    "print(f'Количество ошибок со слияниями: {num_fails}')\n",
    "print(f'Доля ошибок со слияниями: {num_fails/all_fails:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это значение не слишком велико."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор обучающей выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь требуется собрать саму обучающую выборку. \n",
    "\n",
    "Пусть на вход подается некоторое предложение для исправления. В процессе работы модели position selector находит позиции для исправления и подает в candidate scorer список кандидатов. Наша задача &mdash; зафиксировать номер выбранной позиции и список пришедших кандидатов вместе с их признаками.\n",
    "\n",
    "Такое сохранение будет сделано при помощи callback-функции внутри candidate scorer. Она соберет позиции, кандидатов с признаками, результаты скоринга при помощи BERT и запишет это в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:20:31.306796Z",
     "start_time": "2021-01-27T13:20:31.269049Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:20:31.344601Z",
     "start_time": "2021-01-27T13:20:31.309966Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_callback_bert_scorer(num_batch):\n",
    "    def callback_bert_scorer(\n",
    "        tokenized_sentences, indices_processing_sentences, positions, \n",
    "        candidates_features, scoring_results\n",
    "    ):\n",
    "        for num_sent, candidates_sentence in enumerate(candidates_features):\n",
    "            candidates_to_dump = []\n",
    "            position = positions[num_sent]\n",
    "            for i, candidate in enumerate(candidates_sentence):\n",
    "                copy_candidate = copy(candidate)\n",
    "                copy_candidate[1]['bert_score'] = scoring_results[num_sent][i]\n",
    "                candidates_to_dump.append(copy_candidate)\n",
    "\n",
    "            key = (num_batch, indices_processing_sentences[num_sent])\n",
    "            if key not in data:\n",
    "                data[key] = []\n",
    "            data[key].append({\n",
    "                'position': position,\n",
    "                'candidates': candidates_to_dump\n",
    "            })\n",
    "    return callback_bert_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проинициализируем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:23:38.673022Z",
     "start_time": "2021-01-27T13:20:31.348147Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(DATA_PATH, 'external', 'russian_words', \n",
    "                          'russian_words_vocab.dict')\n",
    "vocab = SimpleVocabulary(load_path=vocab_path, save_path=vocab_path)\n",
    "handcode_table_path = os.path.join(DATA_PATH, 'processed', 'handcode_table', \n",
    "                                   'table.json')\n",
    "with open(handcode_table_path, 'r') as inf:\n",
    "    handcode_table = json.load(inf)\n",
    "candidate_generator = CandidateGenerator(\n",
    "    words=vocab.keys(), handcode_table=handcode_table, max_distance=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:23:54.361477Z",
     "start_time": "2021-01-27T13:23:38.675953Z"
    }
   },
   "outputs": [],
   "source": [
    "model_left_right = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'left_right_3_100.arpa.binary')\n",
    ")\n",
    "model_right_left = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'right_left_3_100.arpa.binary')\n",
    ")\n",
    "position_selector = KenlmPositionSelector(model_left_right, model_right_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:24:08.883113Z",
     "start_time": "2021-01-27T13:23:54.397776Z"
    }
   },
   "outputs": [],
   "source": [
    "BERT_PATH = os.path.join(MODEL_PATH, 'conversational_rubert')\n",
    "config = BertConfig.from_json_file(\n",
    "    os.path.join(BERT_PATH, 'bert_config.json')\n",
    ")\n",
    "model = BertForMaskedLM.from_pretrained(\n",
    "    os.path.join(BERT_PATH, 'pytorch_model.bin'),\n",
    "    config=config\n",
    ")\n",
    "bert_tokenizer = BertTokenizer(os.path.join(BERT_PATH, 'vocab.txt'))\n",
    "bert_scorer_correction = BertScorerCorrection(model, bert_tokenizer)\n",
    "agg_subtoken_func = np.mean\n",
    "bert_scorer = BertCandidateScorer(\n",
    "    scorer_basis, agg_subtoken_func\n",
    ")\n",
    "candidate_scorer = CandidateScorer(bert_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:24:08.946904Z",
     "start_time": "2021-01-27T13:24:08.889047Z"
    }
   },
   "outputs": [],
   "source": [
    "stopping_criteria = MarginStoppingCriteria(np.log(2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T13:24:09.000383Z",
     "start_time": "2021-01-27T13:24:08.951408Z"
    }
   },
   "outputs": [],
   "source": [
    "# максимальное количество итераций\n",
    "max_it = 5\n",
    "\n",
    "spellchecker = IterativeSpellChecker(\n",
    "    candidate_generator,\n",
    "    position_selector,\n",
    "    candidate_scorer,\n",
    "    stopping_criteria,\n",
    "    tokenizer,\n",
    "    detokenizer,\n",
    "    num_selected_candidates=None,\n",
    "    max_it=max_it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим сбор обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:46:37.396226Z",
     "start_time": "2021-01-27T13:24:09.003882Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_batches = int(np.ceil(len(sentences_to_check) // batch_size))\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    cur_sentences = sentences_to_check[i*batch_size:(i+1)*batch_size]\n",
    "    spellchecker(\n",
    "        cur_sentences,\n",
    "        callback_candidate_scorer=create_callback_bert_scorer(i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем текущие индексы преложений в исходные и сохраним данные на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:46:37.577715Z",
     "start_time": "2021-01-27T14:46:37.402974Z"
    }
   },
   "outputs": [],
   "source": [
    "data_adjusted = {}\n",
    "for key, value in data.items():\n",
    "    key_adjusted = succ_indices[key[0]*batch_size + key[1]]\n",
    "    data_adjusted[key_adjusted] = value\n",
    "    \n",
    "data = data_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:46:37.876186Z",
     "start_time": "2021-01-27T14:46:37.580028Z"
    }
   },
   "outputs": [],
   "source": [
    "data_with_answers = {}\n",
    "for key, value in data.items():\n",
    "    new_value = []\n",
    "    for item in value:\n",
    "        new_item = copy(item)\n",
    "        new_item['answer'] = answers[key][item['position']]\n",
    "        new_value.append(new_item)\n",
    "    data_with_answers[key] = new_value\n",
    "    \n",
    "data = data_with_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:46:40.911812Z",
     "start_time": "2021-01-27T14:46:39.619749Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'processed', 'scorer_learning', 'data.bin'), 'wb') as ouf:\n",
    "    pickle.dump(data, ouf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналитика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем небольшую аналитку по данным. Найдем:\n",
    "1. Количество предложений без исправлений.\n",
    "2. Среднее количество исправлений.\n",
    "3. Доля случаев, когда использовано максимальное число итераций.\n",
    "4. Доля случаев, когда нет корректного токена в списке кандидатов\n",
    "5. Среднее количество кандидатов.\n",
    "6. Доля случаев, когда надо оставить изначальный токен при изменении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:54.152724Z",
     "start_time": "2021-01-28T12:16:53.760326Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'processed', 'scorer_learning', 'data.bin'), 'rb') as inf:\n",
    "    data = pickle.load(inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). Количество предложений без исправлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:54.222268Z",
     "start_time": "2021-01-28T12:16:54.155829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_to_check) - len(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, почти четверть предложений остались без исправлений. \n",
    "\n",
    "Исходя из статья, посвященной соревнованию, данные которого мы используем, на валидацию и тест было отведено в сумме 1600 корректных предложений. Они разделялись поровно и случайно, поэтому нет гарантий, что удастся получить ровно 800. Следует так же учесть, что часть ошибок модель не смогла заметиь, а часть не может заметить в принципе. Так что полученное значение выглядит правдоподобно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). Среднее количество исправлений в предложениях, в которых хоть что-то было исправлено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:54.707667Z",
     "start_time": "2021-01-28T12:16:54.671162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее количество исправлений: 2.125\n"
     ]
    }
   ],
   "source": [
    "num_corrections = []\n",
    "for key, value in data.items():\n",
    "    num_corrections.append(len(value))\n",
    "num_corrections = np.array(num_corrections)\n",
    "print(f'Среднее количество исправлений: {np.mean(num_corrections):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем имеем примерно два исправления на предложение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3). Доля случаев, когда использовано максимальное число итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:55.228291Z",
     "start_time": "2021-01-28T12:16:55.171513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля случаев, когда использовано максимальное число итерераций: 0.105\n"
     ]
    }
   ],
   "source": [
    "print(f'Доля случаев, когда использовано максимальное число итерераций: '\n",
    "      f'{np.mean(num_corrections == 5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4). Доля случаев, когда нет корректного токена в списке кандидатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:55.615697Z",
     "start_time": "2021-01-28T12:16:55.542584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля случаев, когда нет корректного токена в списке: 0.033\n"
     ]
    }
   ],
   "source": [
    "exist_correct_candidate = []\n",
    "for key, value in data.items():\n",
    "    for item in value:\n",
    "        answer = item['answer']\n",
    "        candidates = [x[0] for x in item['candidates']]\n",
    "        exist_correct_candidate.append(answer in candidates)\n",
    "        \n",
    "print(f'Доля случаев, когда нет корректного токена в списке: '\n",
    "      f'{1-np.mean(exist_correct_candidate):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5). Среднее количество кандидатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:55.915364Z",
     "start_time": "2021-01-28T12:16:55.880555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее количество кандидатов: 19.016\n"
     ]
    }
   ],
   "source": [
    "num_candidates = []\n",
    "for key, value in data.items():\n",
    "    for item in value:\n",
    "        num_candidates.append(len(item['candidates']))\n",
    "print(f'Среднее количество кандидатов: {np.mean(num_candidates):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6). Доля случаев, когда надо оставить изначальный токен при изменении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:16:56.285198Z",
     "start_time": "2021-01-28T12:16:56.237420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля случаев, когда надо оставить изначальный токен: 0.424\n"
     ]
    }
   ],
   "source": [
    "remain_original = []\n",
    "for key, value in data.items():\n",
    "    for item in value:\n",
    "        answer = item['answer']\n",
    "        candidates = [x[0] for x in item['candidates'] if x[1]['is_original']]\n",
    "        remain_original.append(answer in candidates)\n",
    "        \n",
    "print(f'Доля случаев, когда надо оставить изначальный токен: '\n",
    "      f'{np.mean(remain_original):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, изначальный токен надо оставить в достаточно большом количестве случаев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим модель. Будем решать задачу ранжирования, потому что хотим уметь выбирать одного наилучшего кандидата. Попробуем pair-wise подход. Для каждой позиции мы знаем какой из кандидатов (если, конечно, оптимальный кандидат есть в списке) является наилучшим. В таком случае в паре кандидатов, где один правильным мы знаем какой из них лучше &mdash; так и построим обучающую выборку.\n",
    "\n",
    "Следует также определиться с моделью. Думаю, для первых тестов попробуем самый простой подход &mdash; ranking SVM, которая будет обучаться на разницах признаковых векторов и предсказывать класс такого объекта. Для применения этой модели надо будет запустить ее на всех признаковых векторах и найти самый большой логит (это следствие линейности модели)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка датафрейма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с создания датафрейма. Каждому случаю ранжирования надо присвоить отдельную группу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:17:00.263680Z",
     "start_time": "2021-01-28T12:16:59.731487Z"
    }
   },
   "outputs": [],
   "source": [
    "candidate_keys = list(data[0][0]['candidates'][0][1].keys())\n",
    "df_dict = {}\n",
    "df_dict['group'] = []\n",
    "df_dict['answer'] = []\n",
    "df_dict['token'] = []\n",
    "group_idx = 0\n",
    "df_dict.update({key: [] for key in candidate_keys})\n",
    "for key_data, values_data in data.items():\n",
    "    for i, item in enumerate(values_data):\n",
    "        for candidate in item['candidates']:\n",
    "            df_dict['group'].append(group_idx)\n",
    "            df_dict['answer'].append(item['answer'])\n",
    "            df_dict['token'].append(candidate[0])\n",
    "            for key, value in candidate[1].items():\n",
    "                df_dict[key].append(float(value))\n",
    "        group_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:17:00.616433Z",
     "start_time": "2021-01-28T12:17:00.265878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>answer</th>\n",
       "      <th>token</th>\n",
       "      <th>is_title</th>\n",
       "      <th>is_upper</th>\n",
       "      <th>is_lower</th>\n",
       "      <th>is_first</th>\n",
       "      <th>contains_space</th>\n",
       "      <th>contains_hyphen</th>\n",
       "      <th>from_levenshtein_searcher</th>\n",
       "      <th>from_phonetic_searcher</th>\n",
       "      <th>from_handcode_searcher</th>\n",
       "      <th>is_original</th>\n",
       "      <th>from_vocabulary</th>\n",
       "      <th>kenlm_left_right</th>\n",
       "      <th>kenlm_right_left</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>кто бы</td>\n",
       "      <td>ктобы</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.820433</td>\n",
       "      <td>-8.266124</td>\n",
       "      <td>-6.350665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>кто бы</td>\n",
       "      <td>чтобы</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.517485</td>\n",
       "      <td>-4.744852</td>\n",
       "      <td>-6.150540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>кто бы</td>\n",
       "      <td>кто бы</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.504485</td>\n",
       "      <td>-4.735255</td>\n",
       "      <td>-3.705160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>кто бы</td>\n",
       "      <td>кт бы</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.154133</td>\n",
       "      <td>-9.646735</td>\n",
       "      <td>-10.100927</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>кто бы</td>\n",
       "      <td>кто ы</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.029816</td>\n",
       "      <td>-9.861761</td>\n",
       "      <td>-8.961858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group  answer   token  is_title  is_upper  is_lower  is_first  \\\n",
       "0      0  кто бы   ктобы       0.0       0.0       1.0       0.0   \n",
       "1      0  кто бы   чтобы       0.0       0.0       1.0       0.0   \n",
       "2      0  кто бы  кто бы       0.0       0.0       1.0       0.0   \n",
       "3      0  кто бы   кт бы       0.0       0.0       1.0       0.0   \n",
       "4      0  кто бы   кто ы       0.0       0.0       1.0       0.0   \n",
       "\n",
       "   contains_space  contains_hyphen  from_levenshtein_searcher  \\\n",
       "0             0.0              0.0                        0.0   \n",
       "1             0.0              0.0                        1.0   \n",
       "2             1.0              0.0                        1.0   \n",
       "3             1.0              0.0                        1.0   \n",
       "4             1.0              0.0                        1.0   \n",
       "\n",
       "   from_phonetic_searcher  from_handcode_searcher  is_original  \\\n",
       "0                     0.0                     0.0          1.0   \n",
       "1                     0.0                     0.0          0.0   \n",
       "2                     0.0                     0.0          0.0   \n",
       "3                     0.0                     0.0          0.0   \n",
       "4                     0.0                     0.0          0.0   \n",
       "\n",
       "   from_vocabulary  kenlm_left_right  kenlm_right_left  bert_score  is_correct  \n",
       "0              0.0         -7.820433         -8.266124   -6.350665           0  \n",
       "1              1.0         -3.517485         -4.744852   -6.150540           0  \n",
       "2              1.0         -5.504485         -4.735255   -3.705160           1  \n",
       "3              1.0         -9.154133         -9.646735  -10.100927           0  \n",
       "4              1.0         -9.029816         -9.861761   -8.961858           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df_dict)\n",
    "df['is_correct'] = (df['token'] == df['answer']).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь следует создать общий dataframe для попарного подхода. Таргет равен $1$, если кандидат, из которого вычитают строго лучше вычитаемого и $0$ если ситуация обратная (случай, когда не знаем какой из кандидатов лучше не рассматриваем).\n",
    "\n",
    "Также следует заметить, что признаки: `is_title`, `is_upper`, `is_lower`, `is_first` в нашем подходе оказываются бесполезными, потому что они всегда будут нулевыми (внутри одной группы они у всех одинаковы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:17:48.948514Z",
     "start_time": "2021-01-28T12:17:03.841214Z"
    }
   },
   "outputs": [],
   "source": [
    "pairwise_keys = candidate_keys + ['is_correct']\n",
    "df_pairwise = pd.DataFrame(columns=pairwise_keys + ['group'])\n",
    "for group in df.group.unique():\n",
    "    cur_data = df[df.group == group]\n",
    "    cur_correct = cur_data[cur_data.is_correct.astype(bool)][pairwise_keys]\n",
    "    cur_incorrect = cur_data[~cur_data.is_correct.astype(bool)][pairwise_keys]\n",
    "    if not cur_correct.empty:\n",
    "        negative_data = cur_incorrect - cur_correct.iloc[0]\n",
    "        positive_data = -negative_data\n",
    "        negative_data['group'] = group\n",
    "        positive_data['group'] = group\n",
    "        negative_data['is_correct'] = 0\n",
    "        positive_data['is_correct'] = 1\n",
    "        df_pairwise = df_pairwise.append(negative_data)\n",
    "        df_pairwise = df_pairwise.append(positive_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименуем колонки, чтобы было понятнее, что представляет из себя этот датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:17:49.005291Z",
     "start_time": "2021-01-28T12:17:48.950790Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pairwise['is_better'] = df_pairwise['is_correct'].astype(int)\n",
    "df_pairwise.drop(columns=['is_correct'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:17:49.071615Z",
     "start_time": "2021-01-28T12:17:49.008887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_title</th>\n",
       "      <th>is_upper</th>\n",
       "      <th>is_lower</th>\n",
       "      <th>is_first</th>\n",
       "      <th>contains_space</th>\n",
       "      <th>contains_hyphen</th>\n",
       "      <th>from_levenshtein_searcher</th>\n",
       "      <th>from_phonetic_searcher</th>\n",
       "      <th>from_handcode_searcher</th>\n",
       "      <th>is_original</th>\n",
       "      <th>from_vocabulary</th>\n",
       "      <th>kenlm_left_right</th>\n",
       "      <th>kenlm_right_left</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>group</th>\n",
       "      <th>is_better</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.315948</td>\n",
       "      <td>-3.530869</td>\n",
       "      <td>-2.645505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.987001</td>\n",
       "      <td>-0.009597</td>\n",
       "      <td>-2.445380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.649648</td>\n",
       "      <td>-4.911480</td>\n",
       "      <td>-6.395767</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.525331</td>\n",
       "      <td>-5.126506</td>\n",
       "      <td>-5.256697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.476080</td>\n",
       "      <td>-7.381570</td>\n",
       "      <td>-11.472279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_title  is_upper  is_lower  is_first  contains_space  contains_hyphen  \\\n",
       "0       0.0       0.0       0.0       0.0            -1.0              0.0   \n",
       "1       0.0       0.0       0.0       0.0            -1.0              0.0   \n",
       "3       0.0       0.0       0.0       0.0             0.0              0.0   \n",
       "4       0.0       0.0       0.0       0.0             0.0              0.0   \n",
       "5       0.0       0.0       0.0       0.0             0.0              0.0   \n",
       "\n",
       "   from_levenshtein_searcher  from_phonetic_searcher  from_handcode_searcher  \\\n",
       "0                       -1.0                     0.0                     0.0   \n",
       "1                        0.0                     0.0                     0.0   \n",
       "3                        0.0                     0.0                     0.0   \n",
       "4                        0.0                     0.0                     0.0   \n",
       "5                        0.0                     0.0                     0.0   \n",
       "\n",
       "   is_original  from_vocabulary  kenlm_left_right  kenlm_right_left  \\\n",
       "0          1.0             -1.0         -2.315948         -3.530869   \n",
       "1          0.0              0.0          1.987001         -0.009597   \n",
       "3          0.0              0.0         -3.649648         -4.911480   \n",
       "4          0.0              0.0         -3.525331         -5.126506   \n",
       "5          0.0              0.0         -5.476080         -7.381570   \n",
       "\n",
       "   bert_score group  is_better  \n",
       "0   -2.645505     0          0  \n",
       "1   -2.445380     0          0  \n",
       "3   -6.395767     0          0  \n",
       "4   -5.256697     0          0  \n",
       "5  -11.472279     0          0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pairwise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размер датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:17:49.133847Z",
     "start_time": "2021-01-28T12:17:49.074844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100128, 16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pairwise.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достаточно большой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные к виду, с которым будем работать в sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:35:54.628298Z",
     "start_time": "2021-01-28T12:35:54.546969Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_pairwise.drop(columns=['is_better', 'group']).reset_index(drop=True)\n",
    "groups_pairwise = df_pairwise['group'].reset_index(drop=True)\n",
    "groups = df['group'].reset_index(drop=True).unique()\n",
    "y = df_pairwise['is_better'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:35:55.866734Z",
     "start_time": "2021-01-28T12:35:55.789225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_title</th>\n",
       "      <th>is_upper</th>\n",
       "      <th>is_lower</th>\n",
       "      <th>is_first</th>\n",
       "      <th>contains_space</th>\n",
       "      <th>contains_hyphen</th>\n",
       "      <th>from_levenshtein_searcher</th>\n",
       "      <th>from_phonetic_searcher</th>\n",
       "      <th>from_handcode_searcher</th>\n",
       "      <th>is_original</th>\n",
       "      <th>from_vocabulary</th>\n",
       "      <th>kenlm_left_right</th>\n",
       "      <th>kenlm_right_left</th>\n",
       "      <th>bert_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.315948</td>\n",
       "      <td>-3.530869</td>\n",
       "      <td>-2.645505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.987001</td>\n",
       "      <td>-0.009597</td>\n",
       "      <td>-2.445380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.649648</td>\n",
       "      <td>-4.911480</td>\n",
       "      <td>-6.395767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.525331</td>\n",
       "      <td>-5.126506</td>\n",
       "      <td>-5.256697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.476080</td>\n",
       "      <td>-7.381570</td>\n",
       "      <td>-11.472279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_title  is_upper  is_lower  is_first  contains_space  contains_hyphen  \\\n",
       "0       0.0       0.0       0.0       0.0            -1.0              0.0   \n",
       "1       0.0       0.0       0.0       0.0            -1.0              0.0   \n",
       "2       0.0       0.0       0.0       0.0             0.0              0.0   \n",
       "3       0.0       0.0       0.0       0.0             0.0              0.0   \n",
       "4       0.0       0.0       0.0       0.0             0.0              0.0   \n",
       "\n",
       "   from_levenshtein_searcher  from_phonetic_searcher  from_handcode_searcher  \\\n",
       "0                       -1.0                     0.0                     0.0   \n",
       "1                        0.0                     0.0                     0.0   \n",
       "2                        0.0                     0.0                     0.0   \n",
       "3                        0.0                     0.0                     0.0   \n",
       "4                        0.0                     0.0                     0.0   \n",
       "\n",
       "   is_original  from_vocabulary  kenlm_left_right  kenlm_right_left  \\\n",
       "0          1.0             -1.0         -2.315948         -3.530869   \n",
       "1          0.0              0.0          1.987001         -0.009597   \n",
       "2          0.0              0.0         -3.649648         -4.911480   \n",
       "3          0.0              0.0         -3.525331         -5.126506   \n",
       "4          0.0              0.0         -5.476080         -7.381570   \n",
       "\n",
       "   bert_score  \n",
       "0   -2.645505  \n",
       "1   -2.445380  \n",
       "2   -6.395767  \n",
       "3   -5.256697  \n",
       "4  -11.472279  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели и ее тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим модель и протестируем ее. Для этого будем делать кросс-валидацию по номерам группы, а на оставшихся для тестирования группах будем проверять, что самое большое предсказание получает лучший кандидат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:42:22.110145Z",
     "start_time": "2021-01-28T12:42:08.926498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrgeekman/Programs/anaconda3/envs/bs_research/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/mrgeekman/Programs/anaconda3/envs/bs_research/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/mrgeekman/Programs/anaconda3/envs/bs_research/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/mrgeekman/Programs/anaconda3/envs/bs_research/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/mrgeekman/Programs/anaconda3/envs/bs_research/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold()\n",
    "accuracy_values = []\n",
    "for groups_indices_train, groups_indices_test in kf.split(groups):\n",
    "    groups_train = groups[groups_indices_train]\n",
    "    X_train = X.loc[groups_pairwise.isin(groups_train)]\n",
    "    y_train = y.loc[groups_pairwise.isin(groups_train)]\n",
    "    model = Pipeline(\n",
    "        [('scaler', StandardScaler()), ('svc', LinearSVC(random_state=42))]\n",
    "    )\n",
    "    model = LinearSVC(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    succ_predictions = []\n",
    "    possible_indicators = []\n",
    "    for group in groups[groups_indices_test]:\n",
    "        X_test_raw = df[df.group == group]\n",
    "        X_test = X_test_raw[X_train.columns]\n",
    "        scores = model.decision_function(X_test)\n",
    "        prediction_idx = np.argmax(scores)\n",
    "        succ_predictions.append(\n",
    "            X_test_raw.answer.iloc[prediction_idx] \n",
    "            == X_test_raw.token.iloc[prediction_idx]\n",
    "        )\n",
    "        \n",
    "    succ_predictions = np.array(succ_predictions)\n",
    "    possible_indicators = np.array(possible_indicators)\n",
    "    accuracy_values.append(np.mean(succ_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T11:46:49.461827Z",
     "start_time": "2021-01-27T11:46:49.397159Z"
    }
   },
   "source": [
    "Посмотрим на среднее значение accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:42:22.146588Z",
     "start_time": "2021-01-28T12:42:22.113270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(accuracy_values)\n",
    "print(f'Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение весьма хорошее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель на всех данных и сохраним ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:42:25.234114Z",
     "start_time": "2021-01-28T12:42:24.639523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrgeekman/Programs/anaconda3/envs/bs_research/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svc', LinearSVC(random_state=42))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "        [('scaler', StandardScaler()), ('svc', LinearSVC(random_state=42))]\n",
    ")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:42:28.464534Z",
     "start_time": "2021-01-28T12:42:28.394120Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'processed', 'scorer_learning', 'svm.bin'), 'wb') as ouf:\n",
    "    pickle.dump(model, ouf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на веса коэффициентов в модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:42:46.725277Z",
     "start_time": "2021-01-28T12:42:46.663744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_title: 0.000\n",
      "is_upper: 0.000\n",
      "is_lower: 0.000\n",
      "is_first: 0.000\n",
      "contains_space: 0.413\n",
      "contains_hyphen: 0.165\n",
      "from_levenshtein_searcher: 0.969\n",
      "from_phonetic_searcher: 0.663\n",
      "from_handcode_searcher: 0.631\n",
      "is_original: 2.231\n",
      "from_vocabulary: 0.043\n",
      "kenlm_left_right: 0.983\n",
      "kenlm_right_left: 1.148\n",
      "bert_score: 0.822\n"
     ]
    }
   ],
   "source": [
    "for coef, feature_name in zip(model['svc'].coef_.ravel(), X.columns.tolist()):\n",
    "    print(f'{feature_name}: {coef:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Как ожидалось, модель проигнорировала первые четыре признака.\n",
    "2. Модель дает высокую оценку коэффициенту перед индикатором изначального токена. То есть модель дает большое предпочтение изначальному токену.\n",
    "3. Модель выше оценивает, что исправление пришло из levenshtein searcher, хотя казалось, что самый большой вклад должен иметь кандидат из handcode searcher, ведь он почти всегда дает правильное исправление.\n",
    "4. Модель сильнее реагирует на наличие пробелов, чем на наличие дефисов.\n",
    "5. Коэффициенты перед скорами от kenlm по модулю несколько больше коэффициента перед скором от BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "1. В этом ноутбуке была собрана обучающая выборка для candidate scorer, который использует другие признаки помимо скоров из BERT.\n",
    "2. Была обучена модель ranking SVM, которая умеет в списке кандидатов находить самое релевантное исправление.\n",
    "3. Точность исправлений приближается к $90\\%$, что выглядит весьма хорошо."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
