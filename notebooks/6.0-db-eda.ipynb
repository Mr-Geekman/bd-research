{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом нотбуке будет произведен анализ данного нам датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:12.761211Z",
     "start_time": "2021-02-21T19:32:12.735742Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:16.765909Z",
     "start_time": "2021-02-21T19:32:13.224130Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from copy import copy, deepcopy\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "import kenlm\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "from src.models.SpellChecker import *\n",
    "from src.models.BertScorer.bert_scorer_correction import (\n",
    "    BertScorerCorrection\n",
    ")\n",
    "from src.evaluation.spell_ru_eval import (\n",
    "    align_sents, levenstein_dist, extract_words\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:16.813813Z",
     "start_time": "2021-02-21T19:32:16.768493Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение выравнивания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с того, что найдем какие токены в каждой позиции правильные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:16.864784Z",
     "start_time": "2021-02-21T19:32:16.816092Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_tokenizer = MosesTokenizer(lang='ru')\n",
    "raw_detokenizer = MosesDetokenizer(lang='ru')\n",
    "tokenizer = lambda x: raw_tokenizer.tokenize(x, escape=False)\n",
    "detokenizer = lambda x: raw_detokenizer.detokenize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем все предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:16.929174Z",
     "start_time": "2021-02-21T19:32:16.867952Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(DATA_PATH, 'external', 'spell_ru_eval', 'train_source.txt'), \n",
    "    'r'\n",
    ") as inf:\n",
    "    sentences = inf.readlines()\n",
    "    \n",
    "with open(\n",
    "    os.path.join(DATA_PATH, 'external', 'spell_ru_eval', \n",
    "                 'train_corrected.txt'), \n",
    "    'r'\n",
    ") as inf:\n",
    "    sentences_corrected = inf.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем какое-либо случайное предложение на данный момент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:16.969720Z",
     "start_time": "2021-02-21T19:32:16.931100Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idx = np.random.randint(0, len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:17.029359Z",
     "start_time": "2021-02-21T19:32:16.973074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Намазывем уже остывший корж \" кремом \" ( \" фромаж блан \" или творог, риккота, протертые сквозь мелкое сито, даже густая сметана подойдет ), совсем немного, только, чтобы ягоды потом прилипли.\n",
      "\n",
      "Намазываем уже остывший корж кремом фромаж блан или творог риккота протертые сквозь мелкое сито даже густая сметана подойдет совсем немного только чтобы ягоды потом прилипли\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[idx]\n",
    "sentence_corrected = sentences_corrected[idx]\n",
    "print(sentence)\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:17.211971Z",
     "start_time": "2021-02-21T19:32:17.032859Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_sentence_raw = tokenizer(\n",
    "    sentence.lower().replace('ё', 'е')\n",
    ")\n",
    "tokenized_sentence_corrected = tokenizer(\n",
    "    sentence_corrected.lower().replace('ё', 'е')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем пунктуацию из изначального предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:17.262000Z",
     "start_time": "2021-02-21T19:32:17.215222Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "indices_mapping = []\n",
    "for i, token in enumerate(tokenized_sentence_raw):\n",
    "    if not re.fullmatch(f'[{punctuation}]+', token):\n",
    "        tokenized_sentence.append(token)\n",
    "        indices_mapping.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем убрать пунктуацию и вывести выравнивание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:17.479216Z",
     "start_time": "2021-02-21T19:32:17.410846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['намазывем']\t['намазываем']\n",
      "['уже']\t['уже']\n",
      "['остывший']\t['остывший']\n",
      "['корж']\t['корж']\n",
      "['кремом']\t['кремом']\n",
      "['фромаж']\t['фромаж']\n",
      "['блан']\t['блан']\n",
      "['или']\t['или']\n",
      "['творог']\t['творог']\n",
      "['риккота']\t['риккота']\n",
      "['протертые']\t['протертые']\n",
      "['сквозь']\t['сквозь']\n",
      "['мелкое']\t['мелкое']\n",
      "['сито']\t['сито']\n",
      "['даже']\t['даже']\n",
      "['густая']\t['густая']\n",
      "['сметана']\t['сметана']\n",
      "['подойдет']\t['подойдет']\n",
      "['совсем']\t['совсем']\n",
      "['немного']\t['немного']\n",
      "['только']\t['только']\n",
      "['чтобы']\t['чтобы']\n",
      "['ягоды']\t['ягоды']\n",
      "['потом']\t['потом']\n",
      "['прилипли']\t['прилипли']\n"
     ]
    }
   ],
   "source": [
    "alignment = align_sents(tokenized_sentence, tokenized_sentence_corrected)\n",
    "for pair in alignment:\n",
    "    left_indices, right_indices = pair\n",
    "    print(f'{tokenized_sentence[left_indices[0]:left_indices[1]]}\\t'\n",
    "          f'{tokenized_sentence_corrected[right_indices[0]:right_indices[1]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, тут выравнивание оказалось очень простым: 1 к 1. Таким образом, для каждой позиции слева мы нашли корректный токен справа. На пунктуацию можем внимания не обращать, так как она нам не интересна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание о токенизации**\n",
    "\n",
    "Здесь мы будем использовать токенизацию, как в тестирующем скрипте. Токенизация при помощи `MosesTokenizer` обнаруживает 1734 ошибок вместо 1727. Опишем что это за случаи\n",
    "\n",
    "1. Обработка двоеточий.<br>\n",
    "Дает четыре дополнительные ошибки. `MosesTokenizer` разделяет строки вида `'18:00'` на три токена: `['18', ':', '00']`, что после выкидывания пунктуации дает ошибку с двоеточием. Токенизатор в тестирующем скрипте работает с этим, как с одним токеном.\n",
    "\n",
    "2. Обработка дробей.<br>\n",
    "Дает одну ошибку. Ситуация аналогична обработке двоеточий.\n",
    "\n",
    "3. Проблема со словом \"см.\".<br>\n",
    "Дает одну ошибку. `MosesTokenizer` обрабатывает строку `\"см.\"`, как отдельный токен, а в корректно исправленных предложениях этот токен фигурирует, как `\"см\"`.\n",
    "\n",
    "4. Проблема со словом \"иметь-\".<br>\n",
    "Дает одну ошибку. `MosesTokenizer` обрабатывает строку `\"иметь-\"`, как отдельный токен, а в корректно исправленных предложениях этот токен фигурирует, как `\"иметь\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь такое надо сделать со всеми предложениями в датасете.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:20.003566Z",
     "start_time": "2021-02-21T19:32:19.964742Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_corrections(sentence, sentence_corrected):\n",
    "    \"\"\"Find corrections of source sentence using corrected sentence.\"\"\"\n",
    "    tokenized_sentence_raw = extract_words(sentence)\n",
    "    tokenized_sentence_corrected = extract_words(sentence_corrected)\n",
    "    # remove punctuation from source sentence and make mapping \n",
    "    # to initial indices\n",
    "    tokenized_sentence = []\n",
    "    indices_mapping = []\n",
    "    for i, token in enumerate(tokenized_sentence_raw):\n",
    "        if not re.fullmatch(f'[{punctuation}«»]+', token):\n",
    "            tokenized_sentence.append(token)\n",
    "            indices_mapping.append(i)\n",
    "    \n",
    "    alignment = align_sents(tokenized_sentence, tokenized_sentence_corrected, \n",
    "                            return_only_different=True)\n",
    "    \n",
    "    mapping = {}\n",
    "    for i, pair in enumerate(alignment):\n",
    "        left_indices, right_indices = pair\n",
    "        mapping[i] = (\n",
    "            tokenized_sentence[left_indices[0]:left_indices[1]], \n",
    "            tokenized_sentence_corrected[right_indices[0]:right_indices[1]]\n",
    "        )\n",
    "        \n",
    "    return mapping, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняем действие над всеми предложениями в обучающем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:24.708553Z",
     "start_time": "2021-02-21T19:32:20.281846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrections = []\n",
    "alignments = []\n",
    "for i, (sentence, sentence_corrected) in enumerate(\n",
    "    zip(sentences, sentences_corrected)\n",
    "):\n",
    "    corrections_sentence, alignment = find_corrections(\n",
    "        sentence, sentence_corrected\n",
    "    )\n",
    "    corrections.append(corrections_sentence) \n",
    "    alignments.append(alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T18:48:13.315825Z",
     "start_time": "2021-02-10T18:48:13.260402Z"
    }
   },
   "source": [
    "Убедимся, что собраны все предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:24.749154Z",
     "start_time": "2021-02-21T19:32:24.711445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит элемент выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:24.798719Z",
     "start_time": "2021-02-21T19:32:24.754791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (['ктобы'], ['кто', 'бы']), 1: (['не'], ['ни'])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем проводить анализ полученного датасета. Начнем с того, что подсчитаем количества ошибок различных типов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количества ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:26.432031Z",
     "start_time": "2021-02-21T19:32:26.398517Z"
    }
   },
   "outputs": [],
   "source": [
    "num_corrections = 0\n",
    "for corrections_sentence in corrections:\n",
    "    num_corrections += len(corrections_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:26.646162Z",
     "start_time": "2021-02-21T19:32:26.585645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего исправлений: 1727\n"
     ]
    }
   ],
   "source": [
    "print(f'Всего исправлений: {num_corrections}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили ровно то количество ошибок, которое наблюдается после тестирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем количества ошибок разных типов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Один к одному**\n",
    "\n",
    "Посмотрим, как часто один токен просто заменяется на другой токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:28.656096Z",
     "start_time": "2021-02-21T19:32:28.617828Z"
    }
   },
   "outputs": [],
   "source": [
    "num_one_to_one = 0\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 1 and len(corrected) == 1:\n",
    "            num_one_to_one += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:28.849589Z",
     "start_time": "2021-02-21T19:32:28.801639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исправлений одного токена на другой один токен: 1381\n",
      "Доля исправлений одного токена на другой один токен: 0.800\n"
     ]
    }
   ],
   "source": [
    "print(f'Исправлений одного токена на другой один токен: {num_one_to_one}')\n",
    "print(f'Доля исправлений одного токена на другой один токен: '\n",
    "      f'{num_one_to_one/num_corrections:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, целых $80\\%$ ошибок &mdash; это исправление одного токена на другой токен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Один к нескольким**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как часто один токен исправляется на несколько и каковы в таком случае соотношения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:29.511017Z",
     "start_time": "2021-02-21T19:32:29.455307Z"
    }
   },
   "outputs": [],
   "source": [
    "num_one_to_many = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 1 and len(corrected) > 1:\n",
    "            num_one_to_many.append(len(corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:29.619671Z",
     "start_time": "2021-02-21T19:32:29.583185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исправлений одного токена на несколько: 270\n",
      "Доля исправлений одного токена на несколько: 0.156\n"
     ]
    }
   ],
   "source": [
    "print(f'Исправлений одного токена на несколько: {len(num_one_to_many)}')\n",
    "print(f'Доля исправлений одного токена на несколько: '\n",
    "      f'{len(num_one_to_many)/num_corrections:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределения значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:29.915140Z",
     "start_time": "2021-02-21T19:32:29.853114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество замен одного токена на 2: 262\n",
      "Количество замен одного токена на 3: 8\n"
     ]
    }
   ],
   "source": [
    "nums_tokens, nums_cases = np.unique(num_one_to_many, return_counts=True)\n",
    "for num_tokens, num_cases in zip(nums_tokens, nums_cases):\n",
    "    print(f'Количество замен одного токена на {num_tokens}: {num_cases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, существует несколько случаев замены одного токена на целых три. Посмотрим на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:30.150927Z",
     "start_time": "2021-02-21T19:32:30.101473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['инагоночных'], ['и', 'на', 'гоночных'])\n",
      "(['вконтактепро'], ['в', 'контакте', 'про'])\n",
      "(['незнаюкто'], ['не', 'знаю', 'кто'])\n",
      "(['тачтосидит'], ['та', 'что', 'сидит'])\n",
      "(['тойчтостоит'], ['той', 'что', 'стоит'])\n",
      "(['незачто'], ['не', 'за', 'что'])\n",
      "(['незанающиеграниц'], ['не', 'знающие', 'границ'])\n",
      "(['ктомуже'], ['к', 'тому', 'же'])\n"
     ]
    }
   ],
   "source": [
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 1 and len(corrected) == 3:\n",
    "            print(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит, как достаточно редкий случай."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T13:50:52.634583Z",
     "start_time": "2021-02-10T13:50:52.540051Z"
    }
   },
   "source": [
    "**Несколько к одиному**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как часто несколько токенов исправляется на один и каковы в таком случае соотношения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:30.695451Z",
     "start_time": "2021-02-21T19:32:30.634568Z"
    }
   },
   "outputs": [],
   "source": [
    "num_many_to_one = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) > 1 and len(corrected) == 1:\n",
    "            num_many_to_one.append(len(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:30.795421Z",
     "start_time": "2021-02-21T19:32:30.763843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исправлений нескольких токенов на один: 70\n",
      "Доля исправлений нескольких токенов на один: 0.041\n"
     ]
    }
   ],
   "source": [
    "print(f'Исправлений нескольких токенов на один: {len(num_many_to_one)}')\n",
    "print(f'Доля исправлений нескольких токенов на один: '\n",
    "      f'{len(num_many_to_one)/num_corrections:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределения значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:31.572882Z",
     "start_time": "2021-02-21T19:32:31.539861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество замен одного токена на 2: 69\n",
      "Количество замен одного токена на 3: 1\n"
     ]
    }
   ],
   "source": [
    "nums_tokens, nums_cases = np.unique(num_many_to_one, return_counts=True)\n",
    "for num_tokens, num_cases in zip(nums_tokens, nums_cases):\n",
    "    print(f'Количество замен одного токена на {num_tokens}: {num_cases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, существует несколько случаев замены одного токена на целых три. Посмотрим на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:32.891454Z",
     "start_time": "2021-02-21T19:32:32.828012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['прочего', 'прочего', 'прочего'], ['прочего-прочего-прочего'])\n"
     ]
    }
   ],
   "source": [
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 3 and len(corrected) == 1:\n",
    "            print(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит, как достаточно редкий случай."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуем выделить некоторые конкретные типы ошибок, которые тут могли быть:\n",
    "1. Лишний пробел в слове. \n",
    "2. Пробел вместо дефиса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:33.318643Z",
     "start_time": "2021-02-21T19:32:33.278400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['не', 'допонемание'], ['недопонимание'])\n",
      "(['што', 'бы'], ['чтобы'])\n",
      "(['розврат', 'то'], ['разврат-то'])\n",
      "(['потохоньку', 'полегоньку'], ['потихоньку-полегоньку'])\n",
      "(['лапмочку', 'то'], ['лампочку-то'])\n",
      "(['што', 'нить'], ['что-нибудь'])\n",
      "(['не', 'каких'], ['никаких'])\n",
      "(['оптять', 'таки'], ['опять-таки'])\n",
      "(['24х', 'летие'], ['24-летие'])\n",
      "(['какой', 'то'], ['какое-то'])\n",
      "(['мееедленно', 'мееедленно'], ['медленно-медленно'])\n",
      "(['что', 'нибуь'], ['что-нибудь'])\n"
     ]
    }
   ],
   "source": [
    "extra_space = 0\n",
    "need_hyphen = 0\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 2 and len(corrected) == 1:\n",
    "            if source[0] + source[1] == corrected[0]:\n",
    "                extra_space += 1\n",
    "            elif source[0] + '-' + source[1] == corrected[0]:\n",
    "                need_hyphen += 1\n",
    "            else:\n",
    "                print(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:33.480742Z",
     "start_time": "2021-02-21T19:32:33.429158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество случаев, когда стоит ненужный пробел: 26\n",
      "Количество случаев, когда стоит пробел вместо дефиса: 31\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество случаев, когда стоит ненужный пробел: {extra_space}')\n",
    "print(f'Количество случаев, когда стоит пробел вместо дефиса: {need_hyphen}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, почти все случаи этой ошибки относятся всего к двум простым категориям. Правда, пара случаев из них не очень приятные из-за соединения числа и слова, например \"15 летней\" вместо \"15-летней\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Несколько к нескольким**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:34.098796Z",
     "start_time": "2021-02-21T19:32:34.036826Z"
    }
   },
   "outputs": [],
   "source": [
    "num_many_to_many = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) > 1 and len(corrected) > 1:\n",
    "            num_many_to_many.append((len(source), len(corrected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:35.805405Z",
     "start_time": "2021-02-21T19:32:35.772349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исправлений нескольких токенов на несколько: 3\n",
      "Доля исправлений нескольких токенов на несколько: 0.002\n"
     ]
    }
   ],
   "source": [
    "print(f'Исправлений нескольких токенов на несколько: {len(num_many_to_many)}')\n",
    "print(f'Доля исправлений нескольких токенов на несколько: '\n",
    "      f'{len(num_many_to_many)/num_corrections:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, это крайне редкие случаи, посмотрим на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:36.142843Z",
     "start_time": "2021-02-21T19:32:36.093316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['вообщем', 'то'], ['в', 'общем-то'])\n",
      "(['сос', 'тарыми'], ['со', 'старыми'])\n",
      "(['вообщем', 'то'], ['в', 'общем-то'])\n"
     ]
    }
   ],
   "source": [
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) > 1 and len(corrected) > 1:\n",
    "            print(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что два случая вообще одинаковы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояния Дамерау-Левенштейна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь изучим каково распределение расстояний Дамерау-Левенштейна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:38.034306Z",
     "start_time": "2021-02-21T19:32:36.684570Z"
    }
   },
   "outputs": [],
   "source": [
    "distances = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        source_str = ' '.join(source)\n",
    "        corrected_str = ' '.join(corrected)\n",
    "        distance = levenstein_dist(source_str, corrected_str)\n",
    "        distances.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:38.093057Z",
     "start_time": "2021-02-21T19:32:38.037784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество коррекций с расстоянием 1: 1333/1727 ~ 0.772\n",
      "Количество коррекций с расстоянием 2: 267/1727 ~ 0.155\n",
      "Количество коррекций с расстоянием 3: 80/1727 ~ 0.046\n",
      "Количество коррекций с расстоянием 4: 31/1727 ~ 0.018\n",
      "Количество коррекций с расстоянием 5: 11/1727 ~ 0.006\n",
      "Количество коррекций с расстоянием 6: 4/1727 ~ 0.002\n",
      "Количество коррекций с расстоянием 10: 1/1727 ~ 0.001\n"
     ]
    }
   ],
   "source": [
    "distance_values, nums_cases = np.unique(distances, return_counts=True)\n",
    "for distance, num_cases in zip(distance_values, nums_cases):\n",
    "    print(f'Количество коррекций с расстоянием {int(distance)}: '\n",
    "          f'{num_cases}/{num_corrections} ~ {num_cases/num_corrections:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, около $93\\%$ случаев требуют исправления на расстоянии, не превышающем два."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы проанализируем работу компоненты candidage generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Полнота словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим насколько часто правильные исправления есть в используемом нами словаре. Если их там нет, то модель будет ограничена в том, чтобы исправлять такие ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:49.396564Z",
     "start_time": "2021-02-21T19:32:43.445110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 22:32:43.507 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/mrgeekman/Documents/MIPT/НИР/Repo/data/external/russian_words/russian_words_vocab.dict]\n"
     ]
    }
   ],
   "source": [
    "vocab_path = os.path.join(\n",
    "    DATA_PATH, 'external', 'russian_words', 'russian_words_vocab.dict'\n",
    ")\n",
    "vocab = set(SimpleVocabulary(load_path=vocab_path, save_path=vocab_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:49.432293Z",
     "start_time": "2021-02-21T19:32:49.399265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 1531456\n"
     ]
    }
   ],
   "source": [
    "print(f'Размер словаря: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, словарь весьма большой, почти $1.5$ млн слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим как много слов из корректных исправлений наш словарь не знает. Так как мы работаем только с теми местами, где исправления нужны, то игнорируем случай, когда модели нужно не трогать незнакомое ей слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:51.460314Z",
     "start_time": "2021-02-21T19:32:51.408201Z"
    }
   },
   "outputs": [],
   "source": [
    "unknown_words = defaultdict(int)\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        for word in corrected:\n",
    "            if word not in vocab:\n",
    "                unknown_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:52.175766Z",
     "start_time": "2021-02-21T19:32:52.115557Z"
    }
   },
   "outputs": [],
   "source": [
    "keys = np.array(list(unknown_words.keys()))\n",
    "values = np.array(list(unknown_words.values()))\n",
    "indices_sorted = np.argsort(-values)\n",
    "keys = keys[indices_sorted]\n",
    "values = values[indices_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем для каждого несловарного слова его само и его частоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:54.286730Z",
     "start_time": "2021-02-21T19:32:54.235456Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наконец-то: 7\n",
      "общем-то: 4\n",
      "по-любому: 4\n",
      "какие-то: 2\n",
      "какое-то: 2\n",
      "повтыкав: 1\n",
      "отрисовала: 1\n",
      "перкуссионист: 1\n",
      "какая-то: 1\n",
      "онтогенетически: 1\n",
      "очень-очень: 1\n",
      "строительно-магазинное: 1\n",
      "мегапозитивные: 1\n",
      "азербайджан: 1\n",
      "депрессняк: 1\n",
      "4-х: 1\n",
      "спецпсихологии: 1\n",
      "мачо: 1\n",
      "стартапу: 1\n",
      "веб-камеру: 1\n",
      "мегапиковой: 1\n",
      "варикозе: 1\n",
      "кому-то: 1\n",
      "сами-то: 1\n",
      "дауни-младший: 1\n",
      "рязань: 1\n",
      "постебался: 1\n",
      "прочего-прочего-прочего: 1\n",
      "пофотографировав: 1\n",
      "тихо-тихо: 1\n",
      "аборигенку: 1\n",
      "обрызгивается: 1\n",
      "рaда: 1\n",
      "видео-репортаж: 1\n",
      "медленно-медленно: 1\n",
      "очень-то: 1\n",
      "скриншот: 1\n",
      "расставаньям: 1\n",
      "24-летие: 1\n",
      "вообще-то: 1\n",
      "рок-н-ролла: 1\n",
      "репбазы: 1\n",
      "электрокнигу: 1\n",
      "горсобрания: 1\n",
      "папилломовирус: 1\n",
      "предыдущих-последующих: 1\n",
      "папилломавирусы: 1\n",
      "рекомендуемо: 1\n",
      "шкварчать: 1\n",
      "40-летней: 1\n",
      "15-летней: 1\n",
      "60-х: 1\n",
      "коммент: 1\n",
      "рок-звезда: 1\n",
      "разврат-то: 1\n",
      "любви-то: 1\n",
      "встретил-таки: 1\n",
      "кого-нибудь: 1\n",
      "напроектировали: 1\n",
      "чем-то: 1\n",
      "дом-2: 1\n",
      "колясочников: 1\n",
      "самой-то: 1\n",
      "девчонки-продавцы: 1\n",
      "шмотья: 1\n",
      "монголоидом: 1\n",
      "по-взрослому: 1\n",
      "гулливера: 1\n",
      "таиланд: 1\n",
      "викторович: 1\n",
      "лосьончика: 1\n",
      "мастер-классах: 1\n",
      "ого-го: 1\n",
      "госслужбы: 1\n",
      "дело-то: 1\n",
      "лампочку-то: 1\n",
      "айгуль: 1\n",
      "таиланде: 1\n",
      "бабешку: 1\n",
      "студента-экономиста: 1\n",
      "теоретизированное: 1\n",
      "потихоньку-полегоньку: 1\n",
      "долго-долго-долго: 1\n",
      "нашел-таки: 1\n",
      "класснее: 1\n",
      "преинтереснейшее: 1\n",
      "подгрузился: 1\n",
      "непридуманная: 1\n",
      "каким-нибудь: 1\n",
      "кидалы: 1\n",
      "понаобещал: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in zip(keys, values):\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, большинство слов встречаются лишь один раз, но есть и особо частые случаи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:32:56.107960Z",
     "start_time": "2021-02-21T19:32:56.075942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего несловарных слов: 91\n",
      "Всего несловарных случаев: 105\n"
     ]
    }
   ],
   "source": [
    "print(f'Всего несловарных слов: {len(keys)}')\n",
    "print(f'Всего несловарных случаев: {np.sum(values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, около сотни ошибок мы не можем исправить в принципе, потому что просто не имеем таких словарных слов.\n",
    "\n",
    "Возможные решения:\n",
    "1. Внести все те слова, что у нас нет.\n",
    "2. Внести в словарь лишь те слова, что имеют частоту, большую одного.\n",
    "3. Поискать более хороший словарь. Этот способ позволит избежать похожих ошибок еще и на этапе тестирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа `PhoneticSearcher`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компонента candidate generator имеет внутри себя алгоритм для фонетического поиска слова. Попробуем провести какую-то аналитику для него. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:34:07.871388Z",
     "start_time": "2021-02-21T19:33:14.197620Z"
    }
   },
   "outputs": [],
   "source": [
    "phonetic_searcher = PhoneticSeacher(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь интересно как часто он помогает, если слова находятся на расстоянии Левенштейна, большим одного. Рассматривать работу будем только на случае один к одному, так как `PhoneticSearcher` не умеет работать со строками с пробелами внутри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:34:08.900405Z",
     "start_time": "2021-02-21T19:34:07.874449Z"
    }
   },
   "outputs": [],
   "source": [
    "distances = []\n",
    "is_succ = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 1 and len(corrected) == 1:\n",
    "            source = source[0]\n",
    "            corrected = corrected[0]\n",
    "            distance = levenstein_dist(source, corrected)\n",
    "            if distance > 1:\n",
    "                distances.append(distance)\n",
    "                candidates = phonetic_searcher([[source]])[0][0]\n",
    "                if corrected in candidates:\n",
    "                    is_succ.append(True)\n",
    "                else:\n",
    "                    is_succ.append(False)\n",
    "                    \n",
    "is_succ = np.array(is_succ)\n",
    "distances = np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:34:08.935373Z",
     "start_time": "2021-02-21T19:34:08.903636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество успешных нахождений кандидатов: 110/341 ~ 0.323\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество успешных нахождений кандидатов: '\n",
    "      f'{np.sum(is_succ)}/{len(is_succ)} ~ {np.mean(is_succ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит не так уж и плохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, как распределены расстояния, с на которых нам удается найти кандидатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:34:34.429713Z",
     "start_time": "2021-02-21T19:34:34.364743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество коррекций с расстоянием 2: 65/230 ~ 0.283\n",
      "Количество коррекций с расстоянием 3: 28/75 ~ 0.373\n",
      "Количество коррекций с расстоянием 4: 15/26 ~ 0.577\n",
      "Количество коррекций с расстоянием 5: 1/5 ~ 0.200\n",
      "Количество коррекций с расстоянием 6: 0/4 ~ 0.000\n",
      "Количество коррекций с расстоянием 10: 1/1 ~ 1.000\n"
     ]
    }
   ],
   "source": [
    "distance_values, nums_cases = np.unique(distances, return_counts=True)\n",
    "for distance, num_cur_cases in zip(distance_values, nums_cases):\n",
    "    cur_is_succ = is_succ[distances == distance]\n",
    "    print(f'Количество коррекций с расстоянием {int(distance)}: '\n",
    "          f'{np.sum(cur_is_succ)}/{len(cur_is_succ)} ~ '\n",
    "          f'{np.mean(cur_is_succ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, эта компонента не очень хорошо помогает доставать кандидатов с расстояния два, но позволяет доставать и с больших расстояний. Дело в том, что достаточно часто кандидаты на большом расстоянии содержат дублирующиеся гласные, с которыми `PhoneticSearcher` умеет работать. В качестве примера, \"ооочень\" вместо \"очень\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа `HandcodeSearcher`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем аналогичный анализ для `HandcodeSearcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:21.093835Z",
     "start_time": "2021-02-21T19:35:21.053046Z"
    }
   },
   "outputs": [],
   "source": [
    "handcode_table_path = os.path.join(\n",
    "    DATA_PATH, 'processed', 'handcode_table', 'table.json'\n",
    ")\n",
    "with open(handcode_table_path, 'r') as inf:\n",
    "    handcode_table = json.load(inf)\n",
    "\n",
    "handcode_searcher = HandcodeSearcher(handcode_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:23.854306Z",
     "start_time": "2021-02-21T19:35:22.958723Z"
    }
   },
   "outputs": [],
   "source": [
    "distances = []\n",
    "is_succ = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 1 and len(corrected) == 1:\n",
    "            source = source[0]\n",
    "            corrected = corrected[0]\n",
    "            distance = levenstein_dist(source, corrected)\n",
    "            if distance > 1:\n",
    "                distances.append(distance)\n",
    "                candidates = handcode_searcher([[source]])[0][0]\n",
    "                if corrected in candidates:\n",
    "                    is_succ.append(True)\n",
    "                else:\n",
    "                    is_succ.append(False)\n",
    "                    \n",
    "is_succ = np.array(is_succ)\n",
    "distances = np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:25.350725Z",
     "start_time": "2021-02-21T19:35:25.315325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество успешных нахождений кандидатов: 99/341 ~ 0.290\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество успешных нахождений кандидатов: '\n",
    "      f'{np.sum(is_succ)}/{len(is_succ)} ~ {np.mean(is_succ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже выглядит вполне себе неплохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение расстояний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:30.585401Z",
     "start_time": "2021-02-21T19:35:30.546716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество коррекций с расстоянием 2: 54/230 ~ 0.235\n",
      "Количество коррекций с расстоянием 3: 31/75 ~ 0.413\n",
      "Количество коррекций с расстоянием 4: 7/26 ~ 0.269\n",
      "Количество коррекций с расстоянием 5: 4/5 ~ 0.800\n",
      "Количество коррекций с расстоянием 6: 3/4 ~ 0.750\n",
      "Количество коррекций с расстоянием 10: 0/1 ~ 0.000\n"
     ]
    }
   ],
   "source": [
    "distance_values, nums_cases = np.unique(distances, return_counts=True)\n",
    "for distance, num_cur_cases in zip(distance_values, nums_cases):\n",
    "    cur_is_succ = is_succ[distances == distance]\n",
    "    print(f'Количество коррекций с расстоянием {int(distance)}: '\n",
    "          f'{np.sum(cur_is_succ)}/{len(cur_is_succ)} ~'\n",
    "          f' {np.mean(cur_is_succ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что опять удается достать лишь небольшое число кандидатов на расстоянии два."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа `PhoneticSearcher` и `HandcodeSearcher`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим насколько хорошо эти модели действуют в тандеме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:40.487127Z",
     "start_time": "2021-02-21T19:35:39.506310Z"
    }
   },
   "outputs": [],
   "source": [
    "distances = []\n",
    "is_succ = []\n",
    "for corrections_sentence in corrections:\n",
    "    for _, correction in corrections_sentence.items():\n",
    "        source, corrected = correction\n",
    "        if len(source) == 1 and len(corrected) == 1:\n",
    "            source = source[0]\n",
    "            corrected = corrected[0]\n",
    "            distance = levenstein_dist(source, corrected)\n",
    "            if distance > 1:\n",
    "                distances.append(distance)\n",
    "                candidates = (\n",
    "                    phonetic_searcher([[source]])[0][0] \n",
    "                    + handcode_searcher([[source]])[0][0]\n",
    "                )\n",
    "                if corrected in candidates:\n",
    "                    is_succ.append(True)\n",
    "                else:\n",
    "                    is_succ.append(False)\n",
    "                    \n",
    "is_succ = np.array(is_succ)\n",
    "distances = np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:40.526509Z",
     "start_time": "2021-02-21T19:35:40.490114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество успешных нахождений кандидатов: 204/341 ~ 0.598\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество успешных нахождений кандидатов: '\n",
    "      f'{np.sum(is_succ)}/{len(is_succ)} ~ {np.mean(is_succ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим полное отсутствие пересечений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение расстояний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:35:43.368585Z",
     "start_time": "2021-02-21T19:35:43.311553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество коррекций с расстоянием 2: 115/230 ~ 0.500\n",
      "Количество коррекций с расстоянием 3: 58/75 ~ 0.773\n",
      "Количество коррекций с расстоянием 4: 22/26 ~ 0.846\n",
      "Количество коррекций с расстоянием 5: 5/5 ~ 1.000\n",
      "Количество коррекций с расстоянием 6: 3/4 ~ 0.750\n",
      "Количество коррекций с расстоянием 10: 1/1 ~ 1.000\n"
     ]
    }
   ],
   "source": [
    "distance_values, nums_cases = np.unique(distances, return_counts=True)\n",
    "for distance, num_cur_cases in zip(distance_values, nums_cases):\n",
    "    cur_is_succ = is_succ[distances == distance]\n",
    "    print(f'Количество коррекций с расстоянием {int(distance)}: '\n",
    "          f'{np.sum(cur_is_succ)}/{len(cur_is_succ)} ~ '\n",
    "          f'{np.mean(cur_is_succ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что самая большая проблема со словами на расстоянии Дамерау-Левенштейна, равном двум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Механизм объединения токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем насколько хорошо работает наш механизм объединения токенов для обработки ошибок \"несколько на один\". Мы просматриваем пары последовательных токенов, объединяя их через пробел, и если мы находим в словаре слово после удаления пробела или его замены на дефис, то формируем следующий список кандидатов:\n",
    "1. Изначальные два токена.\n",
    "2. Кандидаты сразу для пары токенов, полученные удалением пробела или его заменой на дефис. \n",
    "3. Неизменный первый токен + кандидаты для второго токена.\n",
    "4. Кандидаты для первого токена + неизменный второй токен.\n",
    "\n",
    "Такой механизм в теории позволяет бороться с лишними объединениями, так как мы можем по-прежнему выбрать какие-то исправления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:39:13.600005Z",
     "start_time": "2021-02-21T19:35:46.842625Z"
    }
   },
   "outputs": [],
   "source": [
    "del phonetic_searcher, handcode_searcher\n",
    "gc.collect()\n",
    "candidate_generator = CandidateGenerator(\n",
    "    words=vocab, handcode_table=handcode_table, max_distance=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:39:13.773907Z",
     "start_time": "2021-02-21T19:39:13.611959Z"
    }
   },
   "outputs": [],
   "source": [
    "many_to_one_alignments = set()\n",
    "cnt = 0\n",
    "for num_sent, alignment in enumerate(alignments):\n",
    "    for left_indices, right_indices in alignment:\n",
    "        if (\n",
    "            left_indices[1] - left_indices[0] == 2 \n",
    "            and right_indices[1] - right_indices[0] == 1\n",
    "        ):\n",
    "            many_to_one_alignments.add(\n",
    "                (num_sent, \n",
    "                 (left_indices[0], left_indices[1]-1))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь воспользуемся `MosesTokenizer` и удалением пунктуации уже после процедуры объединения, потому что именно так это будет работать в нашей модели. Объединять лучше до удаления пункуации, потому что часто она может спасти от объединения слов, которые могли бы быть объединены без нее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:39:13.888084Z",
     "start_time": "2021-02-21T19:39:13.777076Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_combinations(sentence):\n",
    "    tokenized_sentence_raw = tokenizer(sentence)\n",
    "    candidates = candidate_generator([tokenized_sentence_raw])\n",
    "    results_combined = candidate_generator.combine_tokens(candidates)\n",
    "    candidates_combined, indices_combined = results_combined\n",
    "    \n",
    "    # remove punctuation from source sentence and make mapping \n",
    "    # to initial indices\n",
    "    tokenized_sentence = []\n",
    "    indices_mapping = []\n",
    "    for i, token in enumerate(tokenized_sentence_raw):\n",
    "        if not re.fullmatch(f'[{punctuation}«»]+', token):\n",
    "            tokenized_sentence.append(token)\n",
    "            indices_mapping.append(i)\n",
    "    indices_inv_mapping = {\n",
    "        value: idx for idx, value in enumerate(indices_mapping)\n",
    "    }\n",
    "    \n",
    "    combinations = []\n",
    "    for indices in indices_combined[0]:\n",
    "        if len(indices) == 2:\n",
    "            combinations.append((\n",
    "                indices_inv_mapping[indices[0]],\n",
    "                indices_inv_mapping[indices[1]]\n",
    "            ))\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:41:55.145537Z",
     "start_time": "2021-02-21T19:39:13.892544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c57d9eaa14c4b1d895650cb4935b9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "combinations = set()\n",
    "for num_sent, sentence in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "    combinations_sentence = find_combinations(sentence)\n",
    "    for combination in combinations_sentence:\n",
    "        combinations.add((num_sent, tuple(combination)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T08:26:08.242110Z",
     "start_time": "2021-02-16T08:26:08.112240Z"
    }
   },
   "source": [
    "Теперь подсчитаем насколько хорошо нашей модели удается делать объединения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T19:41:55.219702Z",
     "start_time": "2021-02-21T19:41:55.148334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 39\n",
      "Precision = 0.071\n",
      "Recall = 0.565\n"
     ]
    }
   ],
   "source": [
    "TP = len(combinations.intersection(many_to_one_alignments))\n",
    "precision = TP / len(combinations)\n",
    "recall = TP / len(many_to_one_alignments)\n",
    "print(f'TP = {TP}')\n",
    "print(f'Precision = {precision:.3f}')\n",
    "print(f'Recall = {recall:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, модель выявляет только около половины ошибок и срабатывает не в нужный момент очень часто. \n",
    "\n",
    "Проблема с большим числом ложных срабатываний кроется в том, что в словаре часто находятся слова, которые подходят под объединение. \n",
    "\n",
    "Проблема с малым recall в том, что многих слов нет в словаре, например \"любви-то\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "См. в ноутбуке `9.0-db-learning_advanced_position_selector`, который посвящен улучшению стандартной версии position selector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "См. в ноутбуке `7.0-db-learning_advanced_scorer`, который посвящен улучшению стандартной версии candidate scorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "\n",
    "1. Был проведен анализ частот различного типа ошибок.\n",
    "2. Было обнаружено, что порядка сотни ошибок связаны с отсутствием корректных исправлений в словаре.\n",
    "3. Было обнаружено, что `PhoneticSearcher` и `HandcodeSearcher` достаточно хорошо покрывают ошибки на расстоянии Дамерау-Левеншейна, большем двух."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
