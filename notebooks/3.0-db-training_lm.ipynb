{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучения языковых моделей\n",
    "\n",
    "В этом ноутбуке будет произведено обучение языковых моделей для модели итеративного исправления. Требуется обучить две модели:\n",
    "\n",
    "1. Слева-направо\n",
    "2. Справа-налево\n",
    "\n",
    "В качестве обучающего корпуса будет взят фрагмент корпуса Тайга, а именно части из соц.сетей, новостных сайтов, субтитров, так как это должен быть достаточно близкий к изучаемому домен.\n",
    "\n",
    "В качестве модели было решено взять KenLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:54:23.061983Z",
     "start_time": "2021-01-02T15:54:23.031571Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:54:25.210586Z",
     "start_time": "2021-01-02T15:54:23.284072Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:54:27.264587Z",
     "start_time": "2021-01-02T15:54:25.212966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:54:27.312727Z",
     "start_time": "2021-01-02T15:54:27.268088Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "CONFIGS_PATH = os.path.join(PROJECT_PATH, 'src', 'configs')\n",
    "os.environ['DP_PROJECT_PATH'] = PROJECT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных для обучения решено было взять фрагмент корпуса [Тайга](https://tatianashavrina.github.io/taiga_site/). Были выбраны разделы:\n",
    "1. Новости\n",
    "2. Соцсети\n",
    "3. Субтитры\n",
    "\n",
    "Все файлы для скачивания доступны по [ссылке](https://tatianashavrina.github.io/taiga_site/downloads) в разделе \"Our special collections for\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "В первую очередь требуется предобработать все тексты, что у нас имеются. Согласно задаче, нас не интересует регистр слов и пунктуация, поэтому избавимся от нее. В качестве результата должны получиться два текстовых файла (прямой и обратный), где каждое предложение расположено на отдельной строчке -- именно в таком виде следует подавать данные для обучающей программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:54:27.504977Z",
     "start_time": "2021-01-02T15:54:27.316435Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ../data/processed/kenlm -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:54:27.554188Z",
     "start_time": "2021-01-02T15:54:27.508524Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "TAIGA_PATH = os.path.join(DATA_PATH, 'external', 'taiga')\n",
    "RESULT_PATH = os.path.join(DATA_PATH, 'processed', 'kenlm')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Новости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с обработки новостей. Каждую из них требуется разбить на предложения и токенизировать, избавившись от пунктуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T17:55:37.164283Z",
     "start_time": "2021-01-01T17:55:37.134737Z"
    }
   },
   "outputs": [],
   "source": [
    "NEWS_PATH = os.path.join(TAIGA_PATH, 'news')\n",
    "news_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала обработаем только Фонтанку, потому что там есть деление по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T17:56:54.822446Z",
     "start_time": "2021-01-01T17:55:37.166967Z"
    }
   },
   "outputs": [],
   "source": [
    "fontanka_path = os.path.join(NEWS_PATH, 'Fontanka', 'texts')\n",
    "for year in tqdm(sorted(os.listdir(fontanka_path))):\n",
    "    year_path = os.path.join(fontanka_path, year)\n",
    "    for filename in sorted(os.listdir(year_path)):\n",
    "        with open(os.path.join(year_path, filename), 'r') as inf:\n",
    "            news_texts.append(inf.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обработаем тексты по всем остальным новостным сайтам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T17:57:22.645902Z",
     "start_time": "2021-01-01T17:56:54.825592Z"
    }
   },
   "outputs": [],
   "source": [
    "for source in tqdm(sorted(os.listdir(NEWS_PATH))):\n",
    "    if source == 'Fontanka':\n",
    "        continue\n",
    "    texts_path = os.path.join(NEWS_PATH, source, 'texts')\n",
    "    for filename in sorted(os.listdir(texts_path)):\n",
    "        with open(os.path.join(texts_path, filename), 'r') as inf:\n",
    "            news_texts.append(inf.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем каждый текст по предложениям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:03:03.292174Z",
     "start_time": "2021-01-01T17:57:22.649897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_sentences = []\n",
    "for text in tqdm(news_texts):\n",
    "    news_sentences += [\n",
    "        x.lower() for x in nltk.tokenize.sent_tokenize(\n",
    "            text, language='russian'\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь каждый текст разобьем на токены и избавимся от тех из них, которые отвечают за пунктуацию. Сначала надо изучить какие вообще символы встречаются в текстах, чтобы понять что из этого может быть пунктуацией (иначе мы можем не учесть какие-то специфичные символы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:04:57.903193Z",
     "start_time": "2021-01-01T18:03:03.294973Z"
    }
   },
   "outputs": [],
   "source": [
    "characters = Counter()\n",
    "for sentence in tqdm(news_sentences):\n",
    "    characters.update(list(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:04:57.988695Z",
     "start_time": "2021-01-01T18:04:57.907518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Беспокоит наличие символов `\\n`, `\\t`. Чтобы избавиться от них заменим `\\n`, `\\t` на пробел.\n",
    "2. Наличие иностранных символов можно объяснить ссылкой на какой-то иностранный источник или имя на оригинальном языке.\n",
    "3. Поиском предложений с соответствующими символами удавалось обнаружить очень \"шумные\" предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример шумного предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:04:58.023184Z",
     "start_time": "2021-01-01T18:04:57.991997Z"
    }
   },
   "outputs": [],
   "source": [
    "news_sentences[2905071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:04:58.059186Z",
     "start_time": "2021-01-01T18:04:58.026878Z"
    }
   },
   "outputs": [],
   "source": [
    "news_sentences[3079526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:04:58.100340Z",
     "start_time": "2021-01-01T18:04:58.062065Z"
    }
   },
   "outputs": [],
   "source": [
    "news_sentences[4015059]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таких предложений, судя по подсчетам символов из них немного, а потому просто проигнорируем их."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие символы мы уже имеем в пунктуации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:15:52.838376Z",
     "start_time": "2021-01-01T21:15:52.807552Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот список надо дополнить символами `«`, `»`, `—`, `…`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:15:53.640956Z",
     "start_time": "2021-01-01T21:15:53.609553Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuation = ''.join(list(punctuation) + ['«', '»', '—', '…'])\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем удалять те токены, которые состоят лишь из знаков пунктуации. Запишем результаты на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:36:34.087504Z",
     "start_time": "2021-01-01T18:04:58.174312Z"
    }
   },
   "outputs": [],
   "source": [
    "for sentence in tqdm(news_sentences):\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(\n",
    "        sentence.replace('\\t', ' ').replace('\\n', ' '), language='russian'\n",
    "    )\n",
    "    cleaned_tokenized_sentence = [\n",
    "        x for x in tokenized_sentence \n",
    "        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "    ]\n",
    "    \n",
    "    with open(os.path.join(RESULT_PATH, 'news_left_right.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(RESULT_PATH, 'news_right_left.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence[::-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T18:36:34.868242Z",
     "start_time": "2021-01-01T18:36:34.089588Z"
    }
   },
   "outputs": [],
   "source": [
    "del news_sentences, news_texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соцсети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обработаем тексты из соцсетей. Насчет включения этого раздела я до сих пор сомневаюсь. Тут весьма специфичный вокабуляр и достаточно много опечаток самих по себе.\n",
    "\n",
    "Особенность обработки в том, что во всех источниках кроме Live Journal разные записи обозначаются при помощи DataBaseItem. Надо будем уметь детектировать разные записи и обрабатывать их отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:43:17.907960Z",
     "start_time": "2021-01-01T21:43:17.718280Z"
    }
   },
   "outputs": [],
   "source": [
    "SOCIAL_PATH = os.path.join(TAIGA_PATH, 'social', 'texts')\n",
    "social_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с Live Journal. При визуальном осмотре удалось заметить несколько особенностей:\n",
    "1. Некоторые предложения очень короткие. Возможно, их стоит выбросить.\n",
    "2. Часто повторяется строчка +100 -- выбросим ее.\n",
    "3. Достаточно часто попадается построка `&quot` -- выбросим ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:43:20.974800Z",
     "start_time": "2021-01-01T21:43:18.459278Z"
    }
   },
   "outputs": [],
   "source": [
    "lj_path = os.path.join(SOCIAL_PATH, 'LiveJournalPostsandcommentsGICR.txt')\n",
    "social_texts = []\n",
    "with open(lj_path, 'r') as inf:\n",
    "    social_texts += inf.readlines()\n",
    "    \n",
    "social_texts = [x.replace('&quot', '') for x in social_texts if x != '+100\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты из всех остальных источников обрабатываются одинаково, надо лишь пропустить строки, обозначающие DataBaseItem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:43:24.163943Z",
     "start_time": "2021-01-01T21:43:20.978559Z"
    }
   },
   "outputs": [],
   "source": [
    "for source in sorted(os.listdir(SOCIAL_PATH)):\n",
    "    if source == 'LiveJournalPostsandcommentsGICR.txt':\n",
    "        continue\n",
    "    with open(os.path.join(SOCIAL_PATH, source), 'r') as inf:\n",
    "        social_texts += inf.readlines()\n",
    "\n",
    "social_texts = [x for x in social_texts if 'DataBaseItem' not in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обработать обращения по имени из ВК. Для этого требуется при помощи регулярных выражений уловить конструкцию `[id|name]` и удалить там `id`. Со всем остальным справится токенизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:43:30.693517Z",
     "start_time": "2021-01-01T21:43:24.167678Z"
    }
   },
   "outputs": [],
   "source": [
    "social_texts = [re.sub('id[0-9]+', '', x) for x in social_texts]\n",
    "social_texts = [re.sub('\\*id\\w+', '', x) for x in social_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых строчках попадаются больше одного предложения. Надо их токенизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T21:48:05.505116Z",
     "start_time": "2021-01-01T21:43:30.696593Z"
    }
   },
   "outputs": [],
   "source": [
    "social_sentences = []\n",
    "for text in tqdm(social_texts):\n",
    "    social_sentences += [\n",
    "        x.lower() for x in nltk.tokenize.sent_tokenize(\n",
    "            text, language='russian'\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось токенизировать предложения и выполнить запись на диск. Также отберем только те предложения, которые состоят по крайней мере из 5 слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:51.999198Z",
     "start_time": "2021-01-01T21:48:05.507390Z"
    }
   },
   "outputs": [],
   "source": [
    "len_filter_border = 5\n",
    "for sentence in tqdm(social_sentences):\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(\n",
    "        sentence.replace('\\t', ' ').replace('\\n', ' '), language='russian'\n",
    "    )\n",
    "    cleaned_tokenized_sentence = [\n",
    "        x for x in tokenized_sentence \n",
    "        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "    ]\n",
    "    if len(cleaned_tokenized_sentence) < len_filter_border:\n",
    "        continue\n",
    "    with open(os.path.join(RESULT_PATH, 'social_left_right.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(RESULT_PATH, 'social_right_left.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence[::-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:52.582331Z",
     "start_time": "2021-01-01T22:13:52.002103Z"
    }
   },
   "outputs": [],
   "source": [
    "del social_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Субтитры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем тексты из субтитров.\n",
    "\n",
    "Особенность обработки в том, что в данных помимо текста есть таймкоды. Также одно и то же предложение в общем случае разбито на несколько таймкодов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с того, что загрузим таблицу с метаданными, чтобы доставать файлы с русскими субтитрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:52.611019Z",
     "start_time": "2021-01-01T22:13:52.585825Z"
    }
   },
   "outputs": [],
   "source": [
    "SUBTITLES_PATH = os.path.join(TAIGA_PATH, 'subtitles')\n",
    "subtitles_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:52.694842Z",
     "start_time": "2021-01-01T22:13:52.614960Z"
    }
   },
   "outputs": [],
   "source": [
    "subtitles_df = pd.read_csv(os.path.join(SUBTITLES_PATH, 'metatable.csv'), sep='\\t')\n",
    "subtitles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:52.746854Z",
     "start_time": "2021-01-01T22:13:52.697828Z"
    }
   },
   "outputs": [],
   "source": [
    "subtitles_df = subtitles_df[subtitles_df['languages'] == 'ru']\n",
    "subtitles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо отдельно обработать случай сериала `Marvels Agents of S.H.I.E.L.D`. Дело в том, что данные между названиями второго и первого сезонов неконсистентны и это не полностью отражено в таблице (есть вариант написания `Marvel s Agents of S.H.I.E.L.D`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:52.795443Z",
     "start_time": "2021-01-01T22:13:52.749685Z"
    }
   },
   "outputs": [],
   "source": [
    "subtitles_df[subtitles_df['filepath'].str.startswith('Marvel')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:52.843715Z",
     "start_time": "2021-01-01T22:13:52.797578Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = [x.replace('Marvel s', 'Marvels') for x in subtitles_df['filepath'].tolist()]\n",
    "folders = [x.split(' - ')[0].strip(' .') for x in filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим несколько особенностей для извелечения именно текста:\n",
    "1. Вытащить именно текст вместо временных меток можно при помощи `split` по табам. \n",
    "2. Лучше сконкатенировать все строчки, так как иногда текст переносится, как уже было отмечено выше.\n",
    "3. На последней строчке, насколько я мог наблюдать, расположены опции по отрисовке субтитров, а потому ее можно проигнорировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:13:59.982151Z",
     "start_time": "2021-01-01T22:13:52.845763Z"
    }
   },
   "outputs": [],
   "source": [
    "for folder, filename in tqdm(zip(folders, filenames), total=len(folders)):\n",
    "    with open(os.path.join(SUBTITLES_PATH, 'texts', folder, filename), 'r') as inf:\n",
    "        subtitles_texts.append(\n",
    "            ' '.join([x.split('\\t')[-1].strip() for x in inf.readlines()][:-1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:17:29.540363Z",
     "start_time": "2021-01-01T22:13:59.984501Z"
    }
   },
   "outputs": [],
   "source": [
    "subtitles_sentences = []\n",
    "for text in tqdm(subtitles_texts):\n",
    "    subtitles_sentences += [\n",
    "        x.lower() for x in nltk.tokenize.sent_tokenize(\n",
    "            text, language='russian'\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось токенизировать предложения и выполнить запись на диск. Также отберем только те предложения, которые состоят по крайней мере из 5 слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:36:24.715546Z",
     "start_time": "2021-01-01T22:17:29.542158Z"
    }
   },
   "outputs": [],
   "source": [
    "len_filter_border = 5\n",
    "lengths = []\n",
    "for sentence in tqdm(subtitles_sentences):\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(\n",
    "        sentence.replace('\\t', ' ').replace('\\n', ' '), language='russian'\n",
    "    )\n",
    "    cleaned_tokenized_sentence = [\n",
    "        x for x in tokenized_sentence \n",
    "        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "    ]\n",
    "    if len(cleaned_tokenized_sentence) < len_filter_border:\n",
    "        continue\n",
    "    with open(os.path.join(RESULT_PATH, 'subtitles_left_right.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(RESULT_PATH, 'subtitles_right_left.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence[::-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T22:36:25.183689Z",
     "start_time": "2021-01-01T22:36:24.719009Z"
    }
   },
   "outputs": [],
   "source": [
    "del subtitles_sentences, subtitles_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сконкатенируем полученные файлы для обучения языковых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:55:08.527806Z",
     "start_time": "2021-01-02T15:54:56.147471Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/processed/kenlm/news_left_right.txt ../data/processed/kenlm/social_left_right.txt ../data/processed/kenlm/subtitles_left_right.txt > ../data/processed/kenlm/left_right.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:55:32.618560Z",
     "start_time": "2021-01-02T15:55:08.533388Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/processed/kenlm/news_right_left.txt ../data/processed/kenlm/social_right_left.txt ../data/processed/kenlm/subtitles_right_left.txt > ../data/processed/kenlm/right_left.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на объем полученных датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:55:32.843112Z",
     "start_time": "2021-01-02T15:55:32.624355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8G\t../data/processed/kenlm/left_right.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ../data/processed/kenlm/left_right.txt -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:55:33.025547Z",
     "start_time": "2021-01-02T15:55:32.847387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8G\t../data/processed/kenlm/right_left.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ../data/processed/kenlm/right_left.txt -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выполним обучение. Для этого вспользуемя [документацией](https://kheafield.com/code/kenlm/estimation/) и [инструкцией](https://github.com/kmario23/KenLM-training).\n",
    "\n",
    "На этом этапе подразумевается, что библиотека уже склонирована в src/kenlm и собрана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T15:58:45.408891Z",
     "start_time": "2021-01-02T15:55:46.244342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/mrgeekman/Documents/MIPT/НИР/Repo/data/processed/kenlm/left_right.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 155911769 types 2420218\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:29042616 2:3398434816 3:6372065280\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 2420218 D1=0.76322 D2=0.928264 D3+=1.11566\n",
      "2 32860060 D1=0.800531 D2=1.08166 D3+=1.2996\n",
      "3 79416358 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 2175 assuming -p 1.5\n",
      "probing 2372 assuming -r models -p 1.5\n",
      "trie     995 without quantization\n",
      "trie     594 assuming -q 8 -b 8 quantization \n",
      "trie     922 assuming -a 22 array pointer compression\n",
      "trie     520 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:29042616 2:525760960 3:1588327160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:29042616 2:525760960 3:1588327160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:9727008 kB\tVmRSS:66828 kB\tRSSMax:3814316 kB\tuser:157.263\tsys:22.7064\tCPU:179.969\treal:178.939\n"
     ]
    }
   ],
   "source": [
    "! ../src/kenlm/build/bin/lmplz -o 3 --discount_fallback < ../data/processed/kenlm/left_right.txt > ../models/kenlm/left_right.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:01:53.066228Z",
     "start_time": "2021-01-02T15:58:51.840075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/mrgeekman/Documents/MIPT/НИР/Repo/data/processed/kenlm/right_left.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 155911769 types 2420218\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:29042616 2:3398434816 3:6372065280\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 2420218 D1=0.760231 D2=0.915235 D3+=1.14722\n",
      "2 32860060 D1=0.798355 D2=1.06966 D3+=1.27319\n",
      "3 79416358 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 2175 assuming -p 1.5\n",
      "probing 2372 assuming -r models -p 1.5\n",
      "trie     995 without quantization\n",
      "trie     594 assuming -q 8 -b 8 quantization \n",
      "trie     922 assuming -a 22 array pointer compression\n",
      "trie     520 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:29042616 2:525760960 3:1588327160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:29042616 2:525760960 3:1588327160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:9727008 kB\tVmRSS:67256 kB\tRSSMax:3819208 kB\tuser:157.65\tsys:22.7813\tCPU:180.431\treal:180.545\n"
     ]
    }
   ],
   "source": [
    "! ../src/kenlm/build/bin/lmplz -o 3 --discount_fallback < ../data/processed/kenlm/right_left.txt > ../models/kenlm/right_left.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, пришлось добавить опцию `--discount_fallback`, потому что при использовании 3-грамм падает ошибка. По всей видимости, ему не хватает полученных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинаризуем модель, чтобы ей можно было быстрее пользоваться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:04:14.850411Z",
     "start_time": "2021-01-02T16:02:18.325961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../models/kenlm/left_right.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/left_right.arpa ../models/kenlm/left_right.arpa.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:06:10.273584Z",
     "start_time": "2021-01-02T16:04:14.859562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../models/kenlm/right_left.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/right_left.arpa ../models/kenlm/right_left.arpa.binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим теперь небинаризованные модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:06:48.847228Z",
     "start_time": "2021-01-02T16:06:47.891653Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm ../models/kenlm/left_right.arpa\n",
    "!rm ../models/kenlm/right_left.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь загрузим модель и попробуем применить ее к какому-либо предложению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:06:56.793912Z",
     "start_time": "2021-01-02T16:06:50.599574Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "model_left_right = kenlm.LanguageModel(os.path.join(MODEL_PATH, 'kenlm', 'left_right.arpa.binary'))\n",
    "model_right_left = kenlm.LanguageModel(os.path.join(MODEL_PATH, 'kenlm', 'right_left.arpa.binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:06:56.834881Z",
     "start_time": "2021-01-02T16:06:56.797949Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 'журналисты всегда все нагло беспардонно переврут'\n",
    "example_reversed = ' '.join(example.split(' ')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:06:56.898783Z",
     "start_time": "2021-01-02T16:06:56.837727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-29.755603790283203"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_left_right.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T16:06:56.955574Z",
     "start_time": "2021-01-02T16:06:56.902007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-29.687007904052734"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_right_left.score(example_reversed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
