{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучения языковых моделей\n",
    "\n",
    "В этом ноутбуке будет произведено обучение языковых моделей для модели итеративного исправления. Требуется обучить две модели:\n",
    "\n",
    "1. Слева-направо\n",
    "2. Справа-налево\n",
    "\n",
    "В качестве обучающего корпуса будет взят корпус Тайга. Там есть части из соц.сетей, новостных сайтов, субтитров,что должно быть достаточно близким к изучаемому доменом.\n",
    "\n",
    "В качестве модели было решено взять KenLM в виду скорости и неплохого качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:27.257114Z",
     "start_time": "2021-01-18T11:02:27.235640Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:29.230093Z",
     "start_time": "2021-01-18T11:02:27.366575Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:29.946297Z",
     "start_time": "2021-01-18T11:02:29.232144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:29.976587Z",
     "start_time": "2021-01-18T11:02:29.949765Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "CONFIGS_PATH = os.path.join(PROJECT_PATH, 'src', 'configs')\n",
    "os.environ['DP_PROJECT_PATH'] = PROJECT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных решено было задействовать все данные корпуса \"Тайга\". Все файлы для скачивания доступны по [ссылке](https://tatianashavrina.github.io/taiga_site/downloads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "В первую очередь требуется предобработать все тексты, что у нас имеются. Согласно задаче, нас не интересует регистр слов и пунктуация, поэтому избавимся от нее. В качестве результата должны получиться два текстовых файла (прямой и обратный), где каждое предложение расположено на отдельной строчке -- именно в таком виде следует подавать данные для обучающей программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:30.190440Z",
     "start_time": "2021-01-18T11:02:29.979470Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ../data/processed/kenlm -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:30.229580Z",
     "start_time": "2021-01-18T11:02:30.192580Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "TAIGA_PATH = os.path.join(DATA_PATH, 'external', 'taiga')\n",
    "RESULT_PATH = os.path.join(DATA_PATH, 'processed', 'kenlm')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:30.266135Z",
     "start_time": "2021-01-18T11:02:30.231947Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = MosesTokenizer(lang='ru')\n",
    "detokenizer = MosesDetokenizer(lang='ru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет брать строчки с предложениями из файлов `texts_tagged`, убирать из них пунктуацию и записывать в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:30.308546Z",
     "start_time": "2021-01-18T11:02:30.268815Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_sentences(lines, out_file_left_right, out_file_right_left):\n",
    "    \"\"\"Чтение, обработка, запись предложений по строчкам в texts_tagged.\"\"\"\n",
    "    with open(out_file_left_right, 'a') as ouf_left_right:\n",
    "        with open(out_file_right_left, 'a') as ouf_right_left:\n",
    "            for line in lines[1:]:\n",
    "                if line.startswith('# text = '):\n",
    "                    sentence = line[len('# text = '):].strip().lower()\n",
    "                    tokenized_sentence = tokenizer.tokenize(\n",
    "                        sentence, escape=False\n",
    "                    )\n",
    "                    cleaned_tokenized_sentence = [\n",
    "                        x for x in tokenized_sentence \n",
    "                        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "                    ]\n",
    "                    ouf_left_right.write(\n",
    "                        detokenizer.detokenize(\n",
    "                            cleaned_tokenized_sentence\n",
    "                        )\n",
    "                        + '\\n'\n",
    "                    )\n",
    "                    ouf_right_left.write(\n",
    "                        detokenizer.detokenize(\n",
    "                            cleaned_tokenized_sentence[::-1]\n",
    "                        )\n",
    "                        + '\\n'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, которая будет читать данные непосредственно из директории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:30.347427Z",
     "start_time": "2021-01-18T11:02:30.311367Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_sentences_from_dir(dir_path, out_file_left_right, out_file_right_left):\n",
    "    \"\"\"Чтение, обработка, запись преложений из директории.\"\"\"\n",
    "    for filename in tqdm(sorted(os.listdir(dir_path))):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        with open(file_path, 'r') as inf:\n",
    "            lines = inf.readlines()\n",
    "            write_sentences(lines, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arzamas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:08:10.452580Z",
     "start_time": "2021-01-16T22:08:10.411070Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Arzamas', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'arzamas_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'arzamas_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:08:24.243962Z",
     "start_time": "2021-01-16T22:08:10.671779Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPlus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:08:25.493265Z",
     "start_time": "2021-01-16T22:08:25.455745Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'NPlus1', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'nplus1_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'nplus1_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:09:40.098548Z",
     "start_time": "2021-01-16T22:08:25.736854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Новости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fontanka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала добавим только Фонтанку, потому что там есть деление по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:09:40.126762Z",
     "start_time": "2021-01-16T22:09:40.101927Z"
    }
   },
   "outputs": [],
   "source": [
    "FONTANKA_PATH = os.path.join(TAIGA_PATH, 'Fontanka', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:51:49.569303Z",
     "start_time": "2021-01-16T22:09:40.129754Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in tqdm(sorted(os.listdir(FONTANKA_PATH))):\n",
    "    year_path = os.path.join(FONTANKA_PATH, year)\n",
    "    write_sentences_from_dir(year_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interfax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:51:49.615876Z",
     "start_time": "2021-01-16T22:51:49.574230Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Interfax', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:56:04.722472Z",
     "start_time": "2021-01-16T22:51:49.618654Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:56:04.762266Z",
     "start_time": "2021-01-16T22:56:04.724283Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'KP', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:59:50.591564Z",
     "start_time": "2021-01-16T22:56:04.765945Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T22:59:50.622617Z",
     "start_time": "2021-01-16T22:59:50.596612Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Lenta', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:04:30.334756Z",
     "start_time": "2021-01-16T22:59:50.625375Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соцсети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обработаем тексты из соцсетей. Насчет включения этого раздела я до сих пор сомневаюсь. Тут весьма специфичный вокабуляр и достаточно много опечаток самих по себе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:04:30.363378Z",
     "start_time": "2021-01-16T23:04:30.336782Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'social', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'social_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'social_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:33:03.276176Z",
     "start_time": "2021-01-16T23:04:30.366435Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Субтитры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем тексты из субтитров.\n",
    "\n",
    "Особенность обработки в том, что в данных помимо текста есть таймкоды. Также одно и то же предложение в общем случае разбито на несколько таймкодов. Поэтому придется научиться фильтровать таймкоды при помощи регулярных выражений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружать таблицу не понадобится, так как в `tagged_texts` уже лежат только субтитры на русском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:33:03.543343Z",
     "start_time": "2021-01-16T23:33:03.290630Z"
    }
   },
   "outputs": [],
   "source": [
    "SUBTITLES_PATH = os.path.join(TAIGA_PATH, 'Subtitles', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'subtitles_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'subtitles_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:56:33.934684Z",
     "start_time": "2021-01-16T23:33:03.546412Z"
    }
   },
   "outputs": [],
   "source": [
    "for title in tqdm(sorted(os.listdir(SUBTITLES_PATH))):\n",
    "    title_path = os.path.join(SUBTITLES_PATH, title)\n",
    "    for filename in sorted(os.listdir(title_path)):\n",
    "            file_path = os.path.join(title_path, filename)\n",
    "            with open(file_path, 'r') as inf:\n",
    "                lines = inf.readlines()\n",
    "                edited_lines = [\n",
    "                    re.sub(\n",
    "                        '\\d+ \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d', \n",
    "                        '', \n",
    "                        x\n",
    "                    )\n",
    "                    for x in lines\n",
    "                ]\n",
    "                write_sentences(edited_lines, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magazines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:56:33.964246Z",
     "start_time": "2021-01-16T23:56:33.937330Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Magazines', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'magazines_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'magazines_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T01:50:52.832612Z",
     "start_time": "2021-01-16T23:56:33.967257Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stihi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот раздел решено было пропустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T23:31:57.074051Z",
     "start_time": "2021-01-10T23:31:57.002557Z"
    }
   },
   "outputs": [],
   "source": [
    "STIHI_PATH = os.path.join(TAIGA_PATH, 'stihi_ru', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'stihi_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'stihi_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-10T23:31:58.175Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in tqdm(sorted(os.listdir(STIHI_PATH))):\n",
    "    year_path = os.path.join(STIHI_PATH, year)\n",
    "    for month in sorted(os.listdir(year_path)):\n",
    "        month_path = os.path.join(year_path, month)\n",
    "        write_sentences_from_dir(month_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этого раздело решено было взять только последние 3 года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T01:50:52.873413Z",
     "start_time": "2021-01-17T01:50:52.840110Z"
    }
   },
   "outputs": [],
   "source": [
    "PROZA_PATH = os.path.join(TAIGA_PATH, 'proza_ru', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'proza_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'proza_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:25:15.268743Z",
     "start_time": "2021-01-17T01:50:52.878311Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in tqdm(sorted(os.listdir(PROZA_PATH))[-3:]):\n",
    "    year_path = os.path.join(PROZA_PATH, year)\n",
    "    for month in tqdm(sorted(os.listdir(year_path))):\n",
    "        month_path = os.path.join(year_path, month)\n",
    "        write_sentences_from_dir(month_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из всего выше было решено взять:\n",
    "* Arzamas\n",
    "* NPlus1\n",
    "* Новости\n",
    "* Соцсети\n",
    "* Субтитры\n",
    "* Magazines\n",
    "* Proza\n",
    "\n",
    "Последний пункт был взят не полностью, а просто чтобы получить в итоге 50/100 млн предложений. Так было решено сделать, чтобы не слишком сильно раздувать размер языковой модели. В противном случае, она просто не поместится в оперативную память даже при использовании бора и квантизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем сконкатенируем полученные файлы для обучения языковых моделей. Для этого проще всего использовать команду `cat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет на 50 млн. предложений вести примерно 6.4 ГБ, а датасет на 100 млн. примерно 12.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выполним обучение. Для этого вспользуемя [документацией](https://kheafield.com/code/kenlm/estimation/) и [инструкцией](https://github.com/kmario23/KenLM-training).\n",
    "\n",
    "На этом этапе подразумевается, что библиотека уже склонирована в src/kenlm и собрана.\n",
    "\n",
    "Код ниже -- это просто демонстрация как может выглядеть обучение для одной конкретной языковой модели.\n",
    "\n",
    "Я обучал несколько вариаций, где варьировал\n",
    "1. Количество данных для обучения.\n",
    "2. Наличие прунинга.\n",
    "3. Наличие фильтрации по словарю.\n",
    "4. Порядок языковой модели (3 или 4).\n",
    "5. Использование данных из соц.сетей при обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T19:57:03.234550Z",
     "start_time": "2021-01-15T19:57:03.166186Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/lmplz -o 3 -S 80% -T /tmp < ../data/processed/kenlm/left_right.txt > ../models/kenlm/left_right.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот шаг существует для того, чтобы убрать из модели те слова/n-граммы, которых нет в целевом датасете, что позволяет уменьшить вес модели и время загрузки: [документация](https://kheafield.com/code/kenlm/filter/).\n",
    "\n",
    "В нашем случае можно попробовать выполнить фильтрацию по используемому словарю и исключить те n-граммы, которые включают неизвестные слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размеры бинаризованных моделей без фильтрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T08:17:31.156237Z",
     "start_time": "2021-01-18T08:17:30.958255Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/left_right_3_50.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним фильтрацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T08:55:11.996287Z",
     "start_time": "2021-01-18T08:51:06.842643Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/external/russian_words/russian_words_vocab.dict | ../src/kenlm/build/bin/filter single model:../models/kenlm/left_right_3_50.arpa ../models/kenlm/left_right_3_50_filtered.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на требуемый объем памяти для различных способов бинаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T08:55:12.409128Z",
     "start_time": "2021-01-18T08:55:12.005747Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/left_right_3_50_filtered.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем бинаризацию с построением бора, потому что она требует меньше памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T08:29:41.580446Z",
     "start_time": "2021-01-18T08:17:53.789656Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/build_binary trie ../models/kenlm/left_right_3_50.arpa ../models/kenlm/left_right_3_50.arpa.binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь загрузим модель и попробуем применить ее к какому-либо предложению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:49.192656Z",
     "start_time": "2021-01-18T11:02:33.807873Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "model_left_right = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'left_right_3_50.arpa.binary')\n",
    ")\n",
    "model_right_left = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'right_left_3_50.arpa.binary')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:02:49.316960Z",
     "start_time": "2021-01-18T11:02:49.202720Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 'журналисты всегда все нагло беспардонно переврут'\n",
    "example_reversed = ' '.join(example.split(' ')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:03:02.264424Z",
     "start_time": "2021-01-18T11:02:49.321207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57 µs ± 11.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model_left_right.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:03:02.295750Z",
     "start_time": "2021-01-18T11:03:02.267281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-26.72813606262207"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_left_right.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:03:02.336921Z",
     "start_time": "2021-01-18T11:03:02.299783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-26.647722244262695"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_right_left.score(example_reversed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
