{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучения языковых моделей\n",
    "\n",
    "В этом ноутбуке будет произведено обучение языковых моделей для модели итеративного исправления. Требуется обучить две модели:\n",
    "\n",
    "1. Слева-направо\n",
    "2. Справа-налево\n",
    "\n",
    "В качестве обучающего корпуса будет взят фрагмент корпуса Тайга, а именно части из соц.сетей, новостных сайтов, субтитров, так как это должен быть достаточно близкий к изучаемому домен.\n",
    "\n",
    "В качестве модели было решено взять KenLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:19:03.748420Z",
     "start_time": "2021-01-03T19:19:03.616579Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:19:05.876132Z",
     "start_time": "2021-01-03T19:19:03.756094Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:19:06.595254Z",
     "start_time": "2021-01-03T19:19:05.879237Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:19:06.638289Z",
     "start_time": "2021-01-03T19:19:06.599663Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "CONFIGS_PATH = os.path.join(PROJECT_PATH, 'src', 'configs')\n",
    "os.environ['DP_PROJECT_PATH'] = PROJECT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных для обучения решено было взять фрагмент корпуса [Тайга](https://tatianashavrina.github.io/taiga_site/). Были выбраны разделы:\n",
    "1. Новости\n",
    "2. Соцсети\n",
    "3. Субтитры\n",
    "\n",
    "Все файлы для скачивания доступны по [ссылке](https://tatianashavrina.github.io/taiga_site/downloads) в разделе \"Our special collections for\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "В первую очередь требуется предобработать все тексты, что у нас имеются. Согласно задаче, нас не интересует регистр слов и пунктуация, поэтому избавимся от нее. В качестве результата должны получиться два текстовых файла (прямой и обратный), где каждое предложение расположено на отдельной строчке -- именно в таком виде следует подавать данные для обучающей программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:19:09.031179Z",
     "start_time": "2021-01-03T19:19:08.774352Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ../data/processed/kenlm -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:19:10.587257Z",
     "start_time": "2021-01-03T19:19:10.532279Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "TAIGA_PATH = os.path.join(DATA_PATH, 'external', 'taiga')\n",
    "RESULT_PATH = os.path.join(DATA_PATH, 'processed', 'kenlm')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Новости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Новости уже хорошо предобработаны. В каждом файле из `texts_tagged` уже содержатся тексты новостей под аттрибутом `text`, потребуется лишь их достать, проигнорировав первую строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:36:12.564389Z",
     "start_time": "2021-01-03T19:36:12.516924Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentences(lines):\n",
    "    \"\"\"Нахождение предложений по строчкам в texts_tagged.\"\"\"\n",
    "    sentences = []\n",
    "    for line in lines[1:]:\n",
    "        if line.startswith('# text = '):\n",
    "            sentences.append(line[len('# text = '):].strip().lower())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:36:13.688722Z",
     "start_time": "2021-01-03T19:36:13.530886Z"
    }
   },
   "outputs": [],
   "source": [
    "NEWS_PATH = os.path.join(TAIGA_PATH, 'news')\n",
    "news_sentences = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала добавим только Фонтанку, потому что там есть деление по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:38:55.206133Z",
     "start_time": "2021-01-03T19:36:14.500710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f5eeefb07f4998a2c2edd701f68696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fontanka_path = os.path.join(NEWS_PATH, 'Fontanka', 'texts_tagged')\n",
    "for year in tqdm(sorted(os.listdir(fontanka_path))):\n",
    "    year_path = os.path.join(fontanka_path, year)\n",
    "    for filename in sorted(os.listdir(year_path)):\n",
    "        with open(os.path.join(year_path, filename), 'r') as inf:\n",
    "            news_sentences += get_sentences(inf.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим предложения по всем остальным новостным сайтам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:41:26.779783Z",
     "start_time": "2021-01-03T19:40:16.610151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e45446f8854868be86cf752edf2f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for source in tqdm(sorted(os.listdir(NEWS_PATH))):\n",
    "    if source == 'Fontanka':\n",
    "        continue\n",
    "    texts_path = os.path.join(NEWS_PATH, source, 'texts_tagged')\n",
    "    for filename in sorted(os.listdir(texts_path)):\n",
    "        with open(os.path.join(texts_path, filename), 'r') as inf:\n",
    "            news_sentences += get_sentences(inf.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:42:07.081252Z",
     "start_time": "2021-01-03T19:42:07.044211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4894406"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь каждый текст разобьем на токены и избавимся от тех из них, которые отвечают за пунктуацию. Сначала надо изучить какие вообще символы встречаются в текстах, чтобы понять что из этого может быть пунктуацией (иначе мы можем не учесть какие-то специфичные символы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:44:07.147089Z",
     "start_time": "2021-01-03T19:42:08.905291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ada67a06ba4d5892255015603e1fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4894406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "characters = Counter()\n",
    "for sentence in tqdm(news_sentences):\n",
    "    characters.update(list(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:44:07.183797Z",
     "start_time": "2021-01-03T19:44:07.150816Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'\"': 3733357,\n",
       "         'г': 8301552,\n",
       "         'а': 39054989,\n",
       "         'з': 7456501,\n",
       "         'п': 14871033,\n",
       "         'р': 26933314,\n",
       "         'о': 52170857,\n",
       "         'м': 14250265,\n",
       "         ' ': 74609033,\n",
       "         'и': 37013519,\n",
       "         'б': 8034209,\n",
       "         'е': 39736435,\n",
       "         'л': 19787078,\n",
       "         'у': 11955728,\n",
       "         'с': 27138786,\n",
       "         'я': 8533973,\n",
       "         'д': 14686825,\n",
       "         'ш': 2729334,\n",
       "         'н': 31798432,\n",
       "         'т': 30157793,\n",
       "         'в': 22437980,\n",
       "         'к': 16618999,\n",
       "         'х': 3894825,\n",
       "         'й': 5863506,\n",
       "         '.': 5566502,\n",
       "         ',': 6749140,\n",
       "         'щ': 1938653,\n",
       "         'ж': 4027292,\n",
       "         'ц': 2890879,\n",
       "         '1': 1713544,\n",
       "         '0': 1944453,\n",
       "         'ь': 6355169,\n",
       "         'ч': 5746368,\n",
       "         'ы': 8052626,\n",
       "         'ф': 1956865,\n",
       "         '2': 1569821,\n",
       "         '7': 451294,\n",
       "         '5': 620518,\n",
       "         '4': 552888,\n",
       "         '%': 105377,\n",
       "         '3': 728700,\n",
       "         '-': 2998758,\n",
       "         'n': 551770,\n",
       "         '6': 543891,\n",
       "         'a': 560516,\n",
       "         'ю': 2652290,\n",
       "         'э': 1341014,\n",
       "         '8': 415889,\n",
       "         's': 537242,\n",
       "         'o': 440135,\n",
       "         'c': 201163,\n",
       "         'i': 523004,\n",
       "         't': 639509,\n",
       "         'e': 667496,\n",
       "         'd': 174374,\n",
       "         'p': 372950,\n",
       "         'r': 658637,\n",
       "         '9': 457373,\n",
       "         'l': 322743,\n",
       "         'u': 457547,\n",
       "         'ъ': 221743,\n",
       "         ':': 706957,\n",
       "         'f': 135465,\n",
       "         '(': 343647,\n",
       "         ')': 345707,\n",
       "         'y': 74928,\n",
       "         'h': 200926,\n",
       "         'g': 120357,\n",
       "         'b': 134517,\n",
       "         'm': 152365,\n",
       "         'w': 565883,\n",
       "         'k': 174694,\n",
       "         '[': 8467,\n",
       "         ']': 8472,\n",
       "         '!': 40933,\n",
       "         '/': 517573,\n",
       "         '+': 85419,\n",
       "         'ё': 101050,\n",
       "         '\\\\': 11558,\n",
       "         'x': 84527,\n",
       "         ';': 1382872,\n",
       "         'v': 66208,\n",
       "         'z': 20969,\n",
       "         'j': 19006,\n",
       "         '$': 9708,\n",
       "         '@': 49735,\n",
       "         \"'\": 24273,\n",
       "         '*': 2603,\n",
       "         '=': 14170,\n",
       "         '&': 27629,\n",
       "         'q': 29561,\n",
       "         '^': 432,\n",
       "         '€': 993,\n",
       "         '~': 167,\n",
       "         '£': 141,\n",
       "         '{': 14669,\n",
       "         '}': 14665,\n",
       "         '#': 3232,\n",
       "         '|': 2116,\n",
       "         '§': 10,\n",
       "         '₤': 1,\n",
       "         '₱': 2,\n",
       "         '₽': 1,\n",
       "         '¥': 4})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наличие иностранных символов можно объяснить ссылкой на какой-то иностранный источник или имя на оригинальном языке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие символы мы уже имеем в пунктуации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T19:45:47.894289Z",
     "start_time": "2021-01-03T19:45:47.838246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширять его не требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем удалять те токены, которые состоят лишь из знаков пунктуации. Запишем результаты на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:21:45.844562Z",
     "start_time": "2021-01-03T19:46:42.797234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8866f12ebcfe4b63b49444b6f73706e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4894406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(news_sentences):\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(\n",
    "        sentence, language='russian'\n",
    "    )\n",
    "    cleaned_tokenized_sentence = [\n",
    "        x for x in tokenized_sentence \n",
    "        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "    ]\n",
    "    \n",
    "    with open(os.path.join(RESULT_PATH, 'news_left_right.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(RESULT_PATH, 'news_right_left.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence[::-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:22:57.620536Z",
     "start_time": "2021-01-03T20:22:56.874655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del news_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соцсети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обработаем тексты из соцсетей. Насчет включения этого раздела я до сих пор сомневаюсь. Тут весьма специфичный вокабуляр и достаточно много опечаток самих по себе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:23:51.253115Z",
     "start_time": "2021-01-03T20:23:51.204861Z"
    }
   },
   "outputs": [],
   "source": [
    "SOCIAL_PATH = os.path.join(TAIGA_PATH, 'social', 'tagged_texts')\n",
    "social_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:25:45.950814Z",
     "start_time": "2021-01-03T20:23:51.624315Z"
    }
   },
   "outputs": [],
   "source": [
    "for source in sorted(os.listdir(SOCIAL_PATH)):\n",
    "    with open(os.path.join(SOCIAL_PATH, source), 'r') as inf:\n",
    "        social_sentences += get_sentences(inf.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:25:46.217122Z",
     "start_time": "2021-01-03T20:25:45.963918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215928"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(social_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось токенизировать предложения и выполнить запись на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:45:20.777257Z",
     "start_time": "2021-01-03T20:26:07.122715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96d59556cf94ab8aa469766ec24c2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3215928.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(social_sentences):\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(\n",
    "        sentence, language='russian'\n",
    "    )\n",
    "    cleaned_tokenized_sentence = [\n",
    "        x for x in tokenized_sentence \n",
    "        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "    ]\n",
    "    \n",
    "    with open(os.path.join(RESULT_PATH, 'social_left_right.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(RESULT_PATH, 'social_right_left.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence[::-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:45:21.572511Z",
     "start_time": "2021-01-03T20:45:20.780588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1383"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del social_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Субтитры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем тексты из субтитров.\n",
    "\n",
    "Особенность обработки в том, что в данных помимо текста есть таймкоды. Также одно и то же предложение в общем случае разбито на несколько таймкодов. Поэтому придется научиться фильтровать таймкоды при помощи регулярных выражений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с того, что загрузим таблицу с метаданными, чтобы доставать файлы с русскими субтитрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:57:50.181463Z",
     "start_time": "2021-01-03T20:57:49.958542Z"
    }
   },
   "outputs": [],
   "source": [
    "SUBTITLES_PATH = os.path.join(TAIGA_PATH, 'subtitles')\n",
    "subtitles_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:57:50.267328Z",
     "start_time": "2021-01-03T20:57:50.183807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>languages</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "      <td>en</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "      <td>i</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10 Things I Hate About You - 1x02 - I Want You...</td>\n",
       "      <td>en</td>\n",
       "      <td>10 Things I Hate About You - 1x02 - I Want You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10 Things I Hate About You - 1x02 - I Want You...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x02 - I Want You...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title languages  \\\n",
       "0   0  10 Things I Hate About You - 1x01 - Pilot.HDTV...        en   \n",
       "1   1  10 Things I Hate About You - 1x01 - Pilot.HDTV...         i   \n",
       "2   2  10 Things I Hate About You - 1x01 - Pilot.HDTV...        ru   \n",
       "3   3  10 Things I Hate About You - 1x02 - I Want You...        en   \n",
       "4   4  10 Things I Hate About You - 1x02 - I Want You...        ru   \n",
       "\n",
       "                                            filepath  \n",
       "0  10 Things I Hate About You - 1x01 - Pilot.HDTV...  \n",
       "1  10 Things I Hate About You - 1x01 - Pilot.HDTV...  \n",
       "2  10 Things I Hate About You - 1x01 - Pilot.HDTV...  \n",
       "3  10 Things I Hate About You - 1x02 - I Want You...  \n",
       "4  10 Things I Hate About You - 1x02 - I Want You...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles_df = pd.read_csv(os.path.join(SUBTITLES_PATH, 'metatable.csv'), sep='\\t')\n",
    "subtitles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:57:50.323058Z",
     "start_time": "2021-01-03T20:57:50.270908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>languages</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x01 - Pilot.HDTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10 Things I Hate About You - 1x02 - I Want You...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x02 - I Want You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10 Things I Hate About You - 1x03 - Won't Get ...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x03 - Won't Get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10 Things I Hate About You - 1x04 - Don't Give...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x04 - Don't Give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>10 Things I Hate About You - 1x05 - Don't Give...</td>\n",
       "      <td>ru</td>\n",
       "      <td>10 Things I Hate About You - 1x05 - Don't Give...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title languages  \\\n",
       "2    2  10 Things I Hate About You - 1x01 - Pilot.HDTV...        ru   \n",
       "4    4  10 Things I Hate About You - 1x02 - I Want You...        ru   \n",
       "7    7  10 Things I Hate About You - 1x03 - Won't Get ...        ru   \n",
       "10  10  10 Things I Hate About You - 1x04 - Don't Give...        ru   \n",
       "13  13  10 Things I Hate About You - 1x05 - Don't Give...        ru   \n",
       "\n",
       "                                             filepath  \n",
       "2   10 Things I Hate About You - 1x01 - Pilot.HDTV...  \n",
       "4   10 Things I Hate About You - 1x02 - I Want You...  \n",
       "7   10 Things I Hate About You - 1x03 - Won't Get ...  \n",
       "10  10 Things I Hate About You - 1x04 - Don't Give...  \n",
       "13  10 Things I Hate About You - 1x05 - Don't Give...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles_df = subtitles_df[subtitles_df['languages'] == 'ru']\n",
    "subtitles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо отдельно обработать случай сериала `Marvels Agents of S.H.I.E.L.D`. Дело в том, что данные между названиями второго и первого сезонов неконсистентны и это не полностью отражено в таблице (есть вариант написания `Marvel s Agents of S.H.I.E.L.D`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:57:50.606210Z",
     "start_time": "2021-01-03T20:57:50.541069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>languages</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>11321</td>\n",
       "      <td>Marvel s Agents of S.H.I.E.L.D. - 2x04 - Face ...</td>\n",
       "      <td>ru</td>\n",
       "      <td>Marvel s Agents of S.H.I.E.L.D. - 2x04 - Face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11325</th>\n",
       "      <td>11325</td>\n",
       "      <td>Marvel s Agents of S.H.I.E.L.D. - 2x11 - After...</td>\n",
       "      <td>ru</td>\n",
       "      <td>Marvel s Agents of S.H.I.E.L.D. - 2x11 - After...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>11327</td>\n",
       "      <td>Marvels Agents of S.H.I.E.L.D. - 1x01 - Pilot....</td>\n",
       "      <td>ru</td>\n",
       "      <td>Marvels Agents of S.H.I.E.L.D. - 1x01 - Pilot....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11328</th>\n",
       "      <td>11328</td>\n",
       "      <td>Marvels Agents of S.H.I.E.L.D. - 1x02 - 0-8-4....</td>\n",
       "      <td>ru</td>\n",
       "      <td>Marvels Agents of S.H.I.E.L.D. - 1x02 - 0-8-4....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11330</th>\n",
       "      <td>11330</td>\n",
       "      <td>Marvels Agents of S.H.I.E.L.D. - 1x03 - The As...</td>\n",
       "      <td>ru</td>\n",
       "      <td>Marvels Agents of S.H.I.E.L.D. - 1x03 - The As...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title languages  \\\n",
       "11321  11321  Marvel s Agents of S.H.I.E.L.D. - 2x04 - Face ...        ru   \n",
       "11325  11325  Marvel s Agents of S.H.I.E.L.D. - 2x11 - After...        ru   \n",
       "11327  11327  Marvels Agents of S.H.I.E.L.D. - 1x01 - Pilot....        ru   \n",
       "11328  11328  Marvels Agents of S.H.I.E.L.D. - 1x02 - 0-8-4....        ru   \n",
       "11330  11330  Marvels Agents of S.H.I.E.L.D. - 1x03 - The As...        ru   \n",
       "\n",
       "                                                filepath  \n",
       "11321  Marvel s Agents of S.H.I.E.L.D. - 2x04 - Face ...  \n",
       "11325  Marvel s Agents of S.H.I.E.L.D. - 2x11 - After...  \n",
       "11327  Marvels Agents of S.H.I.E.L.D. - 1x01 - Pilot....  \n",
       "11328  Marvels Agents of S.H.I.E.L.D. - 1x02 - 0-8-4....  \n",
       "11330  Marvels Agents of S.H.I.E.L.D. - 1x03 - The As...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles_df[subtitles_df['filepath'].str.startswith('Marvel')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:57:50.716566Z",
     "start_time": "2021-01-03T20:57:50.679900Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = subtitles_df['filepath'].tolist()\n",
    "folders = [x.split(' - ')[0].strip(' .').replace('Marvels', 'Marvel s') for x in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:58:45.975438Z",
     "start_time": "2021-01-03T20:57:50.805384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a15823265041ad943afe5a3b9ec7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7899.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for folder, filename in tqdm(zip(folders, filenames), total=len(folders)):\n",
    "    with open(os.path.join(SUBTITLES_PATH, 'tagged_texts', folder, filename), 'r') as inf:\n",
    "        subtitles_sentences += get_sentences(inf.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T20:58:46.003087Z",
     "start_time": "2021-01-03T20:58:45.978424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3948916"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subtitles_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь уберем таймкоды из предложений. Для этого воспользуемся регулярными выражениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:02:05.170967Z",
     "start_time": "2021-01-03T21:02:05.143499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'тебе ясно, бьянка 11 00:00:32,610 00:00:36,437 но они не сделают этого, потому что ты запретил мне с ними встречаться, пока кэт не начнет.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = subtitles_sentences[11]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:02:05.496506Z",
     "start_time": "2021-01-03T21:02:05.466618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'тебе ясно, бьянка  но они не сделают этого, потому что ты запретил мне с ними встречаться, пока кэт не начнет.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\d+ \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d', '', example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:02:33.038197Z",
     "start_time": "2021-01-03T21:02:17.125486Z"
    }
   },
   "outputs": [],
   "source": [
    "subtitles_sentences = [\n",
    "    re.sub('\\d+ \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d', '', x).strip()\n",
    "    for x in subtitles_sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось токенизировать предложения и выполнить запись на диск. Также следует очистить "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:22:45.065427Z",
     "start_time": "2021-01-03T21:02:37.675331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00f9f28c7574f1c8f6e035232e18599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3948916.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for sentence in tqdm(subtitles_sentences):\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(\n",
    "        sentence.replace('\\t', ' ').replace('\\n', ' '), language='russian'\n",
    "    )\n",
    "    cleaned_tokenized_sentence = [\n",
    "        x for x in tokenized_sentence \n",
    "        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "    ]\n",
    "    with open(os.path.join(RESULT_PATH, 'subtitles_left_right.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence) + '\\n')\n",
    "        \n",
    "    with open(os.path.join(RESULT_PATH, 'subtitles_right_left.txt'), 'a') as ouf:\n",
    "        ouf.write(' '.join(cleaned_tokenized_sentence[::-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:23:49.136780Z",
     "start_time": "2021-01-03T21:23:48.086735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del subtitles_sentences, subtitles_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сконкатенируем полученные файлы для обучения языковых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:24:12.446730Z",
     "start_time": "2021-01-03T21:23:52.316630Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/processed/kenlm/news_left_right.txt ../data/processed/kenlm/social_left_right.txt ../data/processed/kenlm/subtitles_left_right.txt > ../data/processed/kenlm/left_right.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:24:35.543256Z",
     "start_time": "2021-01-03T21:24:12.451719Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/processed/kenlm/news_right_left.txt ../data/processed/kenlm/social_right_left.txt ../data/processed/kenlm/subtitles_right_left.txt > ../data/processed/kenlm/right_left.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на объем полученных датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:24:35.803327Z",
     "start_time": "2021-01-03T21:24:35.550259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7G\t../data/processed/kenlm/left_right.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ../data/processed/kenlm/left_right.txt -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:24:36.030175Z",
     "start_time": "2021-01-03T21:24:35.806703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7G\t../data/processed/kenlm/right_left.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ../data/processed/kenlm/right_left.txt -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выполним обучение. Для этого вспользуемя [документацией](https://kheafield.com/code/kenlm/estimation/) и [инструкцией](https://github.com/kmario23/KenLM-training).\n",
    "\n",
    "На этом этапе подразумевается, что библиотека уже склонирована в src/kenlm и собрана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:27:46.415381Z",
     "start_time": "2021-01-03T21:24:45.689604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/mrgeekman/Documents/MIPT/НИР/Repo/data/processed/kenlm/left_right.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 145379227 types 2168057\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:26016684 2:3399487488 3:6374039040\n",
      "Statistics:\n",
      "1 2168057 D1=0.752992 D2=0.924691 D3+=1.14515\n",
      "2 30503408 D1=0.796475 D2=1.08615 D3+=1.3121\n",
      "3 73612195 D1=0.83601 D2=0.766706 D3+=0.020062\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 2015 assuming -p 1.5\n",
      "probing 2198 assuming -r models -p 1.5\n",
      "trie     921 without quantization\n",
      "trie     549 assuming -q 8 -b 8 quantization \n",
      "trie     853 assuming -a 22 array pointer compression\n",
      "trie     480 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:26016684 2:488054528 3:1472243900\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:26016684 2:488054528 3:1472243900\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:9727008 kB\tVmRSS:62328 kB\tRSSMax:3701448 kB\tuser:147.07\tsys:22.6604\tCPU:169.73\treal:180.344\n"
     ]
    }
   ],
   "source": [
    "! ../src/kenlm/build/bin/lmplz -o 3 < ../data/processed/kenlm/left_right.txt > ../models/kenlm/left_right.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:31:37.039572Z",
     "start_time": "2021-01-03T21:28:40.544526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/mrgeekman/Documents/MIPT/НИР/Repo/data/processed/kenlm/right_left.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 145379227 types 2168057\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:26016684 2:3399487488 3:6374039040\n",
      "Statistics:\n",
      "1 2168057 D1=0.750457 D2=0.911682 D3+=1.17527\n",
      "2 30503408 D1=0.79503 D2=1.07301 D3+=1.29189\n",
      "3 73612195 D1=0.83601 D2=0.766706 D3+=0.020062\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 2015 assuming -p 1.5\n",
      "probing 2198 assuming -r models -p 1.5\n",
      "trie     921 without quantization\n",
      "trie     549 assuming -q 8 -b 8 quantization \n",
      "trie     853 assuming -a 22 array pointer compression\n",
      "trie     480 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:26016684 2:488054528 3:1472243900\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:26016684 2:488054528 3:1472243900\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:9727008 kB\tVmRSS:64064 kB\tRSSMax:3705712 kB\tuser:147.065\tsys:23.7451\tCPU:170.81\treal:175.776\n"
     ]
    }
   ],
   "source": [
    "! ../src/kenlm/build/bin/lmplz -o 3 < ../data/processed/kenlm/right_left.txt > ../models/kenlm/right_left.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинаризуем модель, чтобы ей можно было быстрее пользоваться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:33:34.059580Z",
     "start_time": "2021-01-03T21:31:37.399226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../models/kenlm/left_right.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/left_right.arpa ../models/kenlm/left_right.arpa.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:35:26.565683Z",
     "start_time": "2021-01-03T21:33:34.069274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../models/kenlm/right_left.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/right_left.arpa ../models/kenlm/right_left.arpa.binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим теперь небинаризованные модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:37:37.256464Z",
     "start_time": "2021-01-03T21:37:36.334557Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm ../models/kenlm/left_right.arpa\n",
    "!rm ../models/kenlm/right_left.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь загрузим модель и попробуем применить ее к какому-либо предложению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:37:51.476599Z",
     "start_time": "2021-01-03T21:37:45.324927Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "model_left_right = kenlm.LanguageModel(os.path.join(MODEL_PATH, 'kenlm', 'left_right.arpa.binary'))\n",
    "model_right_left = kenlm.LanguageModel(os.path.join(MODEL_PATH, 'kenlm', 'right_left.arpa.binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:37:51.508078Z",
     "start_time": "2021-01-03T21:37:51.479261Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 'журналисты всегда все нагло беспардонно переврут'\n",
    "example_reversed = ' '.join(example.split(' ')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:37:51.570732Z",
     "start_time": "2021-01-03T21:37:51.511486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-29.408767700195312"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_left_right.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:37:51.621583Z",
     "start_time": "2021-01-03T21:37:51.575371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-29.43677520751953"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_right_left.score(example_reversed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
