{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучения языковых моделей\n",
    "\n",
    "В этом ноутбуке будет произведено обучение языковых моделей для модели итеративного исправления. Требуется обучить две модели:\n",
    "\n",
    "1. Слева-направо\n",
    "2. Справа-налево\n",
    "\n",
    "В качестве обучающего корпуса будет взят корпус Тайга. Там есть части из соц.сетей, новостных сайтов, субтитров,что должно быть достаточно близким к изучаемому доменом.\n",
    "\n",
    "В качестве модели было решено взять KenLM в виду скорости и неплохого качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:20.740230Z",
     "start_time": "2021-01-15T13:35:20.708192Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:22.915063Z",
     "start_time": "2021-01-15T13:35:21.074630Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "sys.path.append('..')\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:23.292485Z",
     "start_time": "2021-01-15T13:35:22.918094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrgeekman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:23.328297Z",
     "start_time": "2021-01-15T13:35:23.296251Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.join(os.path.abspath(''), os.pardir)\n",
    "CONFIGS_PATH = os.path.join(PROJECT_PATH, 'src', 'configs')\n",
    "os.environ['DP_PROJECT_PATH'] = PROJECT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных решено было задействовать все данные корпуса \"Тайга\". Все файлы для скачивания доступны по [ссылке](https://tatianashavrina.github.io/taiga_site/downloads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "В первую очередь требуется предобработать все тексты, что у нас имеются. Согласно задаче, нас не интересует регистр слов и пунктуация, поэтому избавимся от нее. В качестве результата должны получиться два текстовых файла (прямой и обратный), где каждое предложение расположено на отдельной строчке -- именно в таком виде следует подавать данные для обучающей программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:26.183415Z",
     "start_time": "2021-01-15T13:35:25.955624Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ../data/processed/kenlm -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:26.755098Z",
     "start_time": "2021-01-15T13:35:26.671826Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "TAIGA_PATH = os.path.join(DATA_PATH, 'external', 'taiga')\n",
    "RESULT_PATH = os.path.join(DATA_PATH, 'processed', 'kenlm')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:31.480169Z",
     "start_time": "2021-01-15T13:35:31.444172Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = MosesTokenizer(lang='ru')\n",
    "detokenizer = MosesDetokenizer(lang='ru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет брать строчки с предложениями из файлов `texts_tagged`, убирать из них пунктуацию и записывать в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:32.095213Z",
     "start_time": "2021-01-15T13:35:32.056037Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_sentences(lines, out_file_left_right, out_file_right_left):\n",
    "    \"\"\"Чтение, обработка, запись предложений по строчкам в texts_tagged.\"\"\"\n",
    "    with open(out_file_left_right, 'a') as ouf_left_right:\n",
    "        with open(out_file_right_left, 'a') as ouf_right_left:\n",
    "            for line in lines[1:]:\n",
    "                if line.startswith('# text = '):\n",
    "                    sentence = line[len('# text = '):].strip().lower()\n",
    "                    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "                    cleaned_tokenized_sentence = [\n",
    "                        x for x in tokenized_sentence \n",
    "                        if not re.fullmatch('[' + punctuation + ']+', x)\n",
    "                    ]\n",
    "                    ouf_left_right.write(\n",
    "                        detokenizer.detokenize(\n",
    "                            cleaned_tokenized_sentence\n",
    "                        )\n",
    "                        + '\\n'\n",
    "                    )\n",
    "                    ouf_right_left.write(\n",
    "                        detokenizer.detokenize(\n",
    "                            cleaned_tokenized_sentence[::-1]\n",
    "                        )\n",
    "                        + '\\n'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, которая будет читать данные непосредственно из директории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:35:33.205875Z",
     "start_time": "2021-01-15T13:35:33.164429Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_sentences_from_dir(dir_path, out_file_left_right, out_file_right_left):\n",
    "    \"\"\"Чтение, обработка, запись преложений из директории.\"\"\"\n",
    "    for filename in tqdm(sorted(os.listdir(dir_path))):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        with open(file_path, 'r') as inf:\n",
    "            lines = inf.readlines()\n",
    "            write_sentences(lines, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arzamas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:10:25.612666Z",
     "start_time": "2021-01-10T17:10:25.587209Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Arzamas', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'arzamas_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'arzamas_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:10:40.066351Z",
     "start_time": "2021-01-10T17:10:25.774233Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPlus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:10:40.119611Z",
     "start_time": "2021-01-10T17:10:40.069604Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'NPlus1', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'nplus1_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'nplus1_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:11:56.826245Z",
     "start_time": "2021-01-10T17:10:40.123282Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Новости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fontanka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала добавим только Фонтанку, потому что там есть деление по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T17:11:56.855349Z",
     "start_time": "2021-01-10T17:11:56.830045Z"
    }
   },
   "outputs": [],
   "source": [
    "FONTANKA_PATH = os.path.join(TAIGA_PATH, 'Fontanka', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:00:38.610977Z",
     "start_time": "2021-01-10T17:11:56.857993Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in tqdm(sorted(os.listdir(FONTANKA_PATH))):\n",
    "    year_path = os.path.join(FONTANKA_PATH, year)\n",
    "    write_sentences_from_dir(year_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interfax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:00:38.725795Z",
     "start_time": "2021-01-10T18:00:38.618337Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Interfax', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:06:40.425148Z",
     "start_time": "2021-01-10T18:00:38.728139Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:06:40.473037Z",
     "start_time": "2021-01-10T18:06:40.436804Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'KP', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:10:53.880883Z",
     "start_time": "2021-01-10T18:06:40.476330Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:10:53.940721Z",
     "start_time": "2021-01-10T18:10:53.883875Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Lenta', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'news_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'news_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:16:16.203118Z",
     "start_time": "2021-01-10T18:10:53.944467Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соцсети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обработаем тексты из соцсетей. Насчет включения этого раздела я до сих пор сомневаюсь. Тут весьма специфичный вокабуляр и достаточно много опечаток самих по себе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:20:42.145634Z",
     "start_time": "2021-01-10T18:20:42.099850Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'social', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'social_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'social_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:48:27.362114Z",
     "start_time": "2021-01-10T18:20:42.650625Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Субтитры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем тексты из субтитров.\n",
    "\n",
    "Особенность обработки в том, что в данных помимо текста есть таймкоды. Также одно и то же предложение в общем случае разбито на несколько таймкодов. Поэтому придется научиться фильтровать таймкоды при помощи регулярных выражений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружать таблицу не понадобится, так как в `tagged_texts` уже лежат только субтитры на русском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T18:58:05.174175Z",
     "start_time": "2021-01-10T18:58:05.128027Z"
    }
   },
   "outputs": [],
   "source": [
    "SUBTITLES_PATH = os.path.join(TAIGA_PATH, 'Subtitles', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'subtitles_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'subtitles_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:26:03.206910Z",
     "start_time": "2021-01-10T18:58:08.667713Z"
    }
   },
   "outputs": [],
   "source": [
    "for title in tqdm(sorted(os.listdir(SUBTITLES_PATH))):\n",
    "    title_path = os.path.join(SUBTITLES_PATH, title)\n",
    "    for filename in sorted(os.listdir(title_path)):\n",
    "            file_path = os.path.join(title_path, filename)\n",
    "            with open(file_path, 'r') as inf:\n",
    "                lines = inf.readlines()\n",
    "                edited_lines = [\n",
    "                    re.sub(\n",
    "                        '\\d+ \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d\\d\\d', \n",
    "                        '', \n",
    "                        x\n",
    "                    )\n",
    "                    for x in lines\n",
    "                ]\n",
    "                write_sentences(edited_lines, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magazines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T19:33:02.543218Z",
     "start_time": "2021-01-10T19:33:02.504190Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_path = os.path.join(TAIGA_PATH, 'Magazines', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'magazines_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'magazines_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T21:51:41.847226Z",
     "start_time": "2021-01-10T19:33:02.750752Z"
    }
   },
   "outputs": [],
   "source": [
    "write_sentences_from_dir(cur_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T23:31:57.074051Z",
     "start_time": "2021-01-10T23:31:57.002557Z"
    }
   },
   "outputs": [],
   "source": [
    "STIHI_PATH = os.path.join(TAIGA_PATH, 'stihi_ru', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'stihi_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'stihi_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-10T23:31:58.175Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in tqdm(sorted(os.listdir(STIHI_PATH))):\n",
    "    year_path = os.path.join(STIHI_PATH, year)\n",
    "    for month in sorted(os.listdir(year_path)):\n",
    "        month_path = os.path.join(year_path, month)\n",
    "        write_sentences_from_dir(month_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:14:11.405187Z",
     "start_time": "2021-01-13T07:14:11.352537Z"
    }
   },
   "outputs": [],
   "source": [
    "PROZA_PATH = os.path.join(TAIGA_PATH, 'proza_ru', 'tagged_texts')\n",
    "left_right_path = os.path.join(RESULT_PATH, 'proza_left_right.txt')\n",
    "right_left_path = os.path.join(RESULT_PATH, 'proza_right_left.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T18:09:04.261396Z",
     "start_time": "2021-01-14T09:15:55.107889Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in tqdm(sorted(os.listdir(PROZA_PATH))[11:]):\n",
    "    year_path = os.path.join(PROZA_PATH, year)\n",
    "    for month in tqdm(sorted(os.listdir(year_path))):\n",
    "        month_path = os.path.join(year_path, month)\n",
    "        write_sentences_from_dir(month_path, left_right_path, right_left_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из всего выше было решено взять:\n",
    "* Arzamas\n",
    "* NPlus1\n",
    "* Новости\n",
    "* Соцсети\n",
    "* Субтитры\n",
    "* Magazines\n",
    "* Proza\n",
    "\n",
    "Последний пункт был взят не полностью, а просто чтобы получить в итоге 50 млн предложений. Так было решено сделать, чтобы не слишком сильно раздувать размер языковой модели. В противном случае, она просто не поместится в оперативную память даже при использовании бора и квантизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сконкатенируем полученные файлы для обучения языковых моделей. Для этого проще всего использовать команду `cat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на объем полученных датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:05.668393Z",
     "start_time": "2021-01-15T13:36:05.491384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4G\t../data/processed/kenlm/left_right.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ../data/processed/kenlm/left_right.txt -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:06.285632Z",
     "start_time": "2021-01-15T13:36:06.100473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4G\t../data/processed/kenlm/right_left.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ../data/processed/kenlm/right_left.txt -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выполним обучение. Для этого вспользуемя [документацией](https://kheafield.com/code/kenlm/estimation/) и [инструкцией](https://github.com/kmario23/KenLM-training).\n",
    "\n",
    "На этом этапе подразумевается, что библиотека уже склонирована в src/kenlm и собрана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:17.863373Z",
     "start_time": "2021-01-15T13:36:17.826916Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/lmplz -o 3 -S 80% -T /tmp < ../data/processed/kenlm/left_right.txt > ../models/kenlm/left_right.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:22.866220Z",
     "start_time": "2021-01-15T13:36:22.787509Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/lmplz -o 3 -S 80% -T /tmp < ../data/processed/kenlm/right_left.txt > ../models/kenlm/right_left.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размеры моделей составляют примерно 21 ГБ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот шаг существует для того, чтобы убрать из модели те слова/n-граммы, которых нет в целевом датасете, что позволяет уменьшить вес модели и время загрузки. [Документация](https://kheafield.com/code/kenlm/filter/).\n",
    "\n",
    "В нашем случае можно попробовать выполнить фильтрацию по используемому словарю и исключить те n-граммы, которые включают неизвестные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:33.241356Z",
     "start_time": "2021-01-15T13:36:33.164506Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/external/russian_words/russian_words_vocab.dict | ../src/kenlm/build/bin/filter single model:../models/kenlm/left_right.arpa ../models/kenlm/left_right_filtered.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:38.085625Z",
     "start_time": "2021-01-15T13:36:38.006813Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../data/external/russian_words/russian_words_vocab.dict | ../src/kenlm/build/bin/filter single model:../models/kenlm/right_left.arpa ../models/kenlm/right_left_filtered.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь размеры моделей составляют порядка 16 ГБ, что значительно меньше и позволит уменьшить размер модели после бинаризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на требуемый объем памяти для различных способов бинаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:44.352877Z",
     "start_time": "2021-01-15T13:36:44.135735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory estimate for binary LM:\r\n",
      "type      MB\r\n",
      "probing 5892 assuming -p 1.5\r\n",
      "probing 6355 assuming -r models -p 1.5\r\n",
      "trie    2511 without quantization\r\n",
      "trie    1417 assuming -q 8 -b 8 quantization \r\n",
      "trie    2324 assuming -a 22 array pointer compression\r\n",
      "trie    1230 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\r\n"
     ]
    }
   ],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/left_right_filtered.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:44.943503Z",
     "start_time": "2021-01-15T13:36:44.724501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory estimate for binary LM:\r\n",
      "type      MB\r\n",
      "probing 5888 assuming -p 1.5\r\n",
      "probing 6351 assuming -r models -p 1.5\r\n",
      "trie    2509 without quantization\r\n",
      "trie    1416 assuming -q 8 -b 8 quantization \r\n",
      "trie    2322 assuming -a 22 array pointer compression\r\n",
      "trie    1229 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\r\n"
     ]
    }
   ],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/right_left_filtered.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем бинаризацию при помощи пробирования. Она требует больше всего памяти, но зато является самой быстрой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:53.830611Z",
     "start_time": "2021-01-15T13:36:53.771726Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/left_right_filtered.arpa ../models/kenlm/left_right.arpa.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:36:57.803397Z",
     "start_time": "2021-01-15T13:36:57.767411Z"
    }
   },
   "outputs": [],
   "source": [
    "!../src/kenlm/build/bin/build_binary ../models/kenlm/right_left_filtered.arpa ../models/kenlm/right_left.arpa.binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь загрузим модель и попробуем применить ее к какому-либо предложению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:37:57.839864Z",
     "start_time": "2021-01-15T13:37:32.727167Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "model_left_right = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'left_right.arpa.binary')\n",
    ")\n",
    "model_right_left = kenlm.LanguageModel(\n",
    "    os.path.join(MODEL_PATH, 'kenlm', 'right_left.arpa.binary')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:37:57.976253Z",
     "start_time": "2021-01-15T13:37:57.849507Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 'журналисты всегда все нагло беспардонно переврут'\n",
    "example_reversed = ' '.join(example.split(' ')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:38:05.698621Z",
     "start_time": "2021-01-15T13:37:57.979049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917 ns ± 37.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model_left_right.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:38:05.732533Z",
     "start_time": "2021-01-15T13:38:05.701280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.65338897705078"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_left_right.score(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T13:38:05.778161Z",
     "start_time": "2021-01-15T13:38:05.736342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.61020278930664"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_right_left.score(example_reversed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
